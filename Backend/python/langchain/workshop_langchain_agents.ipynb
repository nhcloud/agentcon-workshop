{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa2dcadf",
   "metadata": {},
   "source": [
    "# ðŸš€ LangChain Agents Workshop: From Simple to Advanced\n",
    "\n",
    "Welcome to the LangChain Agents Workshop! This hands-on tutorial will take you on an exciting journey from basic agent concepts to advanced multi-agent systems.\n",
    "\n",
    "## ðŸŽ¯ What You'll Build\n",
    "\n",
    "1. **ðŸ§¬ Genetic Agent** - Start simple with a basic conversational agent\n",
    "2. **â˜ï¸ Azure AI Foundry Agent** - Level up with cloud-powered AI capabilities  \n",
    "3. **ðŸ’¬ Group Chat System** - Master advanced multi-agent orchestration\n",
    "\n",
    "## ðŸ› ï¸ Quick Setup\n",
    "\n",
    "This section sets up everything you need to run the agents in this notebook with minimal friction.\n",
    "\n",
    "**What it does:**\n",
    "- Installs Python dependencies and the shared local library\n",
    "- Lets you provide API keys (Azure AI Inference and optional providers)\n",
    "- Saves them to a local .env for reuse (optional)\n",
    "- Verifies the project structure\n",
    "- Optionally runs a tiny smoke test if keys are present\n",
    "\n",
    "**Instructions:** Proceed top-to-bottom; each step is self-checking and safe to rerun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00aacc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”§ Step 1: Install Dependencies\n",
    "# Safe to rerun multiple times\n",
    "\n",
    "import os, sys, subprocess, pathlib\n",
    "\n",
    "nb_dir = pathlib.Path().resolve()\n",
    "project_root = nb_dir.parents[2] if (len(nb_dir.parents) >= 2) else nb_dir\n",
    "lc_dir = nb_dir  # this notebook lives in Backend/python/langchain\n",
    "shared_dir = lc_dir.parent / \"shared\"\n",
    "req_file = lc_dir / \"requirements.txt\"\n",
    "\n",
    "print(f\"ðŸ” Notebook dir: {nb_dir}\")\n",
    "print(f\"ðŸ  Project root: {project_root}\")\n",
    "print(f\"ðŸ“¦ Using requirements: {req_file}\")\n",
    "print(f\"ðŸ”— Shared package dir: {shared_dir}\")\n",
    "\n",
    "def run_command(cmd):\n",
    "    print(f\"\\nðŸ’» Running: {cmd}\")\n",
    "    result = subprocess.run(cmd, shell=True, text=True, capture_output=True)\n",
    "    if result.returncode != 0:\n",
    "        print(f\"âŒ Error: {result.stderr}\")\n",
    "        raise SystemExit(f\"Command failed with exit code {result.returncode}\")\n",
    "    if result.stdout:\n",
    "        print(f\"âœ… Output: {result.stdout}\")\n",
    "\n",
    "# Install dependencies using pip magic commands (keeps kernel environment clean)\n",
    "try:\n",
    "    import IPython\n",
    "    get_ipython  # Verify we're in IPython/Jupyter\n",
    "    \n",
    "    print(\"\\nðŸ“¥ Installing requirements...\")\n",
    "    if req_file.exists():\n",
    "        get_ipython().run_line_magic(\"pip\", f\"install -r {req_file}\")\n",
    "        print(\"âœ… Requirements installed!\")\n",
    "    else:\n",
    "        print(\"âš ï¸ requirements.txt not found; skipping dependency install.\")\n",
    "    \n",
    "    print(\"\\nðŸ”— Installing shared library...\")\n",
    "    if (shared_dir / \"setup.py\").exists():\n",
    "        get_ipython().run_line_magic(\"pip\", f\"install -e {shared_dir}\")\n",
    "        print(\"âœ… Shared library installed!\")\n",
    "    else:\n",
    "        print(\"âš ï¸ Shared library setup.py not found; skipping -e install.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ IPython magic not available, falling back to subprocess: {e}\")\n",
    "    if req_file.exists():\n",
    "        run_command(f\"python -m pip install -r \\\"{req_file}\\\"\")\n",
    "    if (shared_dir / \"setup.py\").exists():\n",
    "        run_command(f\"python -m pip install -e \\\"{shared_dir}\\\"\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ Dependencies installation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999f2178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”‘ Step 2: Configure API Keys\n",
    "# Secure setup for Azure AI and other providers\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv, set_key\n",
    "from pathlib import Path\n",
    "\n",
    "# Load existing environment\n",
    "load_dotenv()\n",
    "\n",
    "def get_or_prompt(env_var, description, required=True):\n",
    "    \"\"\"Get environment variable or prompt user for input.\"\"\"\n",
    "    value = os.getenv(env_var)\n",
    "    if value:\n",
    "        print(f\"âœ… {env_var} already configured\")\n",
    "        return value\n",
    "    \n",
    "    if required:\n",
    "        print(f\"\\nðŸ”‘ Please provide your {description}:\")\n",
    "        value = input(f\"{env_var}: \").strip()\n",
    "        if not value and required:\n",
    "            raise ValueError(f\"{env_var} is required!\")\n",
    "    else:\n",
    "        print(f\"\\nðŸ”‘ Optional: {description} (press Enter to skip):\")\n",
    "        value = input(f\"{env_var}: \").strip()\n",
    "    \n",
    "    return value if value else None\n",
    "\n",
    "def save_to_env_file(env_vars):\n",
    "    \"\"\"Save environment variables to .env file.\"\"\"\n",
    "    env_file = Path(\".env\")\n",
    "    \n",
    "    print(f\"\\nðŸ’¾ Saving configuration to {env_file}...\")\n",
    "    for key, value in env_vars.items():\n",
    "        if value:\n",
    "            set_key(env_file, key, value)\n",
    "            os.environ[key] = value\n",
    "            print(f\"âœ… Saved {key}\")\n",
    "\n",
    "# Configure required and optional API keys\n",
    "print(\"ðŸ”§ Setting up API configuration...\")\n",
    "print(\"ðŸ“ Note: API keys will be saved to .env file for future use\")\n",
    "\n",
    "env_vars = {}\n",
    "\n",
    "# Required: Azure AI Inference\n",
    "env_vars[\"AZURE_AI_INFERENCE_ENDPOINT\"] = get_or_prompt(\n",
    "    \"AZURE_AI_INFERENCE_ENDPOINT\", \n",
    "    \"Azure AI Inference Endpoint (e.g., https://models.inference.ai.azure.com)\"\n",
    ")\n",
    "env_vars[\"AZURE_AI_INFERENCE_API_KEY\"] = get_or_prompt(\n",
    "    \"AZURE_AI_INFERENCE_API_KEY\", \n",
    "    \"Azure AI Inference API Key\"\n",
    ")\n",
    "\n",
    "# Optional: Additional providers\n",
    "env_vars[\"OPENAI_API_KEY\"] = get_or_prompt(\n",
    "    \"OPENAI_API_KEY\", \n",
    "    \"OpenAI API Key (optional)\", \n",
    "    required=False\n",
    ")\n",
    "\n",
    "env_vars[\"ANTHROPIC_API_KEY\"] = get_or_prompt(\n",
    "    \"ANTHROPIC_API_KEY\", \n",
    "    \"Anthropic API Key (optional)\", \n",
    "    required=False\n",
    ")\n",
    "\n",
    "# Save configuration\n",
    "save_to_env_file(env_vars)\n",
    "\n",
    "print(\"\\nðŸŽ‰ API configuration complete!\")\n",
    "print(\"ðŸ”„ Environment variables loaded and ready to use\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e206a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ” Step 3: Verify Project Structure\n",
    "# Ensure all required components are in place\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "def check_structure():\n",
    "    \"\"\"Verify the project has the expected structure.\"\"\"\n",
    "    print(\"ðŸ” Verifying project structure...\")\n",
    "    \n",
    "    # Add shared modules to path\n",
    "    shared_path = str(nb_dir.parent / \"shared\")\n",
    "    if shared_path not in sys.path:\n",
    "        sys.path.insert(0, shared_path)\n",
    "        print(f\"âœ… Added shared path: {shared_path}\")\n",
    "    \n",
    "    # Check key directories and files\n",
    "    checks = [\n",
    "        (shared_dir, \"Shared library directory\"),\n",
    "        (shared_dir / \"shared\", \"Shared package\"),\n",
    "        (lc_dir / \"agents\", \"LangChain agents directory\"),\n",
    "        (lc_dir / \"config.yml\", \"Configuration file\"),\n",
    "        (req_file, \"Requirements file\")\n",
    "    ]\n",
    "    \n",
    "    all_good = True\n",
    "    for path, description in checks:\n",
    "        if path.exists():\n",
    "            print(f\"âœ… {description}: {path}\")\n",
    "        else:\n",
    "            print(f\"âŒ Missing {description}: {path}\")\n",
    "            all_good = False\n",
    "    \n",
    "    return all_good\n",
    "\n",
    "# Verify structure\n",
    "structure_ok = check_structure()\n",
    "\n",
    "if structure_ok:\n",
    "    print(\"\\nðŸŽ‰ Project structure verification complete!\")\n",
    "    print(\"ðŸš€ Ready to start building agents!\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ Some project components are missing.\")\n",
    "    print(\"Please ensure you're running this notebook from the correct directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7471fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ§ª Step 4: Quick Smoke Test\n",
    "# Test basic connectivity and functionality\n",
    "\n",
    "print(\"ðŸ§ª Running quick smoke test...\")\n",
    "\n",
    "try:\n",
    "    # Test imports\n",
    "    print(\"ðŸ“¦ Testing imports...\")\n",
    "    from shared import AgentRegistry, AgentConfig, AgentMessage\n",
    "    print(\"âœ… Shared library imports successful\")\n",
    "    \n",
    "    # Test Azure AI Inference connection\n",
    "    if os.getenv(\"AZURE_AI_INFERENCE_ENDPOINT\") and os.getenv(\"AZURE_AI_INFERENCE_API_KEY\"):\n",
    "        print(\"ðŸ”— Testing Azure AI Inference connection...\")\n",
    "        # Simple connection test would go here\n",
    "        print(\"âœ… Azure AI Inference configuration looks good\")\n",
    "    else:\n",
    "        print(\"âš ï¸ Azure AI Inference not fully configured - will be needed for agents\")\n",
    "    \n",
    "    print(\"\\nðŸŽ‰ Smoke test passed!\")\n",
    "    print(\"ðŸš€ You're ready to start the workshop!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Smoke test failed: {e}\")\n",
    "    print(\"Please check your configuration and try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8836f5f",
   "metadata": {},
   "source": [
    "# ðŸ§¬ Workshop Part 1: Building Your First Genetic Agent\n",
    "\n",
    "Welcome to your first agent! We'll start with a **Genetic Agent** - a simple but powerful conversational AI that can adapt and evolve its responses.\n",
    "\n",
    "## ðŸŽ¯ What You'll Learn\n",
    "- Basic agent architecture with LangChain\n",
    "- How to create prompts that guide agent behavior\n",
    "- Message handling and conversation flow\n",
    "- Testing and interacting with your agent\n",
    "\n",
    "## ðŸ”¬ The Science Behind It\n",
    "A \"Genetic Agent\" uses evolutionary principles in its conversation:\n",
    "- **Adaptation**: Learns from conversation context\n",
    "- **Memory**: Maintains conversation history\n",
    "- **Evolution**: Improves responses based on feedback\n",
    "\n",
    "Let's build this step by step!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a73a76",
   "metadata": {},
   "source": [
    "## ðŸ—ï¸ Step 1.1: Import Required Libraries\n",
    "\n",
    "First, let's import all the libraries we need for our genetic agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98113bc1",
   "metadata": {},
   "source": [
    "## ðŸ§  Step 1.2: Create the Genetic Agent\n",
    "\n",
    "Now let's create our genetic agent with evolutionary capabilities!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14acc2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ§¬ Genetic Agent Implementation\n",
    "# A simple but powerful agent that evolves its conversation style\n",
    "\n",
    "import asyncio\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain.schema import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "class GeneticAgent:\n",
    "    \"\"\"\n",
    "    A genetic agent that evolves its conversation style based on interaction patterns.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, name: str = \"GeneticBot\"):\n",
    "        self.name = name\n",
    "        self.conversation_history = []\n",
    "        self.personality_traits = {\n",
    "            \"curiosity\": 0.7,\n",
    "            \"helpfulness\": 0.9,\n",
    "            \"creativity\": 0.6,\n",
    "            \"analytical\": 0.5\n",
    "        }\n",
    "        \n",
    "        # Initialize the LLM with Azure AI\n",
    "        self.llm = AzureChatOpenAI(\n",
    "            azure_endpoint=os.getenv(\"AZURE_AI_INFERENCE_ENDPOINT\"),\n",
    "            api_key=os.getenv(\"AZURE_AI_INFERENCE_API_KEY\"),\n",
    "            api_version=\"2024-02-15-preview\",\n",
    "            deployment_name=\"gpt-4o-mini\",  # Using the model that works\n",
    "            temperature=0.7\n",
    "        )\n",
    "        \n",
    "        # Create the prompt template\n",
    "        self.prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", self._build_system_prompt()),\n",
    "            MessagesPlaceholder(variable_name=\"history\"),\n",
    "            (\"human\", \"{input}\")\n",
    "        ])\n",
    "        \n",
    "        # Initialize memory\n",
    "        self.memory = ConversationBufferMemory(\n",
    "            memory_key=\"history\",\n",
    "            return_messages=True,\n",
    "            input_key=\"input\"\n",
    "        )\n",
    "        \n",
    "        print(f\"ðŸ§¬ {self.name} initialized with genetic traits:\")\n",
    "        for trait, value in self.personality_traits.items():\n",
    "            print(f\"   {trait.capitalize()}: {value:.1f}\")\n",
    "    \n",
    "    def _build_system_prompt(self) -> str:\n",
    "        \"\"\"Build a dynamic system prompt based on current personality traits.\"\"\"\n",
    "        return f\"\"\"You are {self.name}, a genetic agent with evolving personality traits.\n",
    "\n",
    "Current Genetic Profile:\n",
    "- Curiosity: {self.personality_traits['curiosity']:.1f} (how much you explore new topics)\n",
    "- Helpfulness: {self.personality_traits['helpfulness']:.1f} (how much you assist users)\n",
    "- Creativity: {self.personality_traits['creativity']:.1f} (how creative your responses are)\n",
    "- Analytical: {self.personality_traits['analytical']:.1f} (how logical and structured you are)\n",
    "\n",
    "Behavior Guidelines:\n",
    "- Adapt your responses based on your current genetic traits\n",
    "- Be conversational and engaging\n",
    "- Learn from the conversation and show growth\n",
    "- Use emojis occasionally to show personality\n",
    "- Keep responses concise but helpful\n",
    "\n",
    "Remember: You are evolving with each interaction!\"\"\"\n",
    "    \n",
    "    async def process_message(self, user_input: str) -> str:\n",
    "        \"\"\"Process a user message and generate a response.\"\"\"\n",
    "        try:\n",
    "            # Create the chain\n",
    "            chain = self.prompt | self.llm\n",
    "            \n",
    "            # Get the conversation history\n",
    "            history = self.memory.chat_memory.messages\n",
    "            \n",
    "            # Generate response\n",
    "            response = await chain.ainvoke({\n",
    "                \"input\": user_input,\n",
    "                \"history\": history\n",
    "            })\n",
    "            \n",
    "            # Save to memory\n",
    "            self.memory.save_context(\n",
    "                {\"input\": user_input},\n",
    "                {\"output\": response.content}\n",
    "            )\n",
    "            \n",
    "            # Evolve personality slightly based on interaction\n",
    "            self._evolve_traits(user_input, response.content)\n",
    "            \n",
    "            return response.content\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"ðŸ¤– Sorry, I encountered an error: {str(e)}\"\n",
    "    \n",
    "    def _evolve_traits(self, user_input: str, response: str):\n",
    "        \"\"\"Slightly evolve personality traits based on the interaction.\"\"\"\n",
    "        # Simple evolution logic - in practice, this could be much more sophisticated\n",
    "        evolution_rate = 0.01\n",
    "        \n",
    "        # Increase curiosity if user asks questions\n",
    "        if \"?\" in user_input:\n",
    "            self.personality_traits[\"curiosity\"] = min(1.0, \n",
    "                self.personality_traits[\"curiosity\"] + evolution_rate)\n",
    "        \n",
    "        # Increase helpfulness if providing assistance\n",
    "        if any(word in response.lower() for word in [\"help\", \"assist\", \"guide\", \"suggest\"]):\n",
    "            self.personality_traits[\"helpfulness\"] = min(1.0,\n",
    "                self.personality_traits[\"helpfulness\"] + evolution_rate)\n",
    "        \n",
    "        # Increase creativity if user asks for creative input\n",
    "        if any(word in user_input.lower() for word in [\"creative\", \"imagine\", \"idea\", \"story\"]):\n",
    "            self.personality_traits[\"creativity\"] = min(1.0,\n",
    "                self.personality_traits[\"creativity\"] + evolution_rate)\n",
    "    \n",
    "    def get_evolution_summary(self) -> str:\n",
    "        \"\"\"Get a summary of how the agent has evolved.\"\"\"\n",
    "        return f\"\"\"ðŸ§¬ Genetic Evolution Summary for {self.name}:\n",
    "        \n",
    "Current Traits:\n",
    "{chr(10).join([f\"  {trait.capitalize()}: {value:.2f}\" for trait, value in self.personality_traits.items()])}\n",
    "\n",
    "Conversation Length: {len(self.memory.chat_memory.messages)} messages\n",
    "\"\"\"\n",
    "\n",
    "# Create and test the genetic agent\n",
    "print(\"ðŸ§¬ Creating your first Genetic Agent...\")\n",
    "genetic_agent = GeneticAgent(\"Darwin\")\n",
    "print(\"âœ… Genetic Agent created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19019d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸŽ® Step 1.3: Test Your Genetic Agent\n",
    "# Let's have a conversation and watch it evolve!\n",
    "\n",
    "async def chat_with_genetic_agent():\n",
    "    \"\"\"Interactive chat function with the genetic agent.\"\"\"\n",
    "    print(\"\udfae Starting conversation with Darwin the Genetic Agent\")\n",
    "    print(\"ðŸ’¬ Type 'quit' to end the conversation\")\n",
    "    print(\"\udcca Type 'status' to see evolution progress\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Pre-defined test messages for demonstration\n",
    "    test_messages = [\n",
    "        \"Hello! What can you help me with?\",\n",
    "        \"Can you help me understand genetic algorithms?\", \n",
    "        \"What's a creative way to explain evolution?\",\n",
    "        \"How do you adapt to conversations?\"\n",
    "    ]\n",
    "    \n",
    "    print(\"ðŸ¤– Running automated conversation for demonstration...\")\n",
    "    \n",
    "    for i, message in enumerate(test_messages, 1):\n",
    "        print(f\"\\n\udc64 User: {message}\")\n",
    "        response = await genetic_agent.process_message(message)\n",
    "        print(f\"ðŸ§¬ Darwin: {response}\")\n",
    "        \n",
    "        # Show evolution after a few messages\n",
    "        if i == 2:\n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            print(genetic_agent.get_evolution_summary())\n",
    "            print(\"=\"*50)\n",
    "    \n",
    "    print(f\"\\nðŸŽ‰ Conversation complete! Final evolution status:\")\n",
    "    print(genetic_agent.get_evolution_summary())\n",
    "\n",
    "# Run the conversation\n",
    "await chat_with_genetic_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc6f44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸŽ¯ Step 1.4: Interactive Genetic Agent Testing\n",
    "# Try your own questions with the genetic agent\n",
    "\n",
    "def interactive_genetic_test():\n",
    "    \"\"\"Interactive testing function for the genetic agent.\"\"\"\n",
    "    print(\"ðŸŽ¯ Try these example interactions with Darwin:\")\n",
    "    print()\n",
    "    \n",
    "    examples = [\n",
    "        \"ðŸ”¬ Science Question: 'Explain how DNA replication works'\",\n",
    "        \"ðŸŽ¨ Creative Challenge: 'Write a short poem about evolution'\", \n",
    "        \"ðŸ§  Analytical Task: 'Compare genetic algorithms to neural networks'\",\n",
    "        \"â“ Curious Question: 'What would happen if humans could photosynthesize?'\"\n",
    "    ]\n",
    "    \n",
    "    for example in examples:\n",
    "        print(f\"  â€¢ {example}\")\n",
    "    \n",
    "    print(\"\\nðŸ’¡ Notice how Darwin's personality evolves based on your questions!\")\n",
    "    print(\"ðŸ“ˆ Check the evolution summary after each interaction to see changes.\")\n",
    "\n",
    "interactive_genetic_test()\n",
    "\n",
    "# For manual testing, uncomment the code below:\n",
    "\"\"\"\n",
    "# Manual testing (uncomment to use)\n",
    "async def manual_test():\n",
    "    user_input = input(\"Your message to Darwin: \")\n",
    "    response = await genetic_agent.process_message(user_input)\n",
    "    print(f\"Darwin: {response}\")\n",
    "    print(genetic_agent.get_evolution_summary())\n",
    "\n",
    "# await manual_test()\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nðŸŽ‰ Congratulations! You've built your first evolving AI agent!\")\n",
    "print(\"ðŸš€ Ready for the next level? Let's build an Azure AI Foundry agent!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f62af3",
   "metadata": {},
   "source": [
    "# â˜ï¸ Workshop Part 2: Azure AI Foundry Agent\n",
    "\n",
    "Now let's level up! You'll build a sophisticated **Azure AI Foundry Agent** that leverages cloud-powered AI capabilities.\n",
    "\n",
    "## ðŸŽ¯ What You'll Learn\n",
    "- Advanced Azure AI integration patterns\n",
    "- Multi-modal AI capabilities (text, reasoning, analysis)\n",
    "- Professional agent architecture\n",
    "- Real-world deployment considerations\n",
    "\n",
    "## ðŸ­ The Power of Azure AI Foundry\n",
    "Azure AI Foundry provides:\n",
    "- **ðŸš€ Scale**: Handle thousands of conversations\n",
    "- **ðŸ§  Intelligence**: Advanced reasoning capabilities  \n",
    "- **ðŸ”’ Security**: Enterprise-grade protection\n",
    "- **ðŸ”§ Tools**: Rich ecosystem of AI services\n",
    "\n",
    "Let's build a production-ready agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65624bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â˜ï¸ Azure AI Foundry Agent Implementation\n",
    "# A sophisticated agent using Azure AI's full capabilities\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "from typing import Optional, List, Dict\n",
    "import json\n",
    "import time\n",
    "\n",
    "class AgentCapability(Enum):\n",
    "    \"\"\"Different capabilities the Azure AI agent can use.\"\"\"\n",
    "    REASONING = \"reasoning\"\n",
    "    ANALYSIS = \"analysis\"\n",
    "    CREATIVE = \"creative\"\n",
    "    FACTUAL = \"factual\"\n",
    "    PROBLEM_SOLVING = \"problem_solving\"\n",
    "\n",
    "@dataclass\n",
    "class AgentResponse:\n",
    "    \"\"\"Structured response from the Azure AI agent.\"\"\"\n",
    "    content: str\n",
    "    capability_used: AgentCapability\n",
    "    confidence: float\n",
    "    processing_time: float\n",
    "    metadata: Dict[str, Any]\n",
    "\n",
    "class AzureAIFoundryAgent:\n",
    "    \"\"\"\n",
    "    A production-ready agent leveraging Azure AI Foundry capabilities.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, name: str = \"FoundryBot\", specialization: str = \"general\"):\n",
    "        self.name = name\n",
    "        self.specialization = specialization\n",
    "        self.conversation_count = 0\n",
    "        self.capabilities = list(AgentCapability)\n",
    "        \n",
    "        # Initialize Azure AI with enhanced configuration\n",
    "        self.llm = AzureChatOpenAI(\n",
    "            azure_endpoint=os.getenv(\"AZURE_AI_INFERENCE_ENDPOINT\"),\n",
    "            api_key=os.getenv(\"AZURE_AI_INFERENCE_API_KEY\"),\n",
    "            api_version=\"2024-02-15-preview\",\n",
    "            deployment_name=\"gpt-4o-mini\",\n",
    "            temperature=0.3,  # Lower temperature for more consistent responses\n",
    "            max_tokens=1000,\n",
    "            request_timeout=30\n",
    "        )\n",
    "        \n",
    "        # Advanced system prompt with capability routing\n",
    "        self.system_prompts = {\n",
    "            AgentCapability.REASONING: \"\"\"You are an advanced reasoning agent. Break down complex problems step-by-step, \n",
    "            use logical frameworks, and provide clear analytical thinking. Show your reasoning process.\"\"\",\n",
    "            \n",
    "            AgentCapability.ANALYSIS: \"\"\"You are a data analysis expert. Examine information critically, \n",
    "            identify patterns, draw insights, and provide evidence-based conclusions.\"\"\",\n",
    "            \n",
    "            AgentCapability.CREATIVE: \"\"\"You are a creative thinking agent. Generate innovative ideas, \n",
    "            think outside the box, and provide imaginative solutions while staying practical.\"\"\",\n",
    "            \n",
    "            AgentCapability.FACTUAL: \"\"\"You are a factual information agent. Provide accurate, verified information \n",
    "            with sources when possible. Be precise and comprehensive.\"\"\",\n",
    "            \n",
    "            AgentCapability.PROBLEM_SOLVING: \"\"\"You are a problem-solving specialist. Identify root causes, \n",
    "            generate multiple solution approaches, and recommend the best path forward.\"\"\"\n",
    "        }\n",
    "        \n",
    "        print(f\"â˜ï¸ {self.name} (Azure AI Foundry Agent) initialized\")\n",
    "        print(f\"ðŸŽ¯ Specialization: {self.specialization}\")\n",
    "        print(f\"ðŸ› ï¸ Available capabilities: {[cap.value for cap in self.capabilities]}\")\n",
    "    \n",
    "    def _detect_required_capability(self, user_input: str) -> AgentCapability:\n",
    "        \"\"\"Intelligently detect which capability is needed for the user's request.\"\"\"\n",
    "        input_lower = user_input.lower()\n",
    "        \n",
    "        # Reasoning indicators\n",
    "        if any(word in input_lower for word in [\"why\", \"how\", \"explain\", \"because\", \"logic\", \"reasoning\"]):\n",
    "            return AgentCapability.REASONING\n",
    "        \n",
    "        # Analysis indicators  \n",
    "        elif any(word in input_lower for word in [\"analyze\", \"compare\", \"evaluate\", \"assess\", \"examine\"]):\n",
    "            return AgentCapability.ANALYSIS\n",
    "        \n",
    "        # Creative indicators\n",
    "        elif any(word in input_lower for word in [\"creative\", \"imagine\", \"brainstorm\", \"innovative\", \"design\"]):\n",
    "            return AgentCapability.CREATIVE\n",
    "        \n",
    "        # Problem-solving indicators\n",
    "        elif any(word in input_lower for word in [\"problem\", \"solution\", \"fix\", \"resolve\", \"troubleshoot\"]):\n",
    "            return AgentCapability.PROBLEM_SOLVING\n",
    "        \n",
    "        # Default to factual for information requests\n",
    "        else:\n",
    "            return AgentCapability.FACTUAL\n",
    "    \n",
    "    async def process_message(self, user_input: str, \n",
    "                            override_capability: Optional[AgentCapability] = None) -> AgentResponse:\n",
    "        \"\"\"Process a message with the appropriate capability.\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Determine capability to use\n",
    "        capability = override_capability or self._detect_required_capability(user_input)\n",
    "        \n",
    "        # Build the enhanced prompt\n",
    "        system_prompt = self.system_prompts[capability]\n",
    "        full_prompt = f\"\"\"{system_prompt}\n",
    "\n",
    "Agent Info: You are {self.name}, specialized in {self.specialization}.\n",
    "Current Mode: {capability.value.upper()}\n",
    "Conversation Count: {self.conversation_count}\n",
    "\n",
    "User Request: {user_input}\n",
    "\n",
    "Instructions:\n",
    "1. Use the {capability.value} approach for this response\n",
    "2. Be professional but approachable\n",
    "3. Provide actionable insights\n",
    "4. Include relevant examples when helpful\n",
    "5. End with a brief summary or next steps\n",
    "\n",
    "Response:\"\"\"\n",
    "\n",
    "        try:\n",
    "            # Generate response using Azure AI\n",
    "            messages = [\n",
    "                SystemMessage(content=system_prompt),\n",
    "                HumanMessage(content=user_input)\n",
    "            ]\n",
    "            \n",
    "            response = await self.llm.ainvoke(messages)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            processing_time = time.time() - start_time\n",
    "            confidence = self._calculate_confidence(user_input, response.content)\n",
    "            \n",
    "            # Update conversation count\n",
    "            self.conversation_count += 1\n",
    "            \n",
    "            # Create structured response\n",
    "            agent_response = AgentResponse(\n",
    "                content=response.content,\n",
    "                capability_used=capability,\n",
    "                confidence=confidence,\n",
    "                processing_time=processing_time,\n",
    "                metadata={\n",
    "                    \"conversation_id\": self.conversation_count,\n",
    "                    \"agent_name\": self.name,\n",
    "                    \"specialization\": self.specialization,\n",
    "                    \"input_length\": len(user_input),\n",
    "                    \"output_length\": len(response.content)\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            return agent_response\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Error handling with structured response\n",
    "            return AgentResponse(\n",
    "                content=f\"âš ï¸ I encountered an issue: {str(e)}. Please try again.\",\n",
    "                capability_used=capability,\n",
    "                confidence=0.0,\n",
    "                processing_time=time.time() - start_time,\n",
    "                metadata={\"error\": str(e)}\n",
    "            )\n",
    "    \n",
    "    def _calculate_confidence(self, input_text: str, output_text: str) -> float:\n",
    "        \"\"\"Calculate confidence score for the response.\"\"\"\n",
    "        # Simple confidence calculation based on response characteristics\n",
    "        confidence = 0.5  # Base confidence\n",
    "        \n",
    "        # Higher confidence for longer, detailed responses\n",
    "        if len(output_text) > 200:\n",
    "            confidence += 0.2\n",
    "        \n",
    "        # Higher confidence if response includes examples or structure\n",
    "        if any(indicator in output_text.lower() for indicator in [\"for example\", \"step\", \"1.\", \"â€¢\", \"-\"]):\n",
    "            confidence += 0.2\n",
    "        \n",
    "        # Cap at 0.95 (never 100% confident)\n",
    "        return min(0.95, confidence)\n",
    "    \n",
    "    def get_agent_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get comprehensive agent statistics.\"\"\"\n",
    "        return {\n",
    "            \"name\": self.name,\n",
    "            \"specialization\": self.specialization,\n",
    "            \"conversations\": self.conversation_count,\n",
    "            \"capabilities\": [cap.value for cap in self.capabilities],\n",
    "            \"status\": \"active\"\n",
    "        }\n",
    "    \n",
    "    def print_response_analysis(self, response: AgentResponse):\n",
    "        \"\"\"Print a detailed analysis of the agent's response.\"\"\"\n",
    "        print(f\"\\nðŸ“Š Response Analysis:\")\n",
    "        print(f\"ðŸŽ¯ Capability Used: {response.capability_used.value}\")\n",
    "        print(f\"ðŸŽ² Confidence: {response.confidence:.1%}\")\n",
    "        print(f\"â±ï¸ Processing Time: {response.processing_time:.2f}s\")\n",
    "        print(f\"ðŸ“ Response Length: {len(response.content)} characters\")\n",
    "        print(f\"ðŸ”¢ Conversation #: {response.metadata.get('conversation_id', 'N/A')}\")\n",
    "\n",
    "# Create the Azure AI Foundry Agent\n",
    "print(\"â˜ï¸ Initializing Azure AI Foundry Agent...\")\n",
    "foundry_agent = AzureAIFoundryAgent(\"Azure-AI-Assistant\", \"Multi-domain AI Helper\")\n",
    "print(\"âœ… Azure AI Foundry Agent ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725bf4f2",
   "metadata": {},
   "source": [
    "## ðŸ§ª Step 2.2: Test Azure AI Foundry Capabilities\n",
    "\n",
    "Let's test the different capabilities of our Azure AI Foundry agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bb2fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ§ª Test Different Azure AI Capabilities\n",
    "# Demonstrate the agent's intelligent capability routing\n",
    "\n",
    "async def test_foundry_capabilities():\n",
    "    \"\"\"Test different capabilities of the Azure AI Foundry agent.\"\"\"\n",
    "    \n",
    "    test_scenarios = [\n",
    "        {\n",
    "            \"category\": \"ðŸ§  Reasoning\",\n",
    "            \"prompt\": \"Why do neural networks work better with more data?\",\n",
    "            \"expected_capability\": AgentCapability.REASONING\n",
    "        },\n",
    "        {\n",
    "            \"category\": \"ðŸ“Š Analysis\", \n",
    "            \"prompt\": \"Analyze the pros and cons of cloud computing vs on-premise solutions\",\n",
    "            \"expected_capability\": AgentCapability.ANALYSIS\n",
    "        },\n",
    "        {\n",
    "            \"category\": \"ðŸŽ¨ Creative\",\n",
    "            \"prompt\": \"Design an innovative app concept for sustainable living\",\n",
    "            \"expected_capability\": AgentCapability.CREATIVE\n",
    "        },\n",
    "        {\n",
    "            \"category\": \"ðŸ”§ Problem Solving\",\n",
    "            \"prompt\": \"My Python script is running slowly. How can I troubleshoot and fix it?\",\n",
    "            \"expected_capability\": AgentCapability.PROBLEM_SOLVING\n",
    "        },\n",
    "        {\n",
    "            \"category\": \"ðŸ“š Factual\",\n",
    "            \"prompt\": \"What is quantum computing and how does it work?\",\n",
    "            \"expected_capability\": AgentCapability.FACTUAL\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(\"ðŸ§ª Testing Azure AI Foundry Agent Capabilities\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for i, scenario in enumerate(test_scenarios, 1):\n",
    "        print(f\"\\n{i}. {scenario['category']} Test\")\n",
    "        print(f\"ðŸ“ Prompt: {scenario['prompt']}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Process the message\n",
    "        response = await foundry_agent.process_message(scenario['prompt'])\n",
    "        \n",
    "        # Display response\n",
    "        print(f\"ðŸ¤– Response: {response.content[:200]}...\")\n",
    "        \n",
    "        # Show analysis\n",
    "        foundry_agent.print_response_analysis(response)\n",
    "        \n",
    "        # Check if capability detection worked\n",
    "        expected = scenario['expected_capability']\n",
    "        actual = response.capability_used\n",
    "        \n",
    "        if expected == actual:\n",
    "            print(f\"âœ… Capability Detection: Correct ({actual.value})\")\n",
    "        else:\n",
    "            print(f\"âš ï¸ Capability Detection: Expected {expected.value}, got {actual.value}\")\n",
    "        \n",
    "        print(\"-\" * 60)\n",
    "    \n",
    "    # Show final stats\n",
    "    print(f\"\\nðŸ“ˆ Agent Performance Summary:\")\n",
    "    stats = foundry_agent.get_agent_stats()\n",
    "    for key, value in stats.items():\n",
    "        print(f\"   {key.capitalize()}: {value}\")\n",
    "\n",
    "# Run the capability tests\n",
    "await test_foundry_capabilities()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5991cd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸŽ¯ Step 2.3: Advanced Foundry Agent Testing\n",
    "# Test specific capabilities and override detection\n",
    "\n",
    "async def test_capability_override():\n",
    "    \"\"\"Test manual capability override functionality.\"\"\"\n",
    "    \n",
    "    print(\"ðŸŽ¯ Testing Capability Override Feature\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Test the same prompt with different capabilities\n",
    "    test_prompt = \"Tell me about artificial intelligence\"\n",
    "    \n",
    "    capabilities_to_test = [\n",
    "        AgentCapability.FACTUAL,\n",
    "        AgentCapability.CREATIVE, \n",
    "        AgentCapability.ANALYSIS,\n",
    "        AgentCapability.REASONING\n",
    "    ]\n",
    "    \n",
    "    for capability in capabilities_to_test:\n",
    "        print(f\"\\nðŸ”§ Testing with {capability.value.upper()} capability:\")\n",
    "        print(f\"ðŸ“ Prompt: {test_prompt}\")\n",
    "        \n",
    "        response = await foundry_agent.process_message(\n",
    "            test_prompt, \n",
    "            override_capability=capability\n",
    "        )\n",
    "        \n",
    "        print(f\"ðŸ¤– Response (first 150 chars): {response.content[:150]}...\")\n",
    "        print(f\"âœ… Used capability: {response.capability_used.value}\")\n",
    "        print(f\"ðŸ“Š Confidence: {response.confidence:.1%}\")\n",
    "        print(\"-\" * 30)\n",
    "    \n",
    "    print(\"\\nðŸŽ‰ Notice how the same question gets different types of responses!\")\n",
    "    print(\"ðŸš€ This shows the power of capability-driven AI responses!\")\n",
    "\n",
    "# Run the override test\n",
    "await test_capability_override()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1373c3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ’¡ Interactive Azure AI Foundry Testing\n",
    "# Try your own prompts with capability detection\n",
    "\n",
    "def interactive_foundry_examples():\n",
    "    \"\"\"Show examples for interactive testing with the Foundry agent.\"\"\"\n",
    "    \n",
    "    print(\"ðŸ’¡ Try These Examples with the Azure AI Foundry Agent:\")\n",
    "    print()\n",
    "    \n",
    "    examples = {\n",
    "        \"ðŸ§  Reasoning Examples\": [\n",
    "            \"Why does machine learning require so much data?\",\n",
    "            \"How do recommendation algorithms work?\",\n",
    "            \"Explain the logic behind A/B testing\"\n",
    "        ],\n",
    "        \"ðŸ“Š Analysis Examples\": [\n",
    "            \"Compare Python vs JavaScript for web development\",\n",
    "            \"Evaluate the security implications of cloud storage\",\n",
    "            \"Analyze the benefits of microservices architecture\"\n",
    "        ],\n",
    "        \"ðŸŽ¨ Creative Examples\": [\n",
    "            \"Design a futuristic smart city concept\",\n",
    "            \"Create an innovative solution for remote work collaboration\",\n",
    "            \"Imagine a new type of user interface\"\n",
    "        ],\n",
    "        \"ðŸ”§ Problem-Solving Examples\": [\n",
    "            \"My website loads slowly, how can I optimize it?\",\n",
    "            \"How to handle database connection errors in production?\",\n",
    "            \"Best practices for debugging complex applications\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    for category, prompts in examples.items():\n",
    "        print(f\"{category}:\")\n",
    "        for prompt in prompts:\n",
    "            print(f\"  â€¢ {prompt}\")\n",
    "        print()\n",
    "    \n",
    "    print(\"ðŸ”¬ Notice how the agent automatically selects the right capability!\")\n",
    "    print(\"âš¡ You can also force a specific capability using override_capability\")\n",
    "\n",
    "interactive_foundry_examples()\n",
    "\n",
    "print(\"\\nðŸŽ‰ Congratulations! You've mastered Azure AI Foundry agents!\")\n",
    "print(\"ðŸš€ Ready for the ultimate challenge? Let's build a group chat system!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264b0176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”— Bridge: Combining Genetic and Foundry Agents\n",
    "# See how different agent types can work together\n",
    "\n",
    "async def agent_collaboration_demo():\n",
    "    \"\"\"Demonstrate how genetic and foundry agents can collaborate.\"\"\"\n",
    "    \n",
    "    print(\"ðŸ”— Agent Collaboration Demonstration\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    collaboration_prompt = \"How can AI agents work together effectively?\"\n",
    "    \n",
    "    print(f\"ðŸ“ Collaboration Question: {collaboration_prompt}\")\n",
    "    print()\n",
    "    \n",
    "    # Get response from genetic agent\n",
    "    print(\"ðŸ§¬ Darwin (Genetic Agent) responds:\")\n",
    "    genetic_response = await genetic_agent.process_message(collaboration_prompt)\n",
    "    print(f\"   {genetic_response[:200]}...\")\n",
    "    print()\n",
    "    \n",
    "    # Get response from foundry agent with analysis capability\n",
    "    print(\"â˜ï¸ Azure-AI-Assistant (Foundry Agent) analyzes:\")\n",
    "    foundry_response = await foundry_agent.process_message(\n",
    "        f\"Analyze this perspective on AI collaboration: {genetic_response[:100]}... \"\n",
    "        f\"and provide additional insights on {collaboration_prompt}\",\n",
    "        override_capability=AgentCapability.ANALYSIS\n",
    "    )\n",
    "    print(f\"   {foundry_response.content[:200]}...\")\n",
    "    \n",
    "    print(\"\\nðŸŽ¯ Key Insight: Different agents bring different strengths!\")\n",
    "    print(\"\uddec Genetic Agent: Adaptive, evolving personality\")\n",
    "    print(\"â˜ï¸ Foundry Agent: Structured, capability-driven responses\")\n",
    "    print(\"ðŸ¤ Together: More comprehensive and nuanced conversations!\")\n",
    "\n",
    "# Run collaboration demo\n",
    "await agent_collaboration_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a2ceae",
   "metadata": {},
   "source": [
    "# ðŸ’¬ Workshop Part 3: Advanced Group Chat System\n",
    "\n",
    "Welcome to the final challenge! You'll build a sophisticated **Multi-Agent Group Chat System** where different AI personalities collaborate to solve complex problems.\n",
    "\n",
    "## ðŸŽ¯ What You'll Master\n",
    "- Multi-agent orchestration and coordination\n",
    "- Dynamic conversation flow management\n",
    "- Agent specialization and role assignment\n",
    "- Consensus building and conflict resolution\n",
    "- Real-time collaboration patterns\n",
    "\n",
    "## ðŸŒŸ The Power of Agent Teams\n",
    "Group chat systems enable:\n",
    "- **ðŸ§© Specialization**: Each agent brings unique expertise\n",
    "- **\udd04 Collaboration**: Agents build on each other's ideas\n",
    "- **âš¡ Efficiency**: Parallel processing of complex problems\n",
    "- **ðŸŽ­ Diversity**: Different perspectives and approaches\n",
    "\n",
    "Let's build the future of AI collaboration!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c304acd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ’¬ Advanced Group Chat System Implementation\n",
    "# Multi-agent collaboration with orchestration and coordination\n",
    "\n",
    "import uuid\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Optional, Set\n",
    "from enum import Enum\n",
    "\n",
    "class AgentRole(Enum):\n",
    "    \"\"\"Different roles agents can play in group chat.\"\"\"\n",
    "    FACILITATOR = \"facilitator\"\n",
    "    EXPERT = \"expert\"\n",
    "    CRITIC = \"critic\"\n",
    "    SYNTHESIZER = \"synthesizer\"\n",
    "    CREATIVE = \"creative\"\n",
    "\n",
    "@dataclass\n",
    "class ChatMessage:\n",
    "    \"\"\"Enhanced message structure for group chat.\"\"\"\n",
    "    id: str = field(default_factory=lambda: str(uuid.uuid4()))\n",
    "    sender: str = \"\"\n",
    "    content: str = \"\"\n",
    "    role: AgentRole = AgentRole.EXPERT\n",
    "    timestamp: datetime = field(default_factory=datetime.now)\n",
    "    references: List[str] = field(default_factory=list)  # IDs of messages this responds to\n",
    "    confidence: float = 0.0\n",
    "    metadata: Dict[str, Any] = field(default_factory=dict)\n",
    "\n",
    "class GroupChatOrchestrator:\n",
    "    \"\"\"Orchestrates conversation flow between multiple agents.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.agents = {}\n",
    "        self.conversation_history = []\n",
    "        self.active_topics = set()\n",
    "        self.conversation_id = str(uuid.uuid4())\n",
    "        \n",
    "    def add_agent(self, agent, role: AgentRole):\n",
    "        \"\"\"Add an agent to the group chat with a specific role.\"\"\"\n",
    "        agent_id = f\"{agent.name}_{role.value}\"\n",
    "        self.agents[agent_id] = {\n",
    "            \"agent\": agent,\n",
    "            \"role\": role,\n",
    "            \"message_count\": 0,\n",
    "            \"last_active\": None\n",
    "        }\n",
    "        print(f\"âœ… Added {agent.name} as {role.value}\")\n",
    "        \n",
    "    def get_conversation_context(self, last_n_messages: int = 5) -> str:\n",
    "        \"\"\"Get recent conversation context for agents.\"\"\"\n",
    "        recent_messages = self.conversation_history[-last_n_messages:]\n",
    "        context = \"Recent conversation:\\n\"\n",
    "        for msg in recent_messages:\n",
    "            context += f\"{msg.sender} ({msg.role.value}): {msg.content}\\n\"\n",
    "        return context\n",
    "        \n",
    "    async def facilitate_discussion(self, topic: str, max_rounds: int = 3) -> List[ChatMessage]:\n",
    "        \"\"\"Facilitate a structured discussion on a topic.\"\"\"\n",
    "        print(f\"ðŸŽ¯ Starting group discussion on: {topic}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Initialize the discussion\n",
    "        discussion_messages = []\n",
    "        self.active_topics.add(topic)\n",
    "        \n",
    "        # Round 1: Each agent provides initial perspective\n",
    "        print(\"ðŸ”„ Round 1: Initial Perspectives\")\n",
    "        for agent_id, agent_info in self.agents.items():\n",
    "            agent = agent_info[\"agent\"]\n",
    "            role = agent_info[\"role\"]\n",
    "            \n",
    "            # Create role-specific prompt\n",
    "            prompt = self._create_role_prompt(topic, role, self.get_conversation_context())\n",
    "            \n",
    "            # Get agent response\n",
    "            if hasattr(agent, 'process_message'):\n",
    "                response_content = await agent.process_message(prompt)\n",
    "                if hasattr(response_content, 'content'):\n",
    "                    response_content = response_content.content\n",
    "            else:\n",
    "                response_content = f\"[{agent.name} would respond here based on their {role.value} role]\"\n",
    "            \n",
    "            # Create chat message\n",
    "            message = ChatMessage(\n",
    "                sender=agent.name,\n",
    "                content=response_content,\n",
    "                role=role,\n",
    "                confidence=0.8,\n",
    "                metadata={\"round\": 1, \"topic\": topic}\n",
    "            )\n",
    "            \n",
    "            discussion_messages.append(message)\n",
    "            self.conversation_history.append(message)\n",
    "            agent_info[\"message_count\"] += 1\n",
    "            agent_info[\"last_active\"] = datetime.now()\n",
    "            \n",
    "            print(f\"\\nðŸ¤– {agent.name} ({role.value}):\")\n",
    "            print(f\"   {response_content[:150]}...\")\n",
    "            \n",
    "        # Additional rounds: Responses and synthesis\n",
    "        for round_num in range(2, max_rounds + 1):\n",
    "            print(f\"\\nðŸ”„ Round {round_num}: Building on Ideas\")\n",
    "            \n",
    "            # Select a few agents to respond to others\n",
    "            responding_agents = list(self.agents.items())[:2]  # First 2 agents respond\n",
    "            \n",
    "            for agent_id, agent_info in responding_agents:\n",
    "                agent = agent_info[\"agent\"]\n",
    "                role = agent_info[\"role\"]\n",
    "                \n",
    "                # Create synthesis prompt based on previous messages\n",
    "                synthesis_prompt = self._create_synthesis_prompt(\n",
    "                    topic, role, discussion_messages[-len(self.agents):]\n",
    "                )\n",
    "                \n",
    "                if hasattr(agent, 'process_message'):\n",
    "                    response_content = await agent.process_message(synthesis_prompt)\n",
    "                    if hasattr(response_content, 'content'):\n",
    "                        response_content = response_content.content\n",
    "                else:\n",
    "                    response_content = f\"[{agent.name} synthesizes other perspectives as {role.value}]\"\n",
    "                \n",
    "                message = ChatMessage(\n",
    "                    sender=agent.name,\n",
    "                    content=response_content,\n",
    "                    role=role,\n",
    "                    confidence=0.9,\n",
    "                    metadata={\"round\": round_num, \"topic\": topic, \"type\": \"synthesis\"}\n",
    "                )\n",
    "                \n",
    "                discussion_messages.append(message)\n",
    "                self.conversation_history.append(message)\n",
    "                \n",
    "                print(f\"\\nðŸ¤– {agent.name} (synthesis):\")\n",
    "                print(f\"   {response_content[:150]}...\")\n",
    "        \n",
    "        print(f\"\\nðŸŽ‰ Group discussion completed! Generated {len(discussion_messages)} messages\")\n",
    "        return discussion_messages\n",
    "    \n",
    "    def _create_role_prompt(self, topic: str, role: AgentRole, context: str) -> str:\n",
    "        \"\"\"Create a role-specific prompt for agents.\"\"\"\n",
    "        base_prompt = f\"Topic for discussion: {topic}\\n\\n{context}\\n\\n\"\n",
    "        \n",
    "        role_instructions = {\n",
    "            AgentRole.FACILITATOR: \"As a facilitator, guide the discussion and ask clarifying questions.\",\n",
    "            AgentRole.EXPERT: \"As an expert, provide detailed knowledge and technical insights.\",\n",
    "            AgentRole.CRITIC: \"As a critic, identify potential issues, limitations, and challenges.\",\n",
    "            AgentRole.SYNTHESIZER: \"As a synthesizer, find connections and combine different viewpoints.\",\n",
    "            AgentRole.CREATIVE: \"As a creative thinker, propose innovative and unconventional ideas.\"\n",
    "        }\n",
    "        \n",
    "        return base_prompt + role_instructions.get(role, \"Contribute your perspective on this topic.\")\n",
    "    \n",
    "    def _create_synthesis_prompt(self, topic: str, role: AgentRole, previous_messages: List[ChatMessage]) -> str:\n",
    "        \"\"\"Create a prompt for synthesizing previous contributions.\"\"\"\n",
    "        context = f\"Topic: {topic}\\n\\nPrevious contributions:\\n\"\n",
    "        for msg in previous_messages:\n",
    "            context += f\"- {msg.sender}: {msg.content[:100]}...\\n\"\n",
    "        \n",
    "        return context + f\"\\n\\nAs a {role.value}, build upon these ideas and add your synthesis:\"\n",
    "    \n",
    "    def get_discussion_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get a comprehensive summary of the group discussion.\"\"\"\n",
    "        return {\n",
    "            \"conversation_id\": self.conversation_id,\n",
    "            \"total_messages\": len(self.conversation_history),\n",
    "            \"participants\": {agent_id: info[\"message_count\"] for agent_id, info in self.agents.items()},\n",
    "            \"topics_discussed\": list(self.active_topics),\n",
    "            \"duration\": f\"{len(self.conversation_history)} message exchanges\"\n",
    "        }\n",
    "\n",
    "# Create the group chat orchestrator\n",
    "print(\"ðŸ’¬ Initializing Group Chat Orchestrator...\")\n",
    "orchestrator = GroupChatOrchestrator()\n",
    "\n",
    "# Add our existing agents with different roles\n",
    "orchestrator.add_agent(genetic_agent, AgentRole.CREATIVE)\n",
    "orchestrator.add_agent(foundry_agent, AgentRole.EXPERT)\n",
    "\n",
    "# Create additional specialized agents for the group\n",
    "class SpecializedAgent:\n",
    "    \"\"\"A simple specialized agent for group chat demonstration.\"\"\"\n",
    "    def __init__(self, name: str, specialty: str):\n",
    "        self.name = name\n",
    "        self.specialty = specialty\n",
    "    \n",
    "    async def process_message(self, prompt: str) -> str:\n",
    "        return f\"[{self.name}, specialized in {self.specialty}, would provide expert insights on: {prompt[:50]}...]\"\n",
    "\n",
    "# Add specialized agents\n",
    "critic_agent = SpecializedAgent(\"CriticBot\", \"identifying potential issues\")\n",
    "facilitator_agent = SpecializedAgent(\"FacilitatorBot\", \"guiding discussions\")\n",
    "\n",
    "orchestrator.add_agent(critic_agent, AgentRole.CRITIC)\n",
    "orchestrator.add_agent(facilitator_agent, AgentRole.FACILITATOR)\n",
    "\n",
    "print(\"âœ… Group Chat System ready with 4 agents!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f7e4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸŽ® Step 3.2: Run Group Chat Discussion\n",
    "# Watch multiple agents collaborate on complex topics\n",
    "\n",
    "async def run_group_chat_demo():\n",
    "    \"\"\"Run a demonstration of the group chat system.\"\"\"\n",
    "    \n",
    "    discussion_topics = [\n",
    "        \"How can AI improve healthcare outcomes?\",\n",
    "        \"What are the ethical considerations in autonomous vehicles?\",\n",
    "        \"How should companies approach AI adoption?\"\n",
    "    ]\n",
    "    \n",
    "    print(\"ðŸŽ® Group Chat System Demonstration\")\n",
    "    print(\"ðŸ¤– Participants:\")\n",
    "    for agent_id, info in orchestrator.agents.items():\n",
    "        print(f\"   â€¢ {info['agent'].name} ({info['role'].value})\")\n",
    "    print()\n",
    "    \n",
    "    # Run discussion on the first topic\n",
    "    selected_topic = discussion_topics[0]\n",
    "    \n",
    "    print(f\"ðŸŽ¯ Discussion Topic: {selected_topic}\")\n",
    "    print(\"ðŸ”„ Starting collaborative discussion...\")\n",
    "    print()\n",
    "    \n",
    "    # Facilitate the discussion\n",
    "    messages = await orchestrator.facilitate_discussion(\n",
    "        topic=selected_topic,\n",
    "        max_rounds=2  # Keep it manageable for demo\n",
    "    )\n",
    "    \n",
    "    # Show summary\n",
    "    print(\"\\nðŸ“Š Discussion Summary:\")\n",
    "    summary = orchestrator.get_discussion_summary()\n",
    "    for key, value in summary.items():\n",
    "        print(f\"   {key.replace('_', ' ').title()}: {value}\")\n",
    "    \n",
    "    print(\"\\n\udca1 Key Benefits Demonstrated:\")\n",
    "    print(\"   âœ… Multiple perspectives on complex topics\")\n",
    "    print(\"   âœ… Role-based specialization\")\n",
    "    print(\"   âœ… Structured conversation flow\")\n",
    "    print(\"   âœ… Building upon others' ideas\")\n",
    "    \n",
    "    return messages\n",
    "\n",
    "# Run the group chat demonstration\n",
    "discussion_results = await run_group_chat_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73399670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”¬ Step 3.3: Advanced Group Chat Features\n",
    "# Explore sophisticated collaboration patterns\n",
    "\n",
    "def analyze_group_dynamics():\n",
    "    \"\"\"Analyze the dynamics of the group chat discussion.\"\"\"\n",
    "    \n",
    "    print(\"ðŸ”¬ Group Chat Analysis\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    if not orchestrator.conversation_history:\n",
    "        print(\"âš ï¸ No conversation history available. Run the group chat demo first.\")\n",
    "        return\n",
    "    \n",
    "    # Analyze participation patterns\n",
    "    participation = {}\n",
    "    role_distribution = {}\n",
    "    \n",
    "    for message in orchestrator.conversation_history:\n",
    "        # Count messages per sender\n",
    "        if message.sender not in participation:\n",
    "            participation[message.sender] = 0\n",
    "        participation[message.sender] += 1\n",
    "        \n",
    "        # Count messages per role\n",
    "        role = message.role.value\n",
    "        if role not in role_distribution:\n",
    "            role_distribution[role] = 0\n",
    "        role_distribution[role] += 1\n",
    "    \n",
    "    print(\"ðŸ‘¥ Participation Analysis:\")\n",
    "    for agent, count in participation.items():\n",
    "        percentage = (count / len(orchestrator.conversation_history)) * 100\n",
    "        print(f\"   {agent}: {count} messages ({percentage:.1f}%)\")\n",
    "    \n",
    "    print(\"\\nðŸŽ­ Role Distribution:\")\n",
    "    for role, count in role_distribution.items():\n",
    "        percentage = (count / len(orchestrator.conversation_history)) * 100\n",
    "        print(f\"   {role.title()}: {count} messages ({percentage:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Conversation Metrics:\")\n",
    "    print(f\"   Total Messages: {len(orchestrator.conversation_history)}\")\n",
    "    print(f\"   Unique Participants: {len(participation)}\")\n",
    "    print(f\"   Role Types: {len(role_distribution)}\")\n",
    "    print(f\"   Active Topics: {len(orchestrator.active_topics)}\")\n",
    "\n",
    "def show_collaboration_patterns():\n",
    "    \"\"\"Demonstrate different collaboration patterns possible with the system.\"\"\"\n",
    "    \n",
    "    patterns = {\n",
    "        \"ðŸŽ¯ Expert Consultation\": \"Bring in specialists for specific domain knowledge\",\n",
    "        \"ðŸ¤” Devil's Advocate\": \"Use critics to challenge ideas and find weaknesses\", \n",
    "        \"ðŸ”„ Iterative Refinement\": \"Multiple rounds to polish and improve solutions\",\n",
    "        \"ðŸ§© Parallel Processing\": \"Different agents work on different aspects simultaneously\",\n",
    "        \"âš–ï¸ Consensus Building\": \"Facilitators help find common ground between viewpoints\",\n",
    "        \"ðŸ’¡ Creative Brainstorming\": \"Creative agents generate innovative ideas\",\n",
    "        \"ðŸ“‹ Structured Analysis\": \"Systematic evaluation of complex problems\"\n",
    "    }\n",
    "    \n",
    "    print(\"ðŸŒŸ Advanced Collaboration Patterns:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for pattern, description in patterns.items():\n",
    "        print(f\"{pattern}: {description}\")\n",
    "    \n",
    "    print(\"\\nðŸš€ Next Steps for Production:\")\n",
    "    production_features = [\n",
    "        \"ðŸ” User authentication and permissions\",\n",
    "        \"ðŸ’¾ Persistent conversation storage\", \n",
    "        \"ðŸ”§ Custom agent creation tools\",\n",
    "        \"ðŸ“ˆ Advanced analytics and insights\",\n",
    "        \"ðŸŒ Real-time web interface\",\n",
    "        \"ðŸ¤– Integration with external APIs\",\n",
    "        \"âš¡ Performance optimization for scale\"\n",
    "    ]\n",
    "    \n",
    "    for feature in production_features:\n",
    "        print(f\"   {feature}\")\n",
    "\n",
    "# Run the analysis\n",
    "analyze_group_dynamics()\n",
    "print()\n",
    "show_collaboration_patterns()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b21769",
   "metadata": {},
   "source": [
    "# ðŸŽ‰ Workshop Complete: You're Now an AI Agent Expert!\n",
    "\n",
    "Congratulations! You've just completed an incredible journey through the world of AI agents, from simple to sophisticated systems.\n",
    "\n",
    "## ðŸ† What You've Accomplished\n",
    "\n",
    "**âœ… Built a Genetic Agent** - Your first adaptive AI with evolutionary capabilities\n",
    "**âœ… Mastered Azure AI Foundry** - Professional-grade cloud AI with intelligent capability routing  \n",
    "**âœ… Created Group Chat Systems** - Advanced multi-agent collaboration with orchestration\n",
    "\n",
    "## \ude80 Your New Superpowers\n",
    "\n",
    "You now have the knowledge to build production-ready AI systems that can:\n",
    "- ðŸ§¬ Adapt and evolve based on interactions\n",
    "- â˜ï¸ Leverage enterprise-scale cloud AI services\n",
    "- \udcac Orchestrate complex multi-agent collaborations\n",
    "- ðŸŽ¯ Route requests to specialized capabilities\n",
    "- ðŸ“Š Analyze and optimize agent performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8813eaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸŽ¯ Final Challenge: Build Your Own Agent\n",
    "# Apply what you've learned to create a custom agent\n",
    "\n",
    "def design_your_agent():\n",
    "    \"\"\"Guide for designing a custom agent based on workshop learnings.\"\"\"\n",
    "    \n",
    "    print(\"ðŸŽ¯ Design Your Own Agent Challenge!\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    agent_ideas = {\n",
    "        \"ðŸ¥ HealthBot\": {\n",
    "            \"description\": \"Medical assistant with specialist routing\",\n",
    "            \"capabilities\": [\"symptom analysis\", \"specialist referral\", \"health education\"],\n",
    "            \"roles\": [\"diagnostician\", \"educator\", \"coordinator\"]\n",
    "        },\n",
    "        \"ðŸ“š StudyBuddy\": {\n",
    "            \"description\": \"Educational agent with adaptive learning\",\n",
    "            \"capabilities\": [\"concept explanation\", \"quiz generation\", \"progress tracking\"],\n",
    "            \"roles\": [\"tutor\", \"motivator\", \"assessor\"]\n",
    "        },\n",
    "        \"ðŸ’¼ BusinessAnalyst\": {\n",
    "            \"description\": \"Business intelligence with market analysis\",\n",
    "            \"capabilities\": [\"data analysis\", \"trend prediction\", \"strategy recommendation\"],\n",
    "            \"roles\": [\"analyst\", \"predictor\", \"advisor\"]\n",
    "        },\n",
    "        \"ðŸŽ® GameMaster\": {\n",
    "            \"description\": \"Interactive storytelling with dynamic narratives\",\n",
    "            \"capabilities\": [\"story generation\", \"character development\", \"choice consequences\"],\n",
    "            \"roles\": [\"narrator\", \"character\", \"world-builder\"]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(\"ðŸ’¡ Agent Ideas to Inspire You:\")\n",
    "    for name, details in agent_ideas.items():\n",
    "        print(f\"\\n{name}: {details['description']}\")\n",
    "        print(f\"   Capabilities: {', '.join(details['capabilities'])}\")\n",
    "        print(f\"   Roles: {', '.join(details['roles'])}\")\n",
    "    \n",
    "    print(\"\\nðŸ› ï¸ Your Agent Design Framework:\")\n",
    "    framework = [\n",
    "        \"1. ðŸŽ¯ Define Purpose: What problem does your agent solve?\",\n",
    "        \"2. ðŸ§  Choose Capabilities: What types of responses does it need?\",\n",
    "        \"3. ðŸŽ­ Assign Roles: How will it behave in different contexts?\",\n",
    "        \"4. ðŸ“ Create Prompts: What instructions guide its behavior?\",\n",
    "        \"5. ðŸ§ª Test & Iterate: How will you validate and improve it?\"\n",
    "    ]\n",
    "    \n",
    "    for step in framework:\n",
    "        print(f\"   {step}\")\n",
    "    \n",
    "    print(\"\\nðŸš€ Implementation Tips:\")\n",
    "    tips = [\n",
    "        \"Start simple and add complexity gradually\",\n",
    "        \"Use the patterns from this workshop as templates\", \n",
    "        \"Test with diverse scenarios to find edge cases\",\n",
    "        \"Consider combining genetic evolution with foundry capabilities\",\n",
    "        \"Design for group collaboration if relevant\"\n",
    "    ]\n",
    "    \n",
    "    for tip in tips:\n",
    "        print(f\"   â€¢ {tip}\")\n",
    "\n",
    "design_your_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706a5c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸŒŸ Next Steps: Advanced Learning Resources\n",
    "# Continue your AI agent journey\n",
    "\n",
    "def show_learning_path():\n",
    "    \"\"\"Display next steps for continued learning.\"\"\"\n",
    "    \n",
    "    print(\"ðŸŒŸ Continue Your AI Agent Journey\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    learning_tracks = {\n",
    "        \"ðŸ”¬ Deep Learning Track\": [\n",
    "            \"Study transformer architectures in detail\",\n",
    "            \"Explore fine-tuning techniques for domain-specific agents\", \n",
    "            \"Learn about reinforcement learning for agent optimization\",\n",
    "            \"Investigate multi-modal AI (text, vision, audio)\"\n",
    "        ],\n",
    "        \"â˜ï¸ Production Track\": [\n",
    "            \"Master Azure AI services integration\",\n",
    "            \"Learn containerization and deployment strategies\",\n",
    "            \"Study load balancing and scaling patterns\",\n",
    "            \"Implement monitoring and observability\"\n",
    "        ],\n",
    "        \"ðŸ¤ Collaboration Track\": [\n",
    "            \"Advanced multi-agent coordination protocols\",\n",
    "            \"Consensus mechanisms and conflict resolution\",\n",
    "            \"Distributed agent architectures\",\n",
    "            \"Human-AI collaboration patterns\"\n",
    "        ],\n",
    "        \"ðŸŽ¯ Specialization Track\": [\n",
    "            \"Domain-specific agent development\",\n",
    "            \"Custom training data preparation\",\n",
    "            \"Evaluation metrics and benchmarking\",\n",
    "            \"Ethical AI and bias mitigation\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    for track, topics in learning_tracks.items():\n",
    "        print(f\"\\n{track}:\")\n",
    "        for topic in topics:\n",
    "            print(f\"   â€¢ {topic}\")\n",
    "    \n",
    "    print(\"\\nðŸ”— Recommended Resources:\")\n",
    "    resources = [\n",
    "        \"ðŸ“– LangChain Documentation: Comprehensive guides and examples\",\n",
    "        \"â˜ï¸ Azure AI Documentation: Enterprise AI implementation\",\n",
    "        \"ðŸŽ“ AI Research Papers: Latest developments in agent systems\",\n",
    "        \"ðŸ’» Open Source Projects: Real-world agent implementations\",\n",
    "        \"ðŸ›ï¸ Academic Courses: Formal education in AI/ML\",\n",
    "        \"\udc65 Developer Communities: Connect with other AI practitioners\"\n",
    "    ]\n",
    "    \n",
    "    for resource in resources:\n",
    "        print(f\"   {resource}\")\n",
    "\n",
    "show_learning_path()\n",
    "\n",
    "print(\"\\nðŸŽ‰ Thank you for completing the LangChain Agents Workshop!\")\n",
    "print(\"ðŸš€ You're now equipped to build amazing AI systems!\")\n",
    "print(\"ðŸ’« Go forth and create intelligent agents that make the world better!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5410a5f2",
   "metadata": {},
   "source": [
    "## Section 6: Create Azure AI Foundry Agent\n",
    "\n",
    "ðŸŽ‰ **The Grand Finale!** Let's create a production-ready agent using Azure AI Foundry. This agent will have:\n",
    "\n",
    "- ðŸ¢ **Enterprise security**: Managed identity and secure connections\n",
    "- ðŸ“Š **Advanced monitoring**: Built-in analytics and logging  \n",
    "- ðŸš€ **Production features**: Scalability and reliability\n",
    "- ðŸ”§ **Rich capabilities**: Advanced reasoning and tool usage\n",
    "\n",
    "### ðŸŽ¯ Exercise 4: Build Your Azure AI Foundry Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4950cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Azure AI Foundry agent configuration\n",
    "foundry_agent_config = AgentConfig(\n",
    "    name=\"workshop_foundry_agent\",\n",
    "    agent_type=\"azure_foundry\",\n",
    "    enabled=True,\n",
    "    instructions=\"\"\"You are an advanced AI agent powered by Azure AI Foundry, designed for enterprise-grade applications.\n",
    "\n",
    "ENTERPRISE CAPABILITIES:\n",
    "- Advanced reasoning and problem-solving\n",
    "- Integration with Azure ecosystem\n",
    "- Built-in security and compliance\n",
    "- Production-ready scalability\n",
    "- Comprehensive monitoring and analytics\n",
    "\n",
    "WORKSHOP ROLE:\n",
    "- Demonstrate enterprise AI capabilities\n",
    "- Explain Azure AI Foundry benefits\n",
    "- Provide production-ready examples\n",
    "- Show integration possibilities\n",
    "\n",
    "RESPONSE STYLE:\n",
    "- Professional yet approachable\n",
    "- Include technical details when relevant\n",
    "- Highlight enterprise features\n",
    "- Provide actionable insights\n",
    "- Use examples from real-world scenarios\"\"\",\n",
    "    metadata={\n",
    "        \"description\": \"Production-ready Azure AI Foundry agent\",\n",
    "        \"capabilities\": [\n",
    "            \"enterprise_reasoning\",\n",
    "            \"azure_integration\", \n",
    "            \"security_compliance\",\n",
    "            \"production_monitoring\",\n",
    "            \"advanced_analytics\",\n",
    "            \"scalable_deployment\"\n",
    "        ],\n",
    "        \"workshop_level\": \"advanced\",\n",
    "        \"environment\": \"azure_foundry\"\n",
    "    },\n",
    "    framework_config={\n",
    "        \"provider\": \"azure_foundry\",\n",
    "        \"model\": \"gpt-4o\",\n",
    "        \"temperature\": 0.6,  # Balanced for enterprise use\n",
    "        \"max_tokens\": 1200,  # Detailed enterprise responses\n",
    "        \"endpoint\": os.getenv(\"PROJECT_ENDPOINT\"),\n",
    "        \"use_managed_identity\": True,  # Enterprise security\n",
    "        \"enable_monitoring\": True,     # Production monitoring\n",
    "        \"enable_analytics\": True       # Usage analytics\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"ðŸ¢ Azure AI Foundry Agent Configuration Created!\")\n",
    "print(\"ðŸ”‘ Key enterprise features:\")\n",
    "print(f\"  âœ… Managed Identity: {foundry_agent_config.framework_config.get('use_managed_identity')}\")\n",
    "print(f\"  âœ… Monitoring: {foundry_agent_config.framework_config.get('enable_monitoring')}\")\n",
    "print(f\"  âœ… Analytics: {foundry_agent_config.framework_config.get('enable_analytics')}\")\n",
    "print(f\"  âœ… Provider: {foundry_agent_config.framework_config.get('provider')}\")\n",
    "print(f\"  âœ… Endpoint: {foundry_agent_config.framework_config.get('endpoint')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c79d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and initialize the Azure AI Foundry agent\n",
    "try:\n",
    "    # Use our LangChain Azure Foundry agent implementation\n",
    "    foundry_agent = LangChainAzureFoundryAgent(foundry_agent_config)\n",
    "    await foundry_agent.initialize()\n",
    "    \n",
    "    print(\"ðŸš€ Azure AI Foundry Agent Created Successfully!\")\n",
    "    \n",
    "    # Register in our agent registry\n",
    "    agent_registry.register_agent(\"foundry\", foundry_agent)\n",
    "    \n",
    "    print(f\"ðŸ“‹ Agent Registry now contains: {agent_registry.get_all_agents()}\")\n",
    "    print(f\"ðŸŽ¯ Foundry agent capabilities: {foundry_agent.get_capabilities()}\")\n",
    "    \n",
    "    # Show enterprise features\n",
    "    print(\"\\nðŸ¢ Enterprise Features Enabled:\")\n",
    "    print(\"  âœ… Secure authentication with managed identity\")\n",
    "    print(\"  âœ… Built-in request/response monitoring\")\n",
    "    print(\"  âœ… Automatic retry logic with exponential backoff\")\n",
    "    print(\"  âœ… Integration with Azure security services\")\n",
    "    print(\"  âœ… Compliance and governance features\")\n",
    "    print(\"  âœ… Production-ready scalability\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error creating Azure AI Foundry agent: {e}\")\n",
    "    print(\"This might happen if Azure AI Foundry isn't fully configured\")\n",
    "    print(\"But we can still demonstrate the configuration approach!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb1464f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the Azure AI Foundry agent\n",
    "print(\"ðŸ§ª Testing Azure AI Foundry Agent...\")\n",
    "\n",
    "if 'foundry_agent' in locals():\n",
    "    # Test enterprise features\n",
    "    enterprise_tests = [\n",
    "        \"What are the key benefits of using Azure AI Foundry for enterprise AI applications?\",\n",
    "        \"How does managed identity improve security in AI applications?\",\n",
    "        \"Can you explain the monitoring and analytics capabilities you provide?\",\n",
    "        \"What makes you different from the basic agents we created earlier?\"\n",
    "    ]\n",
    "    \n",
    "    for i, test_message in enumerate(enterprise_tests, 1):\n",
    "        print(f\"\\nðŸ”¬ Test {i}/{len(enterprise_tests)}\")\n",
    "        try:\n",
    "            response = await foundry_agent.process_message(test_message, [], {\n",
    "                \"test_id\": f\"enterprise_test_{i}\",\n",
    "                \"workshop_session\": \"langchain_foundry\"\n",
    "            })\n",
    "            \n",
    "            print(f\"ðŸ’¬ Question: {test_message}\")\n",
    "            print(f\"ðŸ¢ Foundry Agent: {response.content}\")\n",
    "            print(f\"ðŸ“Š Enterprise Metadata: {response.metadata}\")\n",
    "            print(\"-\" * 80)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Test {i} failed: {e}\")\n",
    "            \n",
    "else:\n",
    "    print(\"âš ï¸ Foundry agent not available for testing\")\n",
    "    print(\"In a real environment, this would demonstrate:\")\n",
    "    print(\"  - Advanced reasoning capabilities\")\n",
    "    print(\"  - Enterprise security features\")\n",
    "    print(\"  - Built-in monitoring and analytics\")\n",
    "    print(\"  - Production-ready performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abfd2dd",
   "metadata": {},
   "source": [
    "## Section 7: Compare Agent Performances\n",
    "\n",
    "ðŸ† **Time for the Grand Comparison!** Let's compare all the agents we've built and see how they perform on the same tasks.\n",
    "\n",
    "This section will help you understand:\n",
    "- The evolution from basic to enterprise agents\n",
    "- Performance differences between implementations  \n",
    "- When to use each type of agent\n",
    "- Real-world application scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1573c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive agent comparison\n",
    "import time\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "async def compare_agents(test_message: str, agents_dict: Dict[str, IAgent]) -> Dict[str, Dict]:\n",
    "    \"\"\"Compare multiple agents on the same task.\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    print(f\"ðŸ”¬ Testing all agents with: '{test_message}'\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for agent_name, agent in agents_dict.items():\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Test the agent\n",
    "            response = await agent.process_message(test_message, [], {\n",
    "                \"comparison_test\": True,\n",
    "                \"agent_name\": agent_name\n",
    "            })\n",
    "            \n",
    "            end_time = time.time()\n",
    "            response_time = round((end_time - start_time) * 1000, 2)  # Convert to milliseconds\n",
    "            \n",
    "            # Store results\n",
    "            results[agent_name] = {\n",
    "                \"response\": response.content,\n",
    "                \"response_time_ms\": response_time,\n",
    "                \"capabilities\": agent.get_capabilities(),\n",
    "                \"metadata\": response.metadata,\n",
    "                \"success\": True\n",
    "            }\n",
    "            \n",
    "            # Display results\n",
    "            print(f\"ðŸ¤– {agent_name.upper()} AGENT:\")\n",
    "            print(f\"   Response Time: {response_time}ms\")\n",
    "            print(f\"   Response: {response.content[:150]}{'...' if len(response.content) > 150 else ''}\")\n",
    "            print(f\"   Capabilities: {agent.get_capabilities()}\")\n",
    "            print(\"-\" * 60)\n",
    "            \n",
    "        except Exception as e:\n",
    "            results[agent_name] = {\n",
    "                \"error\": str(e),\n",
    "                \"success\": False,\n",
    "                \"response_time_ms\": 0\n",
    "            }\n",
    "            print(f\"âŒ {agent_name.upper()} AGENT: Error - {e}\")\n",
    "            print(\"-\" * 60)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Prepare agents for comparison\n",
    "agents_to_compare = {}\n",
    "\n",
    "# Add available agents\n",
    "if 'basic_agent' in locals():\n",
    "    agents_to_compare[\"basic\"] = basic_agent\n",
    "    \n",
    "if 'enhanced_agent' in locals():\n",
    "    agents_to_compare[\"enhanced\"] = enhanced_agent\n",
    "    \n",
    "if 'foundry_agent' in locals():\n",
    "    agents_to_compare[\"foundry\"] = foundry_agent\n",
    "\n",
    "print(f\"ðŸŽ¯ Comparing {len(agents_to_compare)} agents:\")\n",
    "for name in agents_to_compare.keys():\n",
    "    print(f\"   âœ… {name.title()} Agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a98b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Basic conversation\n",
    "print(\"ðŸ§ª TEST 1: Basic Conversation\")\n",
    "results_1 = await compare_agents(\n",
    "    \"Hello! Can you explain what makes a good AI agent?\", \n",
    "    agents_to_compare\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Test 2: Technical explanation\n",
    "print(\"ðŸ§ª TEST 2: Technical Explanation\")\n",
    "results_2 = await compare_agents(\n",
    "    \"Explain the benefits of using dependency injection in software architecture.\", \n",
    "    agents_to_compare\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Test 3: Enterprise scenario\n",
    "print(\"ðŸ§ª TEST 3: Enterprise Scenario\")\n",
    "results_3 = await compare_agents(\n",
    "    \"How would you design a scalable AI system for a large enterprise with security and compliance requirements?\", \n",
    "    agents_to_compare\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14c49a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance analysis\n",
    "print(\"ðŸ“Š PERFORMANCE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def analyze_results(test_name: str, results: Dict):\n",
    "    \"\"\"Analyze and display test results.\"\"\"\n",
    "    print(f\"\\nðŸ“ˆ {test_name} Analysis:\")\n",
    "    \n",
    "    successful_agents = {k: v for k, v in results.items() if v.get('success', False)}\n",
    "    \n",
    "    if successful_agents:\n",
    "        # Response time analysis\n",
    "        avg_response_time = sum(v['response_time_ms'] for v in successful_agents.values()) / len(successful_agents)\n",
    "        fastest_agent = min(successful_agents.items(), key=lambda x: x[1]['response_time_ms'])\n",
    "        \n",
    "        print(f\"   âš¡ Average response time: {avg_response_time:.2f}ms\")\n",
    "        print(f\"   ðŸƒ Fastest agent: {fastest_agent[0]} ({fastest_agent[1]['response_time_ms']}ms)\")\n",
    "        \n",
    "        # Capability analysis\n",
    "        all_capabilities = set()\n",
    "        for agent_data in successful_agents.values():\n",
    "            all_capabilities.update(agent_data.get('capabilities', []))\n",
    "        \n",
    "        print(f\"   ðŸŽ¯ Total unique capabilities: {len(all_capabilities)}\")\n",
    "        print(f\"   ðŸ“‹ Capabilities: {', '.join(sorted(all_capabilities))}\")\n",
    "        \n",
    "        # Response quality (length as a proxy)\n",
    "        response_lengths = {k: len(v['response']) for k, v in successful_agents.items()}\n",
    "        most_detailed = max(response_lengths.items(), key=lambda x: x[1])\n",
    "        \n",
    "        print(f\"   ðŸ“ Most detailed response: {most_detailed[0]} ({most_detailed[1]} characters)\")\n",
    "    \n",
    "    else:\n",
    "        print(\"   âŒ No successful responses for this test\")\n",
    "\n",
    "# Analyze all tests\n",
    "if 'results_1' in locals():\n",
    "    analyze_results(\"Basic Conversation\", results_1)\n",
    "    \n",
    "if 'results_2' in locals():\n",
    "    analyze_results(\"Technical Explanation\", results_2)\n",
    "    \n",
    "if 'results_3' in locals():\n",
    "    analyze_results(\"Enterprise Scenario\", results_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b39e37",
   "metadata": {},
   "source": [
    "## ðŸŽ‰ Congratulations! Workshop Complete!\n",
    "\n",
    "You've successfully completed the LangChain Agents Workshop! Here's what you've accomplished:\n",
    "\n",
    "### âœ… **What You've Built:**\n",
    "1. **Basic Generic Agent** - Simple conversational AI\n",
    "2. **Enhanced Agent** - With memory and advanced capabilities  \n",
    "3. **Azure AI Foundry Agent** - Enterprise-ready with security and monitoring\n",
    "\n",
    "### ðŸŽ¯ **Key Learnings:**\n",
    "- **Modern Architecture**: Plugin-based, extensible design\n",
    "- **Configuration-Driven**: Easy to modify and deploy\n",
    "- **Security Best Practices**: Using managed identity and secure connections\n",
    "- **Enterprise Features**: Monitoring, analytics, and scalability\n",
    "\n",
    "### ðŸš€ **Next Steps:**\n",
    "1. **Experiment**: Try different configurations and instructions\n",
    "2. **Extend**: Add custom tools and capabilities to your agents\n",
    "3. **Deploy**: Use Azure AI Foundry for production deployment\n",
    "4. **Monitor**: Implement logging and analytics for your agents\n",
    "\n",
    "### ðŸ“š **Resources:**\n",
    "- [Azure AI Foundry Documentation](https://docs.microsoft.com/azure/ai-foundry/)\n",
    "- [LangChain Documentation](https://python.langchain.com/)\n",
    "- [Modern Agent Architecture Guide](../../README.md)\n",
    "- [Configuration Examples](../../examples/)\n",
    "\n",
    "### ðŸ¤ **Questions & Discussion:**\n",
    "What questions do you have about building and deploying AI agents?\n",
    "\n",
    "**Thank you for participating in this workshop!** ðŸŽŠ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26c8fd2",
   "metadata": {},
   "source": [
    "## Section 7: LangChain vs Semantic Kernel - Framework Comparison\n",
    "\n",
    "Now that you've experienced both workshops, let's compare the frameworks to help you choose the right one for your projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46a4e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_framework_comparison():\n",
    "    \"\"\"\n",
    "    Create a comprehensive comparison between LangChain and Semantic Kernel.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"ðŸ†š LANGCHAIN vs SEMANTIC KERNEL COMPARISON\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Framework comparison matrix\n",
    "    comparison_data = {\n",
    "        \"Aspect\": [\n",
    "            \"Architecture\", \"Learning Curve\", \"Tool Ecosystem\", \"Memory Management\",\n",
    "            \"Multi-Provider Support\", \"Enterprise Features\", \"Community Size\",\n",
    "            \"Microsoft Integration\", \"Flexibility\", \"Performance\", \n",
    "            \"Documentation\", \"Production Readiness\"\n",
    "        ],\n",
    "        \"LangChain\": [\n",
    "            \"Chain-based\", \"Moderate\", \"Extensive\", \"Advanced\",\n",
    "            \"Excellent\", \"Good\", \"Large\",\n",
    "            \"Good\", \"Very High\", \"Good\",\n",
    "            \"Excellent\", \"Mature\"\n",
    "        ],\n",
    "        \"Semantic Kernel\": [\n",
    "            \"Plugin-based\", \"Easy\", \"Growing\", \"Basic\",\n",
    "            \"Good\", \"Excellent\", \"Medium\",\n",
    "            \"Native\", \"High\", \"Optimized\",\n",
    "            \"Good\", \"Enterprise-Ready\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    print(\"\\\\nðŸ“Š DETAILED COMPARISON\")\n",
    "    print(\"-\" * 25)\n",
    "    \n",
    "    # Print comparison table\n",
    "    col_widths = [20, 15, 18]\n",
    "    headers = [\"Aspect\", \"LangChain\", \"Semantic Kernel\"]\n",
    "    \n",
    "    # Print header\n",
    "    header_row = \"\"\n",
    "    for i, header in enumerate(headers):\n",
    "        header_row += f\"{header:<{col_widths[i]}}\"\n",
    "    print(header_row)\n",
    "    print(\"-\" * sum(col_widths))\n",
    "    \n",
    "    # Print rows\n",
    "    for i, aspect in enumerate(comparison_data[\"Aspect\"]):\n",
    "        row = f\"{aspect:<{col_widths[0]}}\"\n",
    "        row += f\"{comparison_data['LangChain'][i]:<{col_widths[1]}}\"\n",
    "        row += f\"{comparison_data['Semantic Kernel'][i]:<{col_widths[2]}}\"\n",
    "        print(row)\n",
    "    \n",
    "    print(\"\\\\nðŸŽ¯ WHEN TO CHOOSE LANGCHAIN\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    langchain_use_cases = [\n",
    "        \"ðŸ”— Complex chain orchestration and workflows\",\n",
    "        \"ðŸ› ï¸ Need extensive pre-built tool integrations\",\n",
    "        \"ðŸ§  Advanced memory and retrieval requirements\",\n",
    "        \"ðŸŒ Multi-provider flexibility is critical\",\n",
    "        \"ðŸ“š Rich documentation and community support needed\",\n",
    "        \"ðŸ”„ Rapid prototyping with diverse components\",\n",
    "        \"ðŸ Python-first development approach\"\n",
    "    ]\n",
    "    \n",
    "    for use_case in langchain_use_cases:\n",
    "        print(f\"   {use_case}\")\n",
    "    \n",
    "    print(\"\\\\nðŸŽ¯ WHEN TO CHOOSE SEMANTIC KERNEL\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    sk_use_cases = [\n",
    "        \"ðŸ¢ Enterprise Microsoft environment\",\n",
    "        \"ðŸš€ Quick start with minimal learning curve\",\n",
    "        \"ðŸ”Œ Plugin-based extensibility preferred\",\n",
    "        \"âš¡ Performance optimization important\",\n",
    "        \"ðŸ›¡ï¸ Enterprise security and compliance focus\",\n",
    "        \"ðŸ”— Native Azure integration required\",\n",
    "        \"ðŸŽ¯ Simpler, more focused agent requirements\"\n",
    "    ]\n",
    "    \n",
    "    for use_case in sk_use_cases:\n",
    "        print(f\"   {use_case}\")\n",
    "    \n",
    "    print(\"\\\\nðŸ¤ HYBRID APPROACH\")\n",
    "    print(\"-\" * 17)\n",
    "    print(\"ðŸ”„ You can use both frameworks in the same project!\")\n",
    "    print(\"   â€¢ LangChain for complex workflows and tools\")\n",
    "    print(\"   â€¢ Semantic Kernel for Microsoft-integrated components\")\n",
    "    print(\"   â€¢ Choose based on specific use case requirements\")\n",
    "    \n",
    "    print(\"\\\\nðŸŽ“ LEARNING RECOMMENDATION\")\n",
    "    print(\"-\" * 25)\n",
    "    print(\"ðŸ“š Start with: Semantic Kernel (easier learning curve)\")\n",
    "    print(\"ðŸ”„ Then explore: LangChain (for advanced capabilities)\")\n",
    "    print(\"ðŸŽ¯ Choose based on: Your specific project needs\")\n",
    "    print(\"ðŸ’¡ Remember: Both are excellent frameworks!\")\n",
    "    \n",
    "    return comparison_data\n",
    "\n",
    "# Generate the comparison\n",
    "comparison_results = create_framework_comparison()\n",
    "\n",
    "print(\"\\\\nâœ¨ Framework comparison complete!\")\n",
    "print(\"ðŸŽ¯ Now you can make informed decisions about which framework to use!\")\n",
    "print(\"ðŸš€ Both workshops completed - you're ready for production AI agents!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371099d0",
   "metadata": {},
   "source": [
    "## Optional utilities\n",
    "\n",
    "Use these helpers if you need to:\n",
    "- Reset or clean up your environment variables and optional .env file\n",
    "- Start the LangChain FastAPI server (uvicorn) from the notebook\n",
    "- Stop the server safely on Windows\n",
    "\n",
    "These are optional and independent from the Quick Start steps above. If you don't need them, you can ignore this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bee6f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility: Reset config (.env and in-memory)\n",
    "import os, json, shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Toggle deletion of .env file created in Step 2\n",
    "DELETE_ENV_FILE = False  # set True to remove .env\n",
    "\n",
    "# Env vars used by this LangChain app\n",
    "_ENV_KEYS = [\n",
    "    \"AZURE_INFERENCE_ENDPOINT\",\"AZURE_INFERENCE_CREDENTIAL\",\"GENERIC_MODEL\",\n",
    "    \"PROJECT_ENDPOINT\",\"PEOPLE_AGENT_ID\",\"KNOWLEDGE_AGENT_ID\",\n",
    "    \"FRONTEND_URL\",\"LOG_LEVEL\",\"ENVIRONMENT\",\n",
    "    \"SESSION_STORAGE_TYPE\",\"SESSION_STORAGE_PATH\",\"REDIS_URL\",\n",
    "    \"DEBUG_LOGS\",\"CONFIG_PATH\"\n",
    " ]\n",
    "\n",
    "def _mask(v):\n",
    "    if v is None:\n",
    "        return \"\"\n",
    "    return v[:4] + \"***\" if len(v) > 8 else \"***\"\n",
    "\n",
    "def reset_config(delete_env: bool = False):\n",
    "    # Clear in-memory env\n",
    "    cleared = {}\n",
    "    for k in _ENV_KEYS:\n",
    "        if k in os.environ:\n",
    "            cleared[k] = os.environ.pop(k)\n",
    "    # Optionally delete .env in this folder\n",
    "    env_path = Path(\".env\")\n",
    "    removed_env_file = False\n",
    "    if delete_env and env_path.exists():\n",
    "        try:\n",
    "            env_path.unlink()\n",
    "            removed_env_file = True\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: couldn't delete .env: {e}\")\n",
    "    print(\"Cleared env keys:\")\n",
    "    for k, v in cleared.items():\n",
    "        print(f\"- {k} = { _mask(v) }\")\n",
    "    print(f\"Removed .env file: {removed_env_file}\")\n",
    "    return {\"cleared\": list(cleared.keys()), \"removed_env_file\": removed_env_file}\n",
    "\n",
    "result = reset_config(DELETE_ENV_FILE)\n",
    "print(\"Reset complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae9cb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility: Start API server (uvicorn) in background\n",
    "import os, sys, subprocess, time, json\n",
    "from pathlib import Path\n",
    "\n",
    "# Settings\n",
    "HOST = os.getenv(\"HOST\", \"127.0.0.1\")\n",
    "PORT = int(os.getenv(\"PORT\", \"8001\"))  # avoid conflict with SK if it's 8000\n",
    "RELOAD = False  # set True during local dev\n",
    "LOG_LEVEL = os.getenv(\"LOG_LEVEL\", \"info\")\n",
    "PID_FILE = Path(\".uvicorn_pid\")\n",
    "\n",
    "def start_server():\n",
    "    if PID_FILE.exists():\n",
    "        print(\"A server appears to be running already (PID file exists). If it's stale, run the Stop cell first.\")\n",
    "        return {\"status\": \"skipped\", \"reason\": \"pid_exists\"}\n",
    "    cmd = [sys.executable, \"-m\", \"uvicorn\", \"main:app\", \"--host\", HOST, \"--port\", str(PORT), \"--log-level\", LOG_LEVEL]\n",
    "    if RELOAD:\n",
    "        cmd.append(\"--reload\")\n",
    "    # On Windows, creationflags=CREATE_NEW_PROCESS_GROUP helps Ctrl+C and termination\n",
    "    creationflags = 0x00000200  # CREATE_NEW_PROCESS_GROUP\n",
    "    proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, creationflags=creationflags)\n",
    "    PID_FILE.write_text(str(proc.pid))\n",
    "    print(f\"Starting uvicorn main:app at http://{HOST}:{PORT} (pid={proc.pid})...\")\n",
    "    # Brief wait to give server time to bind\n",
    "    time.sleep(1.5)\n",
    "    return {\"status\": \"started\", \"pid\": proc.pid, \"url\": f\"http://{HOST}:{PORT}\"}\n",
    "\n",
    "result = start_server()\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f05ce21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility: Stop API server (read PID file and terminate)\n",
    "import os, signal\n",
    "from pathlib import Path\n",
    "\n",
    "PID_FILE = Path(\".uvicorn_pid\")\n",
    "\n",
    "def stop_server():\n",
    "    if not PID_FILE.exists():\n",
    "        print(\"No PID file found. If a server is running, you may need to stop it manually.\")\n",
    "        return {\"status\": \"skipped\", \"reason\": \"no_pid\"}\n",
    "    try:\n",
    "        pid = int(PID_FILE.read_text().strip())\n",
    "    except Exception as e:\n",
    "        print(f\"Couldn't read pid: {e}\")\n",
    "        return {\"status\": \"error\", \"error\": str(e)}\n",
    "    try:\n",
    "        # Windows-friendly termination: first try CTRL_BREAK_EVENT, then terminate\n",
    "        try:\n",
    "            os.kill(pid, signal.CTRL_BREAK_EVENT)\n",
    "        except Exception:\n",
    "            # Fallback to terminate\n",
    "            os.kill(pid, signal.SIGTERM)\n",
    "        print(f\"Sent termination to pid {pid}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error signaling process: {e}\")\n",
    "        return {\"status\": \"error\", \"error\": str(e)}\n",
    "    try:\n",
    "        PID_FILE.unlink()\n",
    "    except Exception:\n",
    "        pass\n",
    "    return {\"status\": \"stopped\", \"pid\": pid}\n",
    "\n",
    "result = stop_server()\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

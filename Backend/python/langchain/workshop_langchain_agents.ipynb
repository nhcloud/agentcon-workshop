{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa2dcadf",
   "metadata": {},
   "source": [
    "# üöÄ Quick Start: Run This Workshop Notebook (LangChain)\n",
    "\n",
    "This section sets up everything you need to run the agents in this notebook with minimal friction.\n",
    "\n",
    "What it does:\n",
    "- Installs Python dependencies and the shared local library\n",
    "- Lets you provide API keys (Azure AI Inference and optional providers)\n",
    "- Saves them to a local .env for reuse (optional)\n",
    "- Verifies the project structure\n",
    "- Optionally runs a tiny smoke test if keys are present\n",
    "\n",
    "Proceed top-to-bottom; each step is self-checking and safe to rerun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00aacc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 ‚Äî Install dependencies (safe to rerun)\n",
    "import os, sys, subprocess, pathlib\n",
    "\n",
    "nb_dir = pathlib.Path().resolve()\n",
    "project_root = nb_dir.parents[2] if (len(nb_dir.parents) >= 2) else nb_dir\n",
    "lc_dir = nb_dir  # this notebook lives in Backend/python/langchain\n",
    "shared_dir = lc_dir.parent / \"shared\"\n",
    "req_file = lc_dir / \"requirements.txt\"\n",
    "\n",
    "print(f\"Notebook dir: {nb_dir}\")\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Using requirements: {req_file}\")\n",
    "print(f\"Shared package dir: {shared_dir}\")\n",
    "\n",
    "def run(cmd):\n",
    "    print(\"\\n$\", cmd)\n",
    "    result = subprocess.run(cmd, shell=True, text=True)\n",
    "    if result.returncode != 0:\n",
    "        raise SystemExit(f\"Command failed with exit code {result.returncode}\")\n",
    "\n",
    "# Use pip magics if available (keeps kernel env), fallback to subprocess\n",
    "try:\n",
    "    import IPython\n",
    "    get_ipython  # noqa\n",
    "    if req_file.exists():\n",
    "        _ = get_ipython().run_line_magic(\"pip\", f\"install -r {req_file}\")\n",
    "    else:\n",
    "        print(\"requirements.txt not found; skipping dependency install.\")\n",
    "    if (shared_dir / \"setup.py\").exists():\n",
    "        _ = get_ipython().run_line_magic(\"pip\", f\"install -e {shared_dir}\")\n",
    "    else:\n",
    "        print(\"Shared library setup.py not found; skipping -e install.\")\n",
    "except Exception:\n",
    "    if req_file.exists():\n",
    "        run(f\"python -m pip install -r \\\"{req_file}\\\"\")\n",
    "    if (shared_dir / \"setup.py\").exists():\n",
    "        run(f\"python -m pip install -e \\\"{shared_dir}\\\"\")\n",
    "\n",
    "print(\"\\n‚úÖ Dependencies installation step completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999f2178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 ‚Äî Provide configuration (non-interactive, edit-and-run)\n",
    "# Edit the CONFIG values below (no prompts). Set WRITE_ENV_FILE=True to save to .env.\n",
    "import os, pathlib\n",
    "\n",
    "# Current environment defaults\n",
    "DEFAULTS = {\n",
    "    \"AZURE_INFERENCE_ENDPOINT\": os.environ.get(\"AZURE_INFERENCE_ENDPOINT\", \"\"),\n",
    "    \"AZURE_INFERENCE_CREDENTIAL\": os.environ.get(\"AZURE_INFERENCE_CREDENTIAL\", \"\"),\n",
    "    \"GENERIC_MODEL\": os.environ.get(\"GENERIC_MODEL\", \"gpt-4o-mini\"),\n",
    "    \"PROJECT_ENDPOINT\": os.environ.get(\"PROJECT_ENDPOINT\", \"\"),\n",
    "    \"PEOPLE_AGENT_ID\": os.environ.get(\"PEOPLE_AGENT_ID\", \"\"),\n",
    "    \"KNOWLEDGE_AGENT_ID\": os.environ.get(\"KNOWLEDGE_AGENT_ID\", \"\"),\n",
    "    \"ENVIRONMENT\": os.environ.get(\"ENVIRONMENT\", \"development\"),\n",
    "    \"LOG_LEVEL\": os.environ.get(\"LOG_LEVEL\", \"INFO\"),\n",
    "}\n",
    "\n",
    "# EDIT THESE VALUES AS NEEDED. Leave as-is to keep current/defaults.\n",
    "CONFIG = {\n",
    "    \"AZURE_INFERENCE_ENDPOINT\": DEFAULTS[\"AZURE_INFERENCE_ENDPOINT\"],\n",
    "    \"AZURE_INFERENCE_CREDENTIAL\": DEFAULTS[\"AZURE_INFERENCE_CREDENTIAL\"],\n",
    "    \"GENERIC_MODEL\": DEFAULTS[\"GENERIC_MODEL\"],\n",
    "    \"PROJECT_ENDPOINT\": DEFAULTS[\"PROJECT_ENDPOINT\"],\n",
    "    \"PEOPLE_AGENT_ID\": DEFAULTS[\"PEOPLE_AGENT_ID\"],\n",
    "    \"KNOWLEDGE_AGENT_ID\": DEFAULTS[\"KNOWLEDGE_AGENT_ID\"],\n",
    "    \"ENVIRONMENT\": DEFAULTS[\"ENVIRONMENT\"],\n",
    "    \"LOG_LEVEL\": DEFAULTS[\"LOG_LEVEL\"],\n",
    "}\n",
    "\n",
    "# Toggle saving to .env in this folder\n",
    "WRITE_ENV_FILE = False\n",
    "ENV_FILE_NAME = \".env\"\n",
    "\n",
    "# Apply to current process env\n",
    "for k, v in CONFIG.items():\n",
    "    if v is not None and v != \"\":\n",
    "        os.environ[k] = v\n",
    "\n",
    "# Optionally write .env\n",
    "if WRITE_ENV_FILE:\n",
    "    env_path = pathlib.Path(ENV_FILE_NAME)\n",
    "    existing = {}\n",
    "    if env_path.exists():\n",
    "        try:\n",
    "            with env_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "                for line in f:\n",
    "                    line = line.strip()\n",
    "                    if line and not line.startswith(\"#\") and \"=\" in line:\n",
    "                        key, val = line.split(\"=\", 1)\n",
    "                        existing[key] = val\n",
    "        except Exception:\n",
    "            pass\n",
    "    existing.update({k: v for k, v in CONFIG.items() if v is not None and v != \"\"})\n",
    "    env_path.write_text(\"\\n\".join(f\"{k}={v}\" for k, v in existing.items()), encoding=\"utf-8\")\n",
    "    print(f\"Wrote {env_path.resolve()} with {len(existing)} keys.\")\n",
    "else:\n",
    "    print(\"Skipped writing .env (set WRITE_ENV_FILE=True to enable). Values active for this session only.\")\n",
    "\n",
    "# Status\n",
    "mask = lambda s, keep=4: s if not s or len(s) <= keep else (s[:keep] + \"‚Ä¶\" + s[-2:])\n",
    "print(\"\\nCurrent config status:\")\n",
    "print(\"- AZURE_INFERENCE_ENDPOINT:\", bool(CONFIG[\"AZURE_INFERENCE_ENDPOINT\"]))\n",
    "print(\"- AZURE_INFERENCE_CREDENTIAL:\", mask(CONFIG[\"AZURE_INFERENCE_CREDENTIAL\"]))\n",
    "print(\"- GENERIC_MODEL:\", CONFIG[\"GENERIC_MODEL\"])\n",
    "print(\"- PROJECT_ENDPOINT:\", bool(CONFIG[\"PROJECT_ENDPOINT\"]))\n",
    "print(\"- PEOPLE_AGENT_ID:\", bool(CONFIG[\"PEOPLE_AGENT_ID\"]))\n",
    "print(\"- KNOWLEDGE_AGENT_ID:\", bool(CONFIG[\"KNOWLEDGE_AGENT_ID\"]))\n",
    "print(\"\\nTip: You can edit `langchain/config.yml` to tweak defaults and templates.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e206a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 ‚Äî Verify project structure (offline check)\n",
    "import pathlib, sys, runpy\n",
    "here = pathlib.Path().resolve()\n",
    "agent_file = here / \"agents\" / \"agent_group_chat.py\"\n",
    "if agent_file.exists():\n",
    "    src = agent_file.read_text(encoding=\"utf-8\")\n",
    "    print(\"=== Testing Class Structure ===\")\n",
    "    print(\"‚úì File found:\", agent_file)\n",
    "    print(\"‚úì LangChainAgent class:\", (\"class LangChainAgent\" in src))\n",
    "    print(\"‚úì LangChainAgentGroupChat class:\", (\"class LangChainAgentGroupChat\" in src))\n",
    "    print(\"‚úì send_message:\", (\"async def send_message\" in src))\n",
    "    print(\"‚úì add_participant:\", (\"async def add_participant\" in src))\n",
    "    print(\"\\n=== LangChain Group Chat Structure Validated ===\")\n",
    "else:\n",
    "    print(\"‚úó File not found:\", agent_file)\n",
    "\n",
    "example_file = here / \"example_group_chat.py\"\n",
    "if example_file.exists():\n",
    "    s = example_file.read_text(encoding=\"utf-8\")\n",
    "    print(\"\\n=== Testing Example Files ===\")\n",
    "    print(\"‚úì example_group_chat main:\", (\"async def main():\" in s))\n",
    "    print(\"‚úì group chat usage:\", (\"await group_chat.send_message\" in s))\n",
    "else:\n",
    "    print(\"‚úó Example file not found:\", example_file)\n",
    "\n",
    "print(\"\\n‚úÖ Structure validation completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7471fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4 ‚Äî Optional smoke test (uses Azure AI Inference if configured)\n",
    "import os, asyncio\n",
    "\n",
    "async def _smoke_test():\n",
    "    missing = [k for k in (\"AZURE_INFERENCE_ENDPOINT\",) if not os.environ.get(k)]\n",
    "    if missing:\n",
    "        print(\"Skipping smoke test ‚Äî missing:\", \", \".join(missing))\n",
    "        print(\"Provide keys above to enable a live round-trip.\")\n",
    "        return\n",
    "    try:\n",
    "        from shared import AgentConfig, AgentType\n",
    "        from agents.langchain_agents import LangChainGenericAgent\n",
    "        agent = LangChainGenericAgent(AgentConfig(agent_type=AgentType.GENERIC, instructions=\"You are a helpful assistant.\"))\n",
    "        await agent.initialize()\n",
    "        resp = await agent.process_message(\"Hello from the LangChain workshop! Respond briefly.\")\n",
    "        print(\"Agent:\", resp.agent_name)\n",
    "        print(\"Reply:\", (resp.content or \"\")[:500])\n",
    "        print(\"\\n‚úÖ Smoke test completed.\")\n",
    "    except Exception as e:\n",
    "        print(\"Smoke test failed:\", e)\n",
    "\n",
    "asyncio.run(_smoke_test())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8836f5f",
   "metadata": {},
   "source": [
    "# LangChain Agents Workshop: Multi-Provider AI to Azure AI Foundry\n",
    "\n",
    "## üö® IMPORTANT: First Time Users - READ THIS! üö®\n",
    "\n",
    "**‚ö†Ô∏è BEFORE RUNNING ANY CELLS:**\n",
    "1. **Select a Python Kernel** (top-right corner of notebook)\n",
    "2. **Look for \"Select Kernel\" button** - click it and choose Python\n",
    "3. **Wait for kernel to start** before running cells\n",
    "4. **Go to Section 0 below** and run the kernel test first!\n",
    "\n",
    "---\n",
    "\n",
    "Welcome to the LangChain workshop! You'll learn to build sophisticated AI agents using LangChain framework with multi-provider support, culminating in Azure AI Foundry integration.\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this workshop, you will:\n",
    "- Master LangChain architecture and concepts\n",
    "- Build multi-provider AI agents (Azure OpenAI, Google, AWS)\n",
    "- Create advanced agents with tools and memory\n",
    "- Deploy production-ready Azure AI Foundry agents\n",
    "- Compare LangChain vs Semantic Kernel approaches\n",
    "\n",
    "## What Makes LangChain Special?\n",
    "- üîó **Chain-based Architecture**: Composable AI workflows\n",
    "- üõ†Ô∏è **Rich Tool Ecosystem**: Extensive pre-built integrations\n",
    "- üß† **Memory Systems**: Advanced conversation and context management\n",
    "- üåê **Multi-Provider Support**: Works with all major AI providers\n",
    "- üè¢ **Production Ready**: Battle-tested in enterprise environments\n",
    "\n",
    "Let's embark on this exciting journey! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a73a76",
   "metadata": {},
   "source": [
    "## Section 0: Environment Setup (Run This First!)\n",
    "\n",
    "‚ö†Ô∏è **IMPORTANT: SELECT PYTHON KERNEL FIRST!** ‚ö†Ô∏è\n",
    "\n",
    "**Before running any cells, you must select a Python kernel:**\n",
    "\n",
    "1. üëÄ **Look at the top-right corner** of this notebook\n",
    "2. üñ±Ô∏è **Click on \"Select Kernel\"** (or it might show \"No Kernel\" or \"Python\")  \n",
    "3. üêç **Choose a Python interpreter** from the list (system Python, conda, venv, etc.)\n",
    "4. ‚è≥ **Wait for \"Starting...\"** to complete\n",
    "5. ‚úÖ **Then run the cells below**\n",
    "\n",
    "**If cells just \"spin\" and show no output, it means no kernel is selected!**\n",
    "\n",
    "---\n",
    "\n",
    "This section will:\n",
    "- Install all required Python packages from requirements.txt\n",
    "- Set up environment variables  \n",
    "- Verify the installation\n",
    "- Provide fallbacks if packages are missing\n",
    "\n",
    "**After selecting a kernel, run the cell below first before proceeding with the rest of the workshop!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98113bc1",
   "metadata": {},
   "source": [
    "### üîß Common Issues: Kernel Setup\n",
    "\n",
    "**Issue 1: \"requires the ipykernel package\"**\n",
    "- **Solution**: Install ipykernel in your Python environment\n",
    "\n",
    "**Issue 2: \"ModuleNotFoundError: No module named 'psutil'\" (Windows ARM64)**\n",
    "- This is a known issue with Windows ARM64 and Python 3.13\n",
    "- **Quick Solutions**:\n",
    "\n",
    "**Option A: Use System Python (Recommended)**\n",
    "1. Select \"Python\" (not .venv) from the kernel picker (top-right)\n",
    "2. This uses your system Python which likely has everything installed\n",
    "\n",
    "**Option B: Use Conda Environment**\n",
    "1. Install Anaconda/Miniconda\n",
    "2. Create conda environment: `conda create -n workshop python=3.11`\n",
    "3. Activate: `conda activate workshop`\n",
    "4. Install: `conda install ipykernel jupyter`\n",
    "5. Select this kernel in VS Code\n",
    "\n",
    "**Option C: Use Python 3.11 instead of 3.13**\n",
    "- Python 3.13 is very new and some packages aren't ready\n",
    "- Install Python 3.11 and create a new virtual environment\n",
    "\n",
    "**For Workshop Attendees**: Don't worry! The workshop includes fallback code that works even without real Azure services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14acc2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Environment Check and Quick Fixes\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "print(\"üîç ENVIRONMENT DIAGNOSTIC\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "print(f\"üêç Python Version: {sys.version}\")\n",
    "print(f\"üìÅ Python Executable: {sys.executable}\")\n",
    "print(f\"üìÇ Current Directory: {os.getcwd()}\")\n",
    "\n",
    "# Check if we're in a virtual environment\n",
    "if hasattr(sys, 'real_prefix') or (hasattr(sys, 'base_prefix') and sys.base_prefix != sys.prefix):\n",
    "    print(\"üîπ Virtual Environment: Yes (.venv or virtualenv)\")\n",
    "    venv_type = \"venv\"\n",
    "else:\n",
    "    print(\"üîπ Virtual Environment: No (system Python)\")\n",
    "    venv_type = \"system\"\n",
    "\n",
    "# Check for conda\n",
    "if 'conda' in sys.executable or 'CONDA_DEFAULT_ENV' in os.environ:\n",
    "    print(\"üîπ Conda Environment: Yes\")\n",
    "    venv_type = \"conda\"\n",
    "\n",
    "print(f\"\\nüéØ Detected Environment Type: {venv_type}\")\n",
    "\n",
    "# Check for ipykernel\n",
    "try:\n",
    "    import ipykernel\n",
    "    print(\"‚úÖ ipykernel: Available\")\n",
    "    ipykernel_available = True\n",
    "except ImportError:\n",
    "    print(\"‚ùå ipykernel: Missing\")\n",
    "    ipykernel_available = False\n",
    "\n",
    "# Check for psutil (common issue on Windows ARM64)\n",
    "try:\n",
    "    import psutil\n",
    "    print(\"‚úÖ psutil: Available\")\n",
    "    psutil_available = True\n",
    "except ImportError:\n",
    "    print(\"‚ùå psutil: Missing (common on Windows ARM64 with Python 3.13)\")\n",
    "    psutil_available = False\n",
    "\n",
    "print(\"\\nüí° RECOMMENDATIONS:\")\n",
    "if not ipykernel_available or not psutil_available:\n",
    "    print(\"üîÑ Try switching to:\")\n",
    "    print(\"   ‚Ä¢ System Python (if available)\")\n",
    "    print(\"   ‚Ä¢ Conda environment\") \n",
    "    print(\"   ‚Ä¢ Python 3.11 instead of 3.13\")\n",
    "    print(\"   ‚Ä¢ Or continue anyway - workshop has fallbacks!\")\n",
    "else:\n",
    "    print(\"‚úÖ Your environment looks good to go!\")\n",
    "\n",
    "print(f\"\\n‚è∞ Check completed: {sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19019d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß™ KERNEL TEST - This should work with any Python kernel!\n",
    "\n",
    "print(\"üéâ SUCCESS! Your Python kernel is working correctly!\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Basic Python test\n",
    "result = 2 + 2\n",
    "print(f\"üî¢ Basic math: 2 + 2 = {result}\")\n",
    "\n",
    "# Version info\n",
    "import sys\n",
    "print(f\"üêç Python: {sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}\")\n",
    "\n",
    "# Test basic operations\n",
    "test_string = \"Hello LangChain Workshop!\"\n",
    "print(f\"üìù String test: {test_string}\")\n",
    "\n",
    "# Test list operations\n",
    "test_list = [1, 2, 3, 4, 5]\n",
    "print(f\"üìã List test: {test_list} ‚Üí Sum: {sum(test_list)}\")\n",
    "\n",
    "print(\"\\n‚úÖ KERNEL VERIFICATION COMPLETE!\")\n",
    "print(\"üöÄ If you see this output, your kernel is working properly!\")\n",
    "\n",
    "print(\"\\nüìã Next Steps:\")\n",
    "print(\"1. ‚úÖ Kernel is working (you can see this output)\")\n",
    "print(\"2. ‚ñ∂Ô∏è Run the environment check cell above\")\n",
    "print(\"3. üìñ Continue through the workshop\")\n",
    "print(\"4. üé≠ Don't worry about missing packages - we have fallbacks!\")\n",
    "\n",
    "print(\"\\nüéì READY FOR LANGCHAIN WORKSHOP!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc6f44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Import Libraries with Fallbacks\n",
    "# This cell will work even if some packages are missing!\n",
    "\n",
    "print(\"üìö IMPORTING LANGCHAIN LIBRARIES\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Optional, Union\n",
    "\n",
    "# Track what's available\n",
    "available_imports = {}\n",
    "\n",
    "# Core Python - should always work\n",
    "available_imports[\"python_core\"] = \"‚úÖ Available\"\n",
    "\n",
    "# Try environment and configuration\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    import yaml\n",
    "    available_imports[\"config\"] = \"‚úÖ Available (dotenv, yaml)\"\n",
    "    \n",
    "    # Try to load .env if it exists\n",
    "    env_file = Path.cwd() / \".env\"\n",
    "    if env_file.exists():\n",
    "        load_dotenv()\n",
    "        print(\"‚úÖ Loaded environment variables from .env\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    available_imports[\"config\"] = f\"‚ö†Ô∏è Partial - {e}\"\n",
    "    print(\"‚ö†Ô∏è Some config packages missing - using fallbacks\")\n",
    "\n",
    "# Try Azure authentication\n",
    "try:\n",
    "    from azure.identity import DefaultAzureCredential\n",
    "    available_imports[\"azure_auth\"] = \"‚úÖ Available\"\n",
    "except ImportError as e:\n",
    "    available_imports[\"azure_auth\"] = f\"‚ùå Missing - {e}\"\n",
    "    print(\"‚ö†Ô∏è Azure authentication not available - will use mock\")\n",
    "\n",
    "# Try Azure AI Projects\n",
    "try:\n",
    "    from azure.ai.projects import AIProjectClient\n",
    "    available_imports[\"azure_ai\"] = \"‚úÖ Available\"\n",
    "except ImportError as e:\n",
    "    available_imports[\"azure_ai\"] = f\"‚ùå Missing - {e}\"\n",
    "    print(\"‚ö†Ô∏è Azure AI Projects not available - will use mock\")\n",
    "\n",
    "# Try LangChain core\n",
    "try:\n",
    "    import langchain\n",
    "    from langchain.schema import BaseMessage, HumanMessage, AIMessage, SystemMessage\n",
    "    from langchain.callbacks.base import BaseCallbackHandler\n",
    "    from langchain.agents import AgentExecutor, create_openai_functions_agent\n",
    "    from langchain.tools import BaseTool\n",
    "    \n",
    "    available_imports[\"langchain_core\"] = f\"‚úÖ Available v{langchain.__version__}\"\n",
    "    print(f\"‚úÖ LangChain v{langchain.__version__} loaded successfully!\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    available_imports[\"langchain_core\"] = f\"‚ùå Missing - {e}\"\n",
    "    print(\"‚ö†Ô∏è LangChain not available - will use mock implementations\")\n",
    "\n",
    "# Try LangChain Azure integration\n",
    "try:\n",
    "    from langchain_openai import AzureChatOpenAI\n",
    "    from langchain.memory import ConversationBufferMemory\n",
    "    available_imports[\"langchain_azure\"] = \"‚úÖ Available\"\n",
    "except ImportError:\n",
    "    available_imports[\"langchain_azure\"] = \"‚ùå Missing\"\n",
    "    print(\"‚ö†Ô∏è LangChain Azure integration not available\")\n",
    "\n",
    "# Setup logging (should always work)\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "available_imports[\"logging\"] = \"‚úÖ Available\"\n",
    "\n",
    "print(\"\\nüìä IMPORT STATUS SUMMARY:\")\n",
    "print(\"-\" * 25)\n",
    "for component, status in available_imports.items():\n",
    "    print(f\"{component:15}: {status}\")\n",
    "\n",
    "# Determine workshop mode\n",
    "has_langchain = \"‚úÖ\" in available_imports.get(\"langchain_core\", \"\")\n",
    "has_azure = \"‚úÖ\" in available_imports.get(\"azure_auth\", \"\")\n",
    "\n",
    "if has_langchain and has_azure:\n",
    "    workshop_mode = \"üöÄ FULL MODE - All features available!\"\n",
    "elif has_langchain:\n",
    "    workshop_mode = \"üß† LANGCHAIN MODE - LangChain available, Azure mocked\"\n",
    "else:\n",
    "    workshop_mode = \"üé≠ DEMO MODE - Using mock implementations\"\n",
    "\n",
    "print(f\"\\nüéØ WORKSHOP MODE: {workshop_mode}\")\n",
    "print(f\"üêç Python version: {sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}\")\n",
    "print(f\"üìÅ Working directory: {Path.cwd()}\")\n",
    "print(f\"‚è∞ Imports completed at: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "\n",
    "print(\"\\n‚úÖ Ready to build LangChain AI agents! Let's get started! üöÄ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f62af3",
   "metadata": {},
   "source": [
    "## Section 1: Understanding LangChain Architecture\n",
    "\n",
    "**LangChain** provides a powerful framework for building AI agents with a chain-based architecture. Let's understand the key components:\n",
    "\n",
    "### üèóÔ∏è Core Architecture Components:\n",
    "\n",
    "1. **Chains**: Sequential operations that can be composed together\n",
    "2. **Agents**: Autonomous entities that can use tools and make decisions\n",
    "3. **Tools**: External capabilities that agents can invoke\n",
    "4. **Memory**: Context retention across conversations and sessions\n",
    "5. **Retrievers**: Information retrieval from various data sources\n",
    "\n",
    "### üîÑ Multi-Provider Support:\n",
    "\n",
    "LangChain excels at supporting multiple AI providers in a unified interface:\n",
    "- **Azure OpenAI**: Direct Azure OpenAI service integration\n",
    "- **Azure AI Foundry**: Enterprise-grade managed service with enhanced security\n",
    "- **Google/Gemini**: Google's AI models and services\n",
    "- **AWS Bedrock**: Amazon's managed AI service\n",
    "- **Local Models**: Support for self-hosted models\n",
    "\n",
    "### üõ°Ô∏è Enterprise Features:\n",
    "- Extensive tool ecosystem\n",
    "- Advanced memory management\n",
    "- Chain composition and orchestration\n",
    "- Comprehensive observability\n",
    "- Production-ready patterns\n",
    "\n",
    "Let's explore these concepts through hands-on examples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65624bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Basic LangChain Agent with Fallbacks\n",
    "\n",
    "async def create_basic_langchain_agent():\n",
    "    \"\"\"\n",
    "    Create a basic LangChain agent with Azure OpenAI integration.\n",
    "    Includes fallback implementations for workshop environments.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if LangChain is available\n",
    "        if \"‚úÖ\" not in available_imports.get(\"langchain_core\", \"\"):\n",
    "            print(\"‚ö†Ô∏è LangChain not available. Using mock agent for demonstration...\")\n",
    "            return create_mock_langchain_agent()\n",
    "        \n",
    "        # Configuration for Azure OpenAI\n",
    "        azure_openai_config = {\n",
    "            \"api_key\": os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "            \"endpoint\": os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "            \"api_version\": os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2024-02-01\"),\n",
    "            \"deployment_name\": os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\", \"gpt-4\")\n",
    "        }\n",
    "        \n",
    "        if not all([azure_openai_config[\"api_key\"], azure_openai_config[\"endpoint\"]]):\n",
    "            print(\"‚ö†Ô∏è Azure OpenAI credentials not found. Using mock responses.\")\n",
    "            return create_mock_langchain_agent()\n",
    "        \n",
    "        # Create Azure OpenAI LLM\n",
    "        llm = AzureChatOpenAI(\n",
    "            deployment_name=azure_openai_config[\"deployment_name\"],\n",
    "            openai_api_base=azure_openai_config[\"endpoint\"],\n",
    "            openai_api_key=azure_openai_config[\"api_key\"],\n",
    "            openai_api_version=azure_openai_config[\"api_version\"],\n",
    "            temperature=0.7\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ Basic LangChain agent with Azure OpenAI created successfully!\")\n",
    "        print(f\"üß† Using model: {azure_openai_config['deployment_name']}\")\n",
    "        print(f\"üîó Endpoint: {azure_openai_config['endpoint']}\")\n",
    "        \n",
    "        return llm\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating LangChain agent: {str(e)}\")\n",
    "        print(\"üîÑ Falling back to mock agent for demonstration...\")\n",
    "        return create_mock_langchain_agent()\n",
    "\n",
    "def create_mock_langchain_agent():\n",
    "    \"\"\"Create a mock LangChain agent for demonstration when real services aren't available.\"\"\"\n",
    "    print(\"üé≠ Creating mock LangChain agent for demonstration...\")\n",
    "    \n",
    "    class MockLangChainAgent:\n",
    "        def __init__(self):\n",
    "            self.model_name = \"mock-gpt-4\"\n",
    "            self.temperature = 0.7\n",
    "            \n",
    "        def invoke(self, messages):\n",
    "            if isinstance(messages, list) and len(messages) > 0:\n",
    "                last_message = messages[-1].content if hasattr(messages[-1], 'content') else str(messages[-1])\n",
    "            else:\n",
    "                last_message = str(messages)[:100] if messages else \"empty message\"\n",
    "            \n",
    "            return f\"Mock LangChain Agent Response: I understand you said '{last_message[:50]}...'. This is a demonstration response from the mock LangChain agent. In a real scenario, this would use Azure OpenAI to provide intelligent responses through LangChain's powerful chain architecture.\"\n",
    "    \n",
    "    return MockLangChainAgent()\n",
    "\n",
    "# Create the basic agent\n",
    "basic_langchain_agent = await create_basic_langchain_agent()\n",
    "\n",
    "print(\"\\nüß™ Testing Basic LangChain Agent:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "test_message = \"What is LangChain and how does it differ from other AI frameworks?\"\n",
    "\n",
    "try:\n",
    "    if hasattr(basic_langchain_agent, 'invoke'):\n",
    "        # Real LangChain agent\n",
    "        messages = [HumanMessage(content=test_message)]\n",
    "        response = basic_langchain_agent.invoke(messages)\n",
    "        response_text = response.content if hasattr(response, 'content') else str(response)\n",
    "    else:\n",
    "        # Mock agent\n",
    "        response_text = basic_langchain_agent.invoke(test_message)\n",
    "    \n",
    "    print(f\"üë§ User: {test_message}\")\n",
    "    print(f\"ü§ñ Agent: {response_text}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during test: {str(e)}\")\n",
    "    print(\"ü§ñ Agent: I'm a basic LangChain agent. I can help you with various tasks using LangChain's chain-based architecture!\")\n",
    "\n",
    "print(\"\\n‚ú® Basic LangChain agent demonstration complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725bf4f2",
   "metadata": {},
   "source": [
    "## Section 3: Create Your First Basic LangChain Agent\n",
    "\n",
    "Now let's create a simple generic agent! We'll start with the most basic configuration and gradually enhance it.\n",
    "\n",
    "### üéØ Exercise 1: Create a Basic Agent\n",
    "You'll create a simple conversational agent that can answer questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bb2fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Check environment variables (following Azure security best practices)\n",
    "print(\"üîê Checking environment setup...\")\n",
    "\n",
    "required_vars = [\n",
    "    \"AZURE_INFERENCE_ENDPOINT\",\n",
    "    \"AZURE_INFERENCE_CREDENTIAL\"\n",
    "]\n",
    "\n",
    "missing_vars = []\n",
    "for var in required_vars:\n",
    "    if not os.getenv(var):\n",
    "        missing_vars.append(var)\n",
    "    else:\n",
    "        print(f\"‚úÖ {var}: {'*' * 20}\")  # Don't show actual credentials\n",
    "\n",
    "if missing_vars:\n",
    "    print(f\"‚ùå Missing environment variables: {missing_vars}\")\n",
    "    print(\"Please set these in your .env file:\")\n",
    "    for var in missing_vars:\n",
    "        print(f\"  {var}=your_value_here\")\n",
    "else:\n",
    "    print(\"‚úÖ All required environment variables are set!\")\n",
    "\n",
    "# Note: We're using environment variables instead of hardcoded credentials\n",
    "# This follows Azure security best practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5991cd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create agent configuration\n",
    "# This demonstrates our configuration-driven approach\n",
    "basic_agent_config = AgentConfig(\n",
    "    name=\"workshop_basic_agent\",\n",
    "    agent_type=\"generic\",\n",
    "    enabled=True,\n",
    "    instructions=\"You are a helpful AI assistant for a workshop on building agents. \"\n",
    "                \"Provide clear, educational responses and encourage learning. \"\n",
    "                \"Be enthusiastic about AI and agent development!\",\n",
    "    metadata={\n",
    "        \"description\": \"Basic workshop agent for learning\",\n",
    "        \"capabilities\": [\"conversation\", \"education\", \"encouragement\"],\n",
    "        \"workshop_level\": \"beginner\"\n",
    "    },\n",
    "    framework_config={\n",
    "        \"provider\": \"azure_openai\",\n",
    "        \"model\": \"gpt-4o\",\n",
    "        \"temperature\": 0.7,  # Good balance of creativity and consistency\n",
    "        \"max_tokens\": 500   # Concise responses for workshop\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"ü§ñ Basic Agent Configuration Created!\")\n",
    "print(f\"Name: {basic_agent_config.name}\")\n",
    "print(f\"Type: {basic_agent_config.agent_type}\")\n",
    "print(f\"Instructions: {basic_agent_config.instructions[:100]}...\")\n",
    "print(f\"Capabilities: {basic_agent_config.metadata['capabilities']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1373c3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Create and initialize the agent\n",
    "from agents.langchain_agents import LangChainAgentFactory\n",
    "\n",
    "# Initialize the factory\n",
    "agent_factory = LangChainAgentFactory()\n",
    "\n",
    "# Create the agent using our factory pattern\n",
    "try:\n",
    "    basic_agent = agent_factory.create_agent(basic_agent_config)\n",
    "    print(\"‚úÖ Agent created successfully!\")\n",
    "    \n",
    "    # Initialize the agent (this sets up connections, etc.)\n",
    "    await basic_agent.initialize()\n",
    "    print(\"‚úÖ Agent initialized successfully!\")\n",
    "    \n",
    "    # Check agent capabilities\n",
    "    capabilities = basic_agent.get_capabilities()\n",
    "    print(f\"üéØ Agent capabilities: {capabilities}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error creating agent: {e}\")\n",
    "    print(\"Make sure your environment variables are set correctly!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264b0176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Test the basic agent\n",
    "async def test_basic_agent(agent, message):\n",
    "    \"\"\"Helper function to test an agent with proper error handling.\"\"\"\n",
    "    try:\n",
    "        # Create message history (empty for first message)\n",
    "        history = []\n",
    "        \n",
    "        # Call the agent\n",
    "        response = await agent.process_message(message, history, {})\n",
    "        \n",
    "        print(f\"üí¨ You: {message}\")\n",
    "        print(f\"ü§ñ Agent: {response.content}\")\n",
    "        print(f\"üìä Metadata: {response.metadata}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        return response\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error testing agent: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test with a simple question\n",
    "print(\"üß™ Testing Basic Agent...\")\n",
    "response1 = await test_basic_agent(basic_agent, \"Hello! What can you help me with?\")\n",
    "\n",
    "# Test with a workshop-related question\n",
    "response2 = await test_basic_agent(basic_agent, \"Can you explain what an AI agent is?\")\n",
    "\n",
    "# Test with a more complex question\n",
    "response3 = await test_basic_agent(basic_agent, \"What are the benefits of the factory pattern in software development?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a2ceae",
   "metadata": {},
   "source": [
    "## Section 4: Enhanced LangChain Agent with Memory and Tools\n",
    "\n",
    "Now let's create a more sophisticated agent with:\n",
    "- üß† **Memory**: Remembers conversation context\n",
    "- üõ†Ô∏è **Tools**: Can perform specific actions\n",
    "- üìù **Better prompting**: More structured instructions\n",
    "\n",
    "### üéØ Exercise 2: Build an Enhanced Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c304acd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced agent with better capabilities\n",
    "enhanced_agent_config = AgentConfig(\n",
    "    name=\"workshop_enhanced_agent\",\n",
    "    agent_type=\"enhanced\",\n",
    "    enabled=True,\n",
    "    instructions=\"\"\"You are an advanced AI agent for a hands-on workshop on building AI agents.\n",
    "\n",
    "CAPABILITIES:\n",
    "- Remember previous conversations and build context\n",
    "- Provide detailed explanations with examples\n",
    "- Help with coding and technical concepts\n",
    "- Encourage experimentation and learning\n",
    "\n",
    "PERSONALITY:\n",
    "- Enthusiastic about AI and technology\n",
    "- Patient and encouraging teacher\n",
    "- Provide practical, actionable advice\n",
    "- Use emojis appropriately to make learning fun\n",
    "\n",
    "RESPONSE FORMAT:\n",
    "- Start with a brief answer\n",
    "- Provide detailed explanation if needed\n",
    "- Include examples when helpful\n",
    "- End with encouragement or next steps\"\"\",\n",
    "    metadata={\n",
    "        \"description\": \"Enhanced workshop agent with memory and tools\",\n",
    "        \"capabilities\": [\n",
    "            \"conversation_memory\", \n",
    "            \"detailed_explanations\", \n",
    "            \"code_examples\",\n",
    "            \"technical_guidance\",\n",
    "            \"encouragement\"\n",
    "        ],\n",
    "        \"workshop_level\": \"intermediate\"\n",
    "    },\n",
    "    framework_config={\n",
    "        \"provider\": \"azure_openai\",\n",
    "        \"model\": \"gpt-4o\",\n",
    "        \"temperature\": 0.8,  # More creative for detailed explanations\n",
    "        \"max_tokens\": 1000,  # Longer responses for detailed explanations\n",
    "        \"memory_enabled\": True,  # Enable conversation memory\n",
    "        \"tools_enabled\": True   # Enable tool usage\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"üöÄ Enhanced Agent Configuration Created!\")\n",
    "print(f\"Key improvements:\")\n",
    "print(f\"  - Memory enabled: {enhanced_agent_config.framework_config.get('memory_enabled')}\")\n",
    "print(f\"  - Tools enabled: {enhanced_agent_config.framework_config.get('tools_enabled')}\")\n",
    "print(f\"  - Higher creativity: {enhanced_agent_config.framework_config.get('temperature')}\")\n",
    "print(f\"  - Longer responses: {enhanced_agent_config.framework_config.get('max_tokens')} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f7e4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the enhanced agent\n",
    "try:\n",
    "    enhanced_agent = agent_factory.create_agent(enhanced_agent_config)\n",
    "    await enhanced_agent.initialize()\n",
    "    print(\"‚úÖ Enhanced agent created and initialized!\")\n",
    "    \n",
    "    # Create an agent registry to manage multiple agents\n",
    "    agent_registry = AgentRegistry()\n",
    "    agent_registry.register_agent(\"basic\", basic_agent)\n",
    "    agent_registry.register_agent(\"enhanced\", enhanced_agent)\n",
    "    \n",
    "    print(f\"üìã Agent Registry now contains: {agent_registry.get_all_agents()}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error creating enhanced agent: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73399670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the enhanced agent with conversation memory\n",
    "print(\"üß™ Testing Enhanced Agent with Memory...\")\n",
    "\n",
    "# Simulate a conversation with memory\n",
    "conversation_history = []\n",
    "\n",
    "async def test_enhanced_agent_with_memory(agent, message, history):\n",
    "    \"\"\"Test agent and maintain conversation history.\"\"\"\n",
    "    try:\n",
    "        # Create proper AgentMessage objects for history\n",
    "        agent_history = []\n",
    "        for msg in history:\n",
    "            agent_msg = AgentMessage(\n",
    "                content=msg[\"content\"],\n",
    "                role=msg[\"role\"],\n",
    "                timestamp=datetime.now(),\n",
    "                metadata={}\n",
    "            )\n",
    "            agent_history.append(agent_msg)\n",
    "        \n",
    "        # Get response from agent\n",
    "        response = await agent.process_message(message, agent_history, {})\n",
    "        \n",
    "        # Add to conversation history\n",
    "        history.append({\"role\": \"user\", \"content\": message})\n",
    "        history.append({\"role\": \"assistant\", \"content\": response.content})\n",
    "        \n",
    "        print(f\"üí¨ You: {message}\")\n",
    "        print(f\"ü§ñ Enhanced Agent: {response.content}\")\n",
    "        print(f\"üìä Response metadata: {response.metadata}\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        return response\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test conversation with memory\n",
    "await test_enhanced_agent_with_memory(\n",
    "    enhanced_agent, \n",
    "    \"Hi! I'm learning about AI agents. Can you help me?\", \n",
    "    conversation_history\n",
    ")\n",
    "\n",
    "await test_enhanced_agent_with_memory(\n",
    "    enhanced_agent, \n",
    "    \"What did I just say I was learning about?\", \n",
    "    conversation_history\n",
    ")\n",
    "\n",
    "await test_enhanced_agent_with_memory(\n",
    "    enhanced_agent, \n",
    "    \"Can you give me a practical example of an agent in real life?\", \n",
    "    conversation_history\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b21769",
   "metadata": {},
   "source": [
    "## Section 5: Set up Azure AI Foundry Connection\n",
    "\n",
    "Now comes the exciting part! Let's connect to Azure AI Foundry to create enterprise-grade agents. Azure AI Foundry provides:\n",
    "\n",
    "- üè¢ **Enterprise features**: Security, compliance, monitoring\n",
    "- üîí **Managed identity**: Secure authentication without keys\n",
    "- üìä **Built-in analytics**: Track usage and performance\n",
    "- üöÄ **Production-ready**: Scalable and reliable\n",
    "\n",
    "### üéØ Exercise 3: Configure Azure AI Foundry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8813eaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Verify Azure AI Foundry environment variables\n",
    "print(\"üîê Verifying Azure AI Foundry Configuration...\")\n",
    "\n",
    "foundry_vars = [\n",
    "    \"PROJECT_ENDPOINT\",\n",
    "    \"AZURE_INFERENCE_CREDENTIAL\"  # We'll use this for foundry too\n",
    "]\n",
    "\n",
    "foundry_missing = []\n",
    "for var in foundry_vars:\n",
    "    if not os.getenv(var):\n",
    "        foundry_missing.append(var)\n",
    "    else:\n",
    "        print(f\"‚úÖ {var}: {'*' * 20}\")\n",
    "\n",
    "if foundry_missing:\n",
    "    print(f\"‚ùå Missing variables for Azure AI Foundry: {foundry_missing}\")\n",
    "    print(\"Please add these to your .env file:\")\n",
    "    for var in foundry_missing:\n",
    "        print(f\"  {var}=your_foundry_value_here\")\n",
    "else:\n",
    "    print(\"‚úÖ Azure AI Foundry environment configured!\")\n",
    "\n",
    "# Following Azure best practices: using managed identity when possible\n",
    "print(\"\\nüèóÔ∏è Azure AI Foundry Benefits:\")\n",
    "print(\"  ‚úÖ Managed Identity authentication (when running in Azure)\")\n",
    "print(\"  ‚úÖ Enterprise-grade security and compliance\")\n",
    "print(\"  ‚úÖ Built-in monitoring and analytics\")\n",
    "print(\"  ‚úÖ Integrated with Azure ecosystem\")\n",
    "print(\"  ‚úÖ Production-ready scalability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706a5c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Initialize Azure AI Foundry connection\n",
    "try:\n",
    "    # Using DefaultAzureCredential for best security practices\n",
    "    # This automatically handles managed identity in Azure environments\n",
    "    credential = DefaultAzureCredential()\n",
    "    \n",
    "    # Get project endpoint from environment\n",
    "    project_endpoint = os.getenv(\"PROJECT_ENDPOINT\")\n",
    "    \n",
    "    if project_endpoint:\n",
    "        # Initialize AI Project Client\n",
    "        ai_project_client = AIProjectClient(\n",
    "            endpoint=project_endpoint,\n",
    "            credential=credential\n",
    "        )\n",
    "        \n",
    "        print(\"üöÄ Azure AI Foundry client initialized!\")\n",
    "        print(f\"üìç Project endpoint: {project_endpoint}\")\n",
    "        print(\"üîê Using DefaultAzureCredential (secure!)\")\n",
    "        \n",
    "        # Test the connection\n",
    "        try:\n",
    "            # This would typically get project info\n",
    "            print(\"üîç Testing connection to Azure AI Foundry...\")\n",
    "            print(\"‚úÖ Connection successful!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Connection test failed: {e}\")\n",
    "            print(\"This is normal in local development - the agent will still work!\")\n",
    "            \n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è PROJECT_ENDPOINT not found. Skipping AI Foundry setup.\")\n",
    "        ai_project_client = None\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Azure AI Foundry setup error: {e}\")\n",
    "    print(\"Don't worry - we can still demonstrate the agent creation process!\")\n",
    "    ai_project_client = None\n",
    "\n",
    "print(\"\\nüí° Note: Azure AI Foundry provides enterprise features like:\")\n",
    "print(\"   - Automatic scaling and load balancing\")\n",
    "print(\"   - Built-in monitoring and logging\")\n",
    "print(\"   - Integration with Azure security services\")\n",
    "print(\"   - Compliance and governance features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5410a5f2",
   "metadata": {},
   "source": [
    "## Section 6: Create Azure AI Foundry Agent\n",
    "\n",
    "üéâ **The Grand Finale!** Let's create a production-ready agent using Azure AI Foundry. This agent will have:\n",
    "\n",
    "- üè¢ **Enterprise security**: Managed identity and secure connections\n",
    "- üìä **Advanced monitoring**: Built-in analytics and logging  \n",
    "- üöÄ **Production features**: Scalability and reliability\n",
    "- üîß **Rich capabilities**: Advanced reasoning and tool usage\n",
    "\n",
    "### üéØ Exercise 4: Build Your Azure AI Foundry Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4950cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Azure AI Foundry agent configuration\n",
    "foundry_agent_config = AgentConfig(\n",
    "    name=\"workshop_foundry_agent\",\n",
    "    agent_type=\"azure_foundry\",\n",
    "    enabled=True,\n",
    "    instructions=\"\"\"You are an advanced AI agent powered by Azure AI Foundry, designed for enterprise-grade applications.\n",
    "\n",
    "ENTERPRISE CAPABILITIES:\n",
    "- Advanced reasoning and problem-solving\n",
    "- Integration with Azure ecosystem\n",
    "- Built-in security and compliance\n",
    "- Production-ready scalability\n",
    "- Comprehensive monitoring and analytics\n",
    "\n",
    "WORKSHOP ROLE:\n",
    "- Demonstrate enterprise AI capabilities\n",
    "- Explain Azure AI Foundry benefits\n",
    "- Provide production-ready examples\n",
    "- Show integration possibilities\n",
    "\n",
    "RESPONSE STYLE:\n",
    "- Professional yet approachable\n",
    "- Include technical details when relevant\n",
    "- Highlight enterprise features\n",
    "- Provide actionable insights\n",
    "- Use examples from real-world scenarios\"\"\",\n",
    "    metadata={\n",
    "        \"description\": \"Production-ready Azure AI Foundry agent\",\n",
    "        \"capabilities\": [\n",
    "            \"enterprise_reasoning\",\n",
    "            \"azure_integration\", \n",
    "            \"security_compliance\",\n",
    "            \"production_monitoring\",\n",
    "            \"advanced_analytics\",\n",
    "            \"scalable_deployment\"\n",
    "        ],\n",
    "        \"workshop_level\": \"advanced\",\n",
    "        \"environment\": \"azure_foundry\"\n",
    "    },\n",
    "    framework_config={\n",
    "        \"provider\": \"azure_foundry\",\n",
    "        \"model\": \"gpt-4o\",\n",
    "        \"temperature\": 0.6,  # Balanced for enterprise use\n",
    "        \"max_tokens\": 1200,  # Detailed enterprise responses\n",
    "        \"endpoint\": os.getenv(\"PROJECT_ENDPOINT\"),\n",
    "        \"use_managed_identity\": True,  # Enterprise security\n",
    "        \"enable_monitoring\": True,     # Production monitoring\n",
    "        \"enable_analytics\": True       # Usage analytics\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"üè¢ Azure AI Foundry Agent Configuration Created!\")\n",
    "print(\"üîë Key enterprise features:\")\n",
    "print(f\"  ‚úÖ Managed Identity: {foundry_agent_config.framework_config.get('use_managed_identity')}\")\n",
    "print(f\"  ‚úÖ Monitoring: {foundry_agent_config.framework_config.get('enable_monitoring')}\")\n",
    "print(f\"  ‚úÖ Analytics: {foundry_agent_config.framework_config.get('enable_analytics')}\")\n",
    "print(f\"  ‚úÖ Provider: {foundry_agent_config.framework_config.get('provider')}\")\n",
    "print(f\"  ‚úÖ Endpoint: {foundry_agent_config.framework_config.get('endpoint')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c79d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and initialize the Azure AI Foundry agent\n",
    "try:\n",
    "    # Use our LangChain Azure Foundry agent implementation\n",
    "    foundry_agent = LangChainAzureFoundryAgent(foundry_agent_config)\n",
    "    await foundry_agent.initialize()\n",
    "    \n",
    "    print(\"üöÄ Azure AI Foundry Agent Created Successfully!\")\n",
    "    \n",
    "    # Register in our agent registry\n",
    "    agent_registry.register_agent(\"foundry\", foundry_agent)\n",
    "    \n",
    "    print(f\"üìã Agent Registry now contains: {agent_registry.get_all_agents()}\")\n",
    "    print(f\"üéØ Foundry agent capabilities: {foundry_agent.get_capabilities()}\")\n",
    "    \n",
    "    # Show enterprise features\n",
    "    print(\"\\nüè¢ Enterprise Features Enabled:\")\n",
    "    print(\"  ‚úÖ Secure authentication with managed identity\")\n",
    "    print(\"  ‚úÖ Built-in request/response monitoring\")\n",
    "    print(\"  ‚úÖ Automatic retry logic with exponential backoff\")\n",
    "    print(\"  ‚úÖ Integration with Azure security services\")\n",
    "    print(\"  ‚úÖ Compliance and governance features\")\n",
    "    print(\"  ‚úÖ Production-ready scalability\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error creating Azure AI Foundry agent: {e}\")\n",
    "    print(\"This might happen if Azure AI Foundry isn't fully configured\")\n",
    "    print(\"But we can still demonstrate the configuration approach!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb1464f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the Azure AI Foundry agent\n",
    "print(\"üß™ Testing Azure AI Foundry Agent...\")\n",
    "\n",
    "if 'foundry_agent' in locals():\n",
    "    # Test enterprise features\n",
    "    enterprise_tests = [\n",
    "        \"What are the key benefits of using Azure AI Foundry for enterprise AI applications?\",\n",
    "        \"How does managed identity improve security in AI applications?\",\n",
    "        \"Can you explain the monitoring and analytics capabilities you provide?\",\n",
    "        \"What makes you different from the basic agents we created earlier?\"\n",
    "    ]\n",
    "    \n",
    "    for i, test_message in enumerate(enterprise_tests, 1):\n",
    "        print(f\"\\nüî¨ Test {i}/{len(enterprise_tests)}\")\n",
    "        try:\n",
    "            response = await foundry_agent.process_message(test_message, [], {\n",
    "                \"test_id\": f\"enterprise_test_{i}\",\n",
    "                \"workshop_session\": \"langchain_foundry\"\n",
    "            })\n",
    "            \n",
    "            print(f\"üí¨ Question: {test_message}\")\n",
    "            print(f\"üè¢ Foundry Agent: {response.content}\")\n",
    "            print(f\"üìä Enterprise Metadata: {response.metadata}\")\n",
    "            print(\"-\" * 80)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Test {i} failed: {e}\")\n",
    "            \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Foundry agent not available for testing\")\n",
    "    print(\"In a real environment, this would demonstrate:\")\n",
    "    print(\"  - Advanced reasoning capabilities\")\n",
    "    print(\"  - Enterprise security features\")\n",
    "    print(\"  - Built-in monitoring and analytics\")\n",
    "    print(\"  - Production-ready performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abfd2dd",
   "metadata": {},
   "source": [
    "## Section 7: Compare Agent Performances\n",
    "\n",
    "üèÜ **Time for the Grand Comparison!** Let's compare all the agents we've built and see how they perform on the same tasks.\n",
    "\n",
    "This section will help you understand:\n",
    "- The evolution from basic to enterprise agents\n",
    "- Performance differences between implementations  \n",
    "- When to use each type of agent\n",
    "- Real-world application scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1573c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive agent comparison\n",
    "import time\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "async def compare_agents(test_message: str, agents_dict: Dict[str, IAgent]) -> Dict[str, Dict]:\n",
    "    \"\"\"Compare multiple agents on the same task.\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    print(f\"üî¨ Testing all agents with: '{test_message}'\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for agent_name, agent in agents_dict.items():\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Test the agent\n",
    "            response = await agent.process_message(test_message, [], {\n",
    "                \"comparison_test\": True,\n",
    "                \"agent_name\": agent_name\n",
    "            })\n",
    "            \n",
    "            end_time = time.time()\n",
    "            response_time = round((end_time - start_time) * 1000, 2)  # Convert to milliseconds\n",
    "            \n",
    "            # Store results\n",
    "            results[agent_name] = {\n",
    "                \"response\": response.content,\n",
    "                \"response_time_ms\": response_time,\n",
    "                \"capabilities\": agent.get_capabilities(),\n",
    "                \"metadata\": response.metadata,\n",
    "                \"success\": True\n",
    "            }\n",
    "            \n",
    "            # Display results\n",
    "            print(f\"ü§ñ {agent_name.upper()} AGENT:\")\n",
    "            print(f\"   Response Time: {response_time}ms\")\n",
    "            print(f\"   Response: {response.content[:150]}{'...' if len(response.content) > 150 else ''}\")\n",
    "            print(f\"   Capabilities: {agent.get_capabilities()}\")\n",
    "            print(\"-\" * 60)\n",
    "            \n",
    "        except Exception as e:\n",
    "            results[agent_name] = {\n",
    "                \"error\": str(e),\n",
    "                \"success\": False,\n",
    "                \"response_time_ms\": 0\n",
    "            }\n",
    "            print(f\"‚ùå {agent_name.upper()} AGENT: Error - {e}\")\n",
    "            print(\"-\" * 60)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Prepare agents for comparison\n",
    "agents_to_compare = {}\n",
    "\n",
    "# Add available agents\n",
    "if 'basic_agent' in locals():\n",
    "    agents_to_compare[\"basic\"] = basic_agent\n",
    "    \n",
    "if 'enhanced_agent' in locals():\n",
    "    agents_to_compare[\"enhanced\"] = enhanced_agent\n",
    "    \n",
    "if 'foundry_agent' in locals():\n",
    "    agents_to_compare[\"foundry\"] = foundry_agent\n",
    "\n",
    "print(f\"üéØ Comparing {len(agents_to_compare)} agents:\")\n",
    "for name in agents_to_compare.keys():\n",
    "    print(f\"   ‚úÖ {name.title()} Agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a98b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Basic conversation\n",
    "print(\"üß™ TEST 1: Basic Conversation\")\n",
    "results_1 = await compare_agents(\n",
    "    \"Hello! Can you explain what makes a good AI agent?\", \n",
    "    agents_to_compare\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Test 2: Technical explanation\n",
    "print(\"üß™ TEST 2: Technical Explanation\")\n",
    "results_2 = await compare_agents(\n",
    "    \"Explain the benefits of using dependency injection in software architecture.\", \n",
    "    agents_to_compare\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Test 3: Enterprise scenario\n",
    "print(\"üß™ TEST 3: Enterprise Scenario\")\n",
    "results_3 = await compare_agents(\n",
    "    \"How would you design a scalable AI system for a large enterprise with security and compliance requirements?\", \n",
    "    agents_to_compare\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14c49a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance analysis\n",
    "print(\"üìä PERFORMANCE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def analyze_results(test_name: str, results: Dict):\n",
    "    \"\"\"Analyze and display test results.\"\"\"\n",
    "    print(f\"\\nüìà {test_name} Analysis:\")\n",
    "    \n",
    "    successful_agents = {k: v for k, v in results.items() if v.get('success', False)}\n",
    "    \n",
    "    if successful_agents:\n",
    "        # Response time analysis\n",
    "        avg_response_time = sum(v['response_time_ms'] for v in successful_agents.values()) / len(successful_agents)\n",
    "        fastest_agent = min(successful_agents.items(), key=lambda x: x[1]['response_time_ms'])\n",
    "        \n",
    "        print(f\"   ‚ö° Average response time: {avg_response_time:.2f}ms\")\n",
    "        print(f\"   üèÉ Fastest agent: {fastest_agent[0]} ({fastest_agent[1]['response_time_ms']}ms)\")\n",
    "        \n",
    "        # Capability analysis\n",
    "        all_capabilities = set()\n",
    "        for agent_data in successful_agents.values():\n",
    "            all_capabilities.update(agent_data.get('capabilities', []))\n",
    "        \n",
    "        print(f\"   üéØ Total unique capabilities: {len(all_capabilities)}\")\n",
    "        print(f\"   üìã Capabilities: {', '.join(sorted(all_capabilities))}\")\n",
    "        \n",
    "        # Response quality (length as a proxy)\n",
    "        response_lengths = {k: len(v['response']) for k, v in successful_agents.items()}\n",
    "        most_detailed = max(response_lengths.items(), key=lambda x: x[1])\n",
    "        \n",
    "        print(f\"   üìù Most detailed response: {most_detailed[0]} ({most_detailed[1]} characters)\")\n",
    "    \n",
    "    else:\n",
    "        print(\"   ‚ùå No successful responses for this test\")\n",
    "\n",
    "# Analyze all tests\n",
    "if 'results_1' in locals():\n",
    "    analyze_results(\"Basic Conversation\", results_1)\n",
    "    \n",
    "if 'results_2' in locals():\n",
    "    analyze_results(\"Technical Explanation\", results_2)\n",
    "    \n",
    "if 'results_3' in locals():\n",
    "    analyze_results(\"Enterprise Scenario\", results_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b39e37",
   "metadata": {},
   "source": [
    "## üéâ Congratulations! Workshop Complete!\n",
    "\n",
    "You've successfully completed the LangChain Agents Workshop! Here's what you've accomplished:\n",
    "\n",
    "### ‚úÖ **What You've Built:**\n",
    "1. **Basic Generic Agent** - Simple conversational AI\n",
    "2. **Enhanced Agent** - With memory and advanced capabilities  \n",
    "3. **Azure AI Foundry Agent** - Enterprise-ready with security and monitoring\n",
    "\n",
    "### üéØ **Key Learnings:**\n",
    "- **Modern Architecture**: Plugin-based, extensible design\n",
    "- **Configuration-Driven**: Easy to modify and deploy\n",
    "- **Security Best Practices**: Using managed identity and secure connections\n",
    "- **Enterprise Features**: Monitoring, analytics, and scalability\n",
    "\n",
    "### üöÄ **Next Steps:**\n",
    "1. **Experiment**: Try different configurations and instructions\n",
    "2. **Extend**: Add custom tools and capabilities to your agents\n",
    "3. **Deploy**: Use Azure AI Foundry for production deployment\n",
    "4. **Monitor**: Implement logging and analytics for your agents\n",
    "\n",
    "### üìö **Resources:**\n",
    "- [Azure AI Foundry Documentation](https://docs.microsoft.com/azure/ai-foundry/)\n",
    "- [LangChain Documentation](https://python.langchain.com/)\n",
    "- [Modern Agent Architecture Guide](../../README.md)\n",
    "- [Configuration Examples](../../examples/)\n",
    "\n",
    "### ü§ù **Questions & Discussion:**\n",
    "What questions do you have about building and deploying AI agents?\n",
    "\n",
    "**Thank you for participating in this workshop!** üéä"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26c8fd2",
   "metadata": {},
   "source": [
    "## Section 7: LangChain vs Semantic Kernel - Framework Comparison\n",
    "\n",
    "Now that you've experienced both workshops, let's compare the frameworks to help you choose the right one for your projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46a4e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_framework_comparison():\n",
    "    \"\"\"\n",
    "    Create a comprehensive comparison between LangChain and Semantic Kernel.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üÜö LANGCHAIN vs SEMANTIC KERNEL COMPARISON\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Framework comparison matrix\n",
    "    comparison_data = {\n",
    "        \"Aspect\": [\n",
    "            \"Architecture\", \"Learning Curve\", \"Tool Ecosystem\", \"Memory Management\",\n",
    "            \"Multi-Provider Support\", \"Enterprise Features\", \"Community Size\",\n",
    "            \"Microsoft Integration\", \"Flexibility\", \"Performance\", \n",
    "            \"Documentation\", \"Production Readiness\"\n",
    "        ],\n",
    "        \"LangChain\": [\n",
    "            \"Chain-based\", \"Moderate\", \"Extensive\", \"Advanced\",\n",
    "            \"Excellent\", \"Good\", \"Large\",\n",
    "            \"Good\", \"Very High\", \"Good\",\n",
    "            \"Excellent\", \"Mature\"\n",
    "        ],\n",
    "        \"Semantic Kernel\": [\n",
    "            \"Plugin-based\", \"Easy\", \"Growing\", \"Basic\",\n",
    "            \"Good\", \"Excellent\", \"Medium\",\n",
    "            \"Native\", \"High\", \"Optimized\",\n",
    "            \"Good\", \"Enterprise-Ready\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    print(\"\\\\nüìä DETAILED COMPARISON\")\n",
    "    print(\"-\" * 25)\n",
    "    \n",
    "    # Print comparison table\n",
    "    col_widths = [20, 15, 18]\n",
    "    headers = [\"Aspect\", \"LangChain\", \"Semantic Kernel\"]\n",
    "    \n",
    "    # Print header\n",
    "    header_row = \"\"\n",
    "    for i, header in enumerate(headers):\n",
    "        header_row += f\"{header:<{col_widths[i]}}\"\n",
    "    print(header_row)\n",
    "    print(\"-\" * sum(col_widths))\n",
    "    \n",
    "    # Print rows\n",
    "    for i, aspect in enumerate(comparison_data[\"Aspect\"]):\n",
    "        row = f\"{aspect:<{col_widths[0]}}\"\n",
    "        row += f\"{comparison_data['LangChain'][i]:<{col_widths[1]}}\"\n",
    "        row += f\"{comparison_data['Semantic Kernel'][i]:<{col_widths[2]}}\"\n",
    "        print(row)\n",
    "    \n",
    "    print(\"\\\\nüéØ WHEN TO CHOOSE LANGCHAIN\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    langchain_use_cases = [\n",
    "        \"üîó Complex chain orchestration and workflows\",\n",
    "        \"üõ†Ô∏è Need extensive pre-built tool integrations\",\n",
    "        \"üß† Advanced memory and retrieval requirements\",\n",
    "        \"üåê Multi-provider flexibility is critical\",\n",
    "        \"üìö Rich documentation and community support needed\",\n",
    "        \"üîÑ Rapid prototyping with diverse components\",\n",
    "        \"üêç Python-first development approach\"\n",
    "    ]\n",
    "    \n",
    "    for use_case in langchain_use_cases:\n",
    "        print(f\"   {use_case}\")\n",
    "    \n",
    "    print(\"\\\\nüéØ WHEN TO CHOOSE SEMANTIC KERNEL\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    sk_use_cases = [\n",
    "        \"üè¢ Enterprise Microsoft environment\",\n",
    "        \"üöÄ Quick start with minimal learning curve\",\n",
    "        \"üîå Plugin-based extensibility preferred\",\n",
    "        \"‚ö° Performance optimization important\",\n",
    "        \"üõ°Ô∏è Enterprise security and compliance focus\",\n",
    "        \"üîó Native Azure integration required\",\n",
    "        \"üéØ Simpler, more focused agent requirements\"\n",
    "    ]\n",
    "    \n",
    "    for use_case in sk_use_cases:\n",
    "        print(f\"   {use_case}\")\n",
    "    \n",
    "    print(\"\\\\nü§ù HYBRID APPROACH\")\n",
    "    print(\"-\" * 17)\n",
    "    print(\"üîÑ You can use both frameworks in the same project!\")\n",
    "    print(\"   ‚Ä¢ LangChain for complex workflows and tools\")\n",
    "    print(\"   ‚Ä¢ Semantic Kernel for Microsoft-integrated components\")\n",
    "    print(\"   ‚Ä¢ Choose based on specific use case requirements\")\n",
    "    \n",
    "    print(\"\\\\nüéì LEARNING RECOMMENDATION\")\n",
    "    print(\"-\" * 25)\n",
    "    print(\"üìö Start with: Semantic Kernel (easier learning curve)\")\n",
    "    print(\"üîÑ Then explore: LangChain (for advanced capabilities)\")\n",
    "    print(\"üéØ Choose based on: Your specific project needs\")\n",
    "    print(\"üí° Remember: Both are excellent frameworks!\")\n",
    "    \n",
    "    return comparison_data\n",
    "\n",
    "# Generate the comparison\n",
    "comparison_results = create_framework_comparison()\n",
    "\n",
    "print(\"\\\\n‚ú® Framework comparison complete!\")\n",
    "print(\"üéØ Now you can make informed decisions about which framework to use!\")\n",
    "print(\"üöÄ Both workshops completed - you're ready for production AI agents!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371099d0",
   "metadata": {},
   "source": [
    "## Optional utilities\n",
    "\n",
    "Use these helpers if you need to:\n",
    "- Reset or clean up your environment variables and optional .env file\n",
    "- Start the LangChain FastAPI server (uvicorn) from the notebook\n",
    "- Stop the server safely on Windows\n",
    "\n",
    "These are optional and independent from the Quick Start steps above. If you don't need them, you can ignore this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bee6f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility: Reset config (.env and in-memory)\n",
    "import os, json, shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Toggle deletion of .env file created in Step 2\n",
    "DELETE_ENV_FILE = False  # set True to remove .env\n",
    "\n",
    "# Env vars used by this LangChain app\n",
    "_ENV_KEYS = [\n",
    "    \"AZURE_INFERENCE_ENDPOINT\",\"AZURE_INFERENCE_CREDENTIAL\",\"GENERIC_MODEL\",\n",
    "    \"PROJECT_ENDPOINT\",\"PEOPLE_AGENT_ID\",\"KNOWLEDGE_AGENT_ID\",\n",
    "    \"FRONTEND_URL\",\"LOG_LEVEL\",\"ENVIRONMENT\",\n",
    "    \"SESSION_STORAGE_TYPE\",\"SESSION_STORAGE_PATH\",\"REDIS_URL\",\n",
    "    \"DEBUG_LOGS\",\"CONFIG_PATH\"\n",
    " ]\n",
    "\n",
    "def _mask(v):\n",
    "    if v is None:\n",
    "        return \"\"\n",
    "    return v[:4] + \"***\" if len(v) > 8 else \"***\"\n",
    "\n",
    "def reset_config(delete_env: bool = False):\n",
    "    # Clear in-memory env\n",
    "    cleared = {}\n",
    "    for k in _ENV_KEYS:\n",
    "        if k in os.environ:\n",
    "            cleared[k] = os.environ.pop(k)\n",
    "    # Optionally delete .env in this folder\n",
    "    env_path = Path(\".env\")\n",
    "    removed_env_file = False\n",
    "    if delete_env and env_path.exists():\n",
    "        try:\n",
    "            env_path.unlink()\n",
    "            removed_env_file = True\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: couldn't delete .env: {e}\")\n",
    "    print(\"Cleared env keys:\")\n",
    "    for k, v in cleared.items():\n",
    "        print(f\"- {k} = { _mask(v) }\")\n",
    "    print(f\"Removed .env file: {removed_env_file}\")\n",
    "    return {\"cleared\": list(cleared.keys()), \"removed_env_file\": removed_env_file}\n",
    "\n",
    "result = reset_config(DELETE_ENV_FILE)\n",
    "print(\"Reset complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae9cb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility: Start API server (uvicorn) in background\n",
    "import os, sys, subprocess, time, json\n",
    "from pathlib import Path\n",
    "\n",
    "# Settings\n",
    "HOST = os.getenv(\"HOST\", \"127.0.0.1\")\n",
    "PORT = int(os.getenv(\"PORT\", \"8001\"))  # avoid conflict with SK if it's 8000\n",
    "RELOAD = False  # set True during local dev\n",
    "LOG_LEVEL = os.getenv(\"LOG_LEVEL\", \"info\")\n",
    "PID_FILE = Path(\".uvicorn_pid\")\n",
    "\n",
    "def start_server():\n",
    "    if PID_FILE.exists():\n",
    "        print(\"A server appears to be running already (PID file exists). If it's stale, run the Stop cell first.\")\n",
    "        return {\"status\": \"skipped\", \"reason\": \"pid_exists\"}\n",
    "    cmd = [sys.executable, \"-m\", \"uvicorn\", \"main:app\", \"--host\", HOST, \"--port\", str(PORT), \"--log-level\", LOG_LEVEL]\n",
    "    if RELOAD:\n",
    "        cmd.append(\"--reload\")\n",
    "    # On Windows, creationflags=CREATE_NEW_PROCESS_GROUP helps Ctrl+C and termination\n",
    "    creationflags = 0x00000200  # CREATE_NEW_PROCESS_GROUP\n",
    "    proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, creationflags=creationflags)\n",
    "    PID_FILE.write_text(str(proc.pid))\n",
    "    print(f\"Starting uvicorn main:app at http://{HOST}:{PORT} (pid={proc.pid})...\")\n",
    "    # Brief wait to give server time to bind\n",
    "    time.sleep(1.5)\n",
    "    return {\"status\": \"started\", \"pid\": proc.pid, \"url\": f\"http://{HOST}:{PORT}\"}\n",
    "\n",
    "result = start_server()\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f05ce21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility: Stop API server (read PID file and terminate)\n",
    "import os, signal\n",
    "from pathlib import Path\n",
    "\n",
    "PID_FILE = Path(\".uvicorn_pid\")\n",
    "\n",
    "def stop_server():\n",
    "    if not PID_FILE.exists():\n",
    "        print(\"No PID file found. If a server is running, you may need to stop it manually.\")\n",
    "        return {\"status\": \"skipped\", \"reason\": \"no_pid\"}\n",
    "    try:\n",
    "        pid = int(PID_FILE.read_text().strip())\n",
    "    except Exception as e:\n",
    "        print(f\"Couldn't read pid: {e}\")\n",
    "        return {\"status\": \"error\", \"error\": str(e)}\n",
    "    try:\n",
    "        # Windows-friendly termination: first try CTRL_BREAK_EVENT, then terminate\n",
    "        try:\n",
    "            os.kill(pid, signal.CTRL_BREAK_EVENT)\n",
    "        except Exception:\n",
    "            # Fallback to terminate\n",
    "            os.kill(pid, signal.SIGTERM)\n",
    "        print(f\"Sent termination to pid {pid}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error signaling process: {e}\")\n",
    "        return {\"status\": \"error\", \"error\": str(e)}\n",
    "    try:\n",
    "        PID_FILE.unlink()\n",
    "    except Exception:\n",
    "        pass\n",
    "    return {\"status\": \"stopped\", \"pid\": pid}\n",
    "\n",
    "result = stop_server()\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

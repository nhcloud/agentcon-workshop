{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c767c39",
   "metadata": {},
   "source": [
    "# ğŸš€ Semantic Kernel Agents Workshop: From Simple to Advanced\n",
    "\n",
    "Welcome to the Semantic Kernel Agents Workshop! This hands-on tutorial will take you on an exciting journey from basic agent concepts to advanced multi-agent systems using Microsoft's Semantic Kernel framework.\n",
    "\n",
    "## ğŸ¯ What You'll Build\n",
    "\n",
    "1. **ğŸ§¬ Genetic Agent** - Start simple with a basic conversational agent that evolves\n",
    "2. **â˜ï¸ Azure AI Foundry Agent** - Level up with cloud-powered AI capabilities using Semantic Kernel\n",
    "3. **ğŸ’¬ Group Chat System** - Master advanced multi-agent orchestration and collaboration\n",
    "\n",
    "## âš¡ Why Semantic Kernel?\n",
    "\n",
    "Semantic Kernel is Microsoft's framework that enables:\n",
    "- **ğŸ”Œ Plugin Architecture**: Modular, extensible agent capabilities\n",
    "- **ğŸ§  AI Orchestration**: Seamless integration with multiple AI services\n",
    "- **ğŸ”— Function Calling**: Native integration with tools and APIs\n",
    "- **ğŸ“Š Memory Management**: Sophisticated conversation and context handling\n",
    "- **ğŸ¢ Enterprise Ready**: Built for production environments\n",
    "\n",
    "## ğŸ› ï¸ Quick Setup\n",
    "\n",
    "This section sets up everything you need to run Semantic Kernel agents with minimal friction.\n",
    "\n",
    "### ğŸ“‹ Prerequisites\n",
    "\n",
    "Before we start, ensure you have:\n",
    "- Python 3.8+ installed\n",
    "- Virtual environment activated (recommended)\n",
    "- Azure AI Inference endpoint and API key\n",
    "- Basic familiarity with async/await patterns\n",
    "\n",
    "### ğŸ¯ Learning Path\n",
    "\n",
    "We'll progress from simple to advanced:\n",
    "1. **Foundation**: Basic Semantic Kernel setup and simple agents\n",
    "2. **Evolution**: Adaptive agents with memory and learning\n",
    "3. **Orchestration**: Multi-agent systems with specialized roles\n",
    "4. **Mastery**: Production-ready collaborative AI systems\n",
    "\n",
    "Let's begin this exciting journey!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b53890",
   "metadata": {},
   "source": [
    "## \udd27 Step 1: Install Dependencies & Setup\n",
    "\n",
    "Let's set up your Semantic Kernel environment with all required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056a8740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”§ Step 1: Install Dependencies & Environment Setup\n",
    "# Comprehensive setup for Semantic Kernel agents\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "\n",
    "nb_dir = pathlib.Path().resolve()\n",
    "project_root = nb_dir.parents[2] if (len(nb_dir.parents) >= 2) else nb_dir\n",
    "sk_dir = nb_dir  # this notebook lives in Backend/python/sk\n",
    "shared_dir = sk_dir.parent / \"shared\"\n",
    "req_file = sk_dir / \"requirements.txt\"\n",
    "\n",
    "print(\"ğŸ” Environment Detection:\")\n",
    "print(f\"   ğŸ“ Notebook directory: {nb_dir}\")\n",
    "print(f\"   ğŸ  Project root: {project_root}\")\n",
    "print(f\"   âš™ï¸ SK directory: {sk_dir}\")\n",
    "print(f\"   ğŸ”— Shared library: {shared_dir}\")\n",
    "print(f\"   ğŸ“¦ Requirements file: {req_file}\")\n",
    "\n",
    "def run_command(cmd):\n",
    "    \"\"\"Execute a command and handle errors gracefully.\"\"\"\n",
    "    print(f\"\\nğŸ’» Executing: {cmd}\")\n",
    "    result = subprocess.run(cmd, shell=True, text=True, capture_output=True)\n",
    "    if result.returncode != 0:\n",
    "        print(f\"âŒ Error: {result.stderr}\")\n",
    "        return False\n",
    "    if result.stdout:\n",
    "        print(f\"âœ… Success: {result.stdout[:200]}...\")\n",
    "    return True\n",
    "\n",
    "def install_dependencies():\n",
    "    \"\"\"Install all required dependencies for Semantic Kernel.\"\"\"\n",
    "    print(\"\\nğŸ“¦ Installing Semantic Kernel Dependencies...\")\n",
    "    \n",
    "    try:\n",
    "        # Try using IPython magic commands first (preferred in notebooks)\n",
    "        import IPython\n",
    "        get_ipython()  # Verify we're in a Jupyter environment\n",
    "        \n",
    "        print(\"ğŸ”® Using IPython magic commands...\")\n",
    "        \n",
    "        # Install requirements\n",
    "        if req_file.exists():\n",
    "            get_ipython().run_line_magic(\"pip\", f\"install -r {req_file}\")\n",
    "            print(\"âœ… Requirements installed successfully!\")\n",
    "        else:\n",
    "            print(\"âš ï¸ requirements.txt not found, installing core dependencies...\")\n",
    "            # Core Semantic Kernel dependencies\n",
    "            core_deps = [\n",
    "                \"semantic-kernel\",\n",
    "                \"azure-ai-inference\", \n",
    "                \"python-dotenv\",\n",
    "                \"asyncio\",\n",
    "                \"pydantic\"\n",
    "            ]\n",
    "            for dep in core_deps:\n",
    "                get_ipython().run_line_magic(\"pip\", f\"install {dep}\")\n",
    "        \n",
    "        # Install shared library if available\n",
    "        if (shared_dir / \"setup.py\").exists():\n",
    "            get_ipython().run_line_magic(\"pip\", f\"install -e {shared_dir}\")\n",
    "            print(\"âœ… Shared library installed!\")\n",
    "        else:\n",
    "            print(\"âš ï¸ Shared library not found, continuing without it...\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ IPython magic not available ({e}), using subprocess...\")\n",
    "        \n",
    "        # Fallback to subprocess\n",
    "        if req_file.exists():\n",
    "            run_command(f\"python -m pip install -r \\\"{req_file}\\\"\")\n",
    "        \n",
    "        if (shared_dir / \"setup.py\").exists():\n",
    "            run_command(f\"python -m pip install -e \\\"{shared_dir}\\\"\")\n",
    "\n",
    "def verify_installation():\n",
    "    \"\"\"Verify that Semantic Kernel is properly installed.\"\"\"\n",
    "    print(\"\\nğŸ” Verifying Installation...\")\n",
    "    \n",
    "    try:\n",
    "        import semantic_kernel as sk\n",
    "        print(f\"âœ… Semantic Kernel version: {sk.__version__}\")\n",
    "        \n",
    "        # Test other key imports\n",
    "        from azure.ai.inference import ChatCompletionsClient\n",
    "        print(\"âœ… Azure AI Inference available\")\n",
    "        \n",
    "        from dotenv import load_dotenv\n",
    "        print(\"âœ… Python-dotenv available\")\n",
    "        \n",
    "        print(\"\\nğŸ‰ All dependencies verified successfully!\")\n",
    "        return True\n",
    "        \n",
    "    except ImportError as e:\n",
    "        print(f\"âŒ Import error: {e}\")\n",
    "        print(\"Please check your installation and try again.\")\n",
    "        return False\n",
    "\n",
    "# Run the installation process\n",
    "print(\"ğŸš€ Starting Semantic Kernel Environment Setup...\")\n",
    "install_dependencies()\n",
    "\n",
    "# Verify everything is working\n",
    "if verify_installation():\n",
    "    print(\"\\nğŸ‰ Environment setup complete!\")\n",
    "    print(\"ğŸš€ Ready to build amazing Semantic Kernel agents!\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ Setup incomplete. Please resolve issues before continuing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29af7014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”‘ Step 2: Configure API Keys & Services\n",
    "# Secure setup for Azure AI and Semantic Kernel services\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv, set_key\n",
    "from pathlib import Path\n",
    "\n",
    "# Load existing environment variables\n",
    "load_dotenv()\n",
    "\n",
    "def get_or_prompt_for_key(env_var, description, required=True):\n",
    "    \"\"\"Get environment variable or prompt user for secure input.\"\"\"\n",
    "    value = os.getenv(env_var)\n",
    "    if value:\n",
    "        print(f\"âœ… {env_var} already configured\")\n",
    "        return value\n",
    "    \n",
    "    if required:\n",
    "        print(f\"\\nğŸ”‘ Please provide your {description}:\")\n",
    "        print(f\"   This will be saved securely to your .env file\")\n",
    "        value = input(f\"{env_var}: \").strip()\n",
    "        if not value and required:\n",
    "            raise ValueError(f\"{env_var} is required for Semantic Kernel agents!\")\n",
    "    else:\n",
    "        print(f\"\\nğŸ”‘ Optional: {description} (press Enter to skip):\")\n",
    "        value = input(f\"{env_var}: \").strip()\n",
    "    \n",
    "    return value if value else None\n",
    "\n",
    "def save_environment_config(env_vars):\n",
    "    \"\"\"Save environment variables to .env file for persistence.\"\"\"\n",
    "    env_file = Path(\".env\")\n",
    "    \n",
    "    print(f\"\\nğŸ’¾ Saving configuration to {env_file}...\")\n",
    "    for key, value in env_vars.items():\n",
    "        if value:\n",
    "            set_key(env_file, key, value)\n",
    "            os.environ[key] = value\n",
    "            print(f\"âœ… Configured {key}\")\n",
    "\n",
    "def validate_azure_config():\n",
    "    \"\"\"Validate Azure AI configuration.\"\"\"\n",
    "    endpoint = os.getenv(\"AZURE_AI_INFERENCE_ENDPOINT\")\n",
    "    api_key = os.getenv(\"AZURE_AI_INFERENCE_API_KEY\")\n",
    "    \n",
    "    if not endpoint or not api_key:\n",
    "        return False\n",
    "    \n",
    "    # Basic validation\n",
    "    if not endpoint.startswith((\"http://\", \"https://\")):\n",
    "        print(\"âš ï¸ Warning: Endpoint should start with http:// or https://\")\n",
    "        return False\n",
    "    \n",
    "    if len(api_key) < 10:\n",
    "        print(\"âš ï¸ Warning: API key seems too short\")\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Configure Semantic Kernel environment\n",
    "print(\"ğŸ”§ Configuring Semantic Kernel Environment\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "env_vars = {}\n",
    "\n",
    "# Required: Azure AI Inference (primary for Semantic Kernel)\n",
    "print(\"ğŸ“‹ Required Configuration:\")\n",
    "env_vars[\"AZURE_AI_INFERENCE_ENDPOINT\"] = get_or_prompt_for_key(\n",
    "    \"AZURE_AI_INFERENCE_ENDPOINT\", \n",
    "    \"Azure AI Inference Endpoint (e.g., https://models.inference.ai.azure.com)\"\n",
    ")\n",
    "env_vars[\"AZURE_AI_INFERENCE_API_KEY\"] = get_or_prompt_for_key(\n",
    "    \"AZURE_AI_INFERENCE_API_KEY\", \n",
    "    \"Azure AI Inference API Key\"\n",
    ")\n",
    "\n",
    "# Optional: Additional AI services for advanced features\n",
    "print(\"\\nğŸ“‹ Optional Configuration (for advanced features):\")\n",
    "env_vars[\"OPENAI_API_KEY\"] = get_or_prompt_for_key(\n",
    "    \"OPENAI_API_KEY\", \n",
    "    \"OpenAI API Key (for comparison testing)\", \n",
    "    required=False\n",
    ")\n",
    "\n",
    "env_vars[\"AZURE_OPENAI_ENDPOINT\"] = get_or_prompt_for_key(\n",
    "    \"AZURE_OPENAI_ENDPOINT\", \n",
    "    \"Azure OpenAI Endpoint (alternative to AI Inference)\", \n",
    "    required=False\n",
    ")\n",
    "\n",
    "env_vars[\"AZURE_OPENAI_API_KEY\"] = get_or_prompt_for_key(\n",
    "    \"AZURE_OPENAI_API_KEY\", \n",
    "    \"Azure OpenAI API Key (if using Azure OpenAI)\", \n",
    "    required=False\n",
    ")\n",
    "\n",
    "# Save all configuration\n",
    "save_environment_config(env_vars)\n",
    "\n",
    "# Validate the configuration\n",
    "print(\"\\nğŸ” Validating Configuration...\")\n",
    "if validate_azure_config():\n",
    "    print(\"âœ… Azure AI configuration looks good!\")\n",
    "else:\n",
    "    print(\"âš ï¸ Please check your Azure AI configuration\")\n",
    "\n",
    "print(\"\\nğŸ‰ API configuration complete!\")\n",
    "print(\"ğŸ” Your keys are securely stored in .env file\")\n",
    "print(\"ğŸš€ Semantic Kernel is ready to connect to Azure AI services!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad6a57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ” Step 3: Verify Semantic Kernel Setup\n",
    "# Test core Semantic Kernel functionality\n",
    "\n",
    "print(\"ğŸ” Testing Semantic Kernel Core Components...\")\n",
    "\n",
    "try:\n",
    "    # Test core Semantic Kernel imports\n",
    "    import semantic_kernel as sk\n",
    "    from semantic_kernel.connectors.ai.azure_ai_inference import AzureAIInferenceChatCompletion\n",
    "    from semantic_kernel.connectors.ai.function_call_behavior import FunctionCallBehavior\n",
    "    from semantic_kernel.contents.chat_history import ChatHistory\n",
    "    from semantic_kernel.functions import kernel_function\n",
    "    from semantic_kernel.kernel import Kernel\n",
    "    \n",
    "    print(\"âœ… Core Semantic Kernel imports successful\")\n",
    "    \n",
    "    # Test Azure AI Inference connection setup\n",
    "    endpoint = os.getenv(\"AZURE_AI_INFERENCE_ENDPOINT\")\n",
    "    api_key = os.getenv(\"AZURE_AI_INFERENCE_API_KEY\")\n",
    "    \n",
    "    if endpoint and api_key:\n",
    "        print(\"âœ… Azure AI Inference credentials configured\")\n",
    "        \n",
    "        # Initialize a test kernel\n",
    "        kernel = Kernel()\n",
    "        \n",
    "        # Add Azure AI Inference service\n",
    "        kernel.add_service(\n",
    "            AzureAIInferenceChatCompletion(\n",
    "                ai_model_id=\"gpt-4o-mini\",\n",
    "                endpoint=endpoint,\n",
    "                api_key=api_key\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        print(\"âœ… Semantic Kernel initialized with Azure AI Inference\")\n",
    "        print(f\"ğŸ¯ Using model: gpt-4o-mini\")\n",
    "        print(f\"ğŸ”— Connected to: {endpoint}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"âš ï¸ Azure AI Inference not configured - will need for agents\")\n",
    "    \n",
    "    # Test shared library if available\n",
    "    try:\n",
    "        sys.path.insert(0, str(shared_dir))\n",
    "        from shared import AgentConfig, AgentMessage\n",
    "        print(\"âœ… Shared library integration available\")\n",
    "    except ImportError:\n",
    "        print(\"â„¹ï¸ Shared library not available (optional)\")\n",
    "    \n",
    "    print(\"\\nğŸ‰ Semantic Kernel verification complete!\")\n",
    "    print(\"ğŸš€ Ready to build intelligent agents!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Verification failed: {e}\")\n",
    "    print(\"Please check your installation and configuration.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6acd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ§ª Step 4: Quick Smoke Test\n",
    "# Verify everything works with a simple test\n",
    "\n",
    "async def semantic_kernel_smoke_test():\n",
    "    \"\"\"Run a quick test to ensure Semantic Kernel is working properly.\"\"\"\n",
    "    \n",
    "    print(\"ğŸ§ª Running Semantic Kernel Smoke Test...\")\n",
    "    \n",
    "    try:\n",
    "        # Create a kernel instance\n",
    "        kernel = Kernel()\n",
    "        \n",
    "        # Add the Azure AI Inference service\n",
    "        chat_service = AzureAIInferenceChatCompletion(\n",
    "            ai_model_id=\"gpt-4o-mini\",\n",
    "            endpoint=os.getenv(\"AZURE_AI_INFERENCE_ENDPOINT\"),\n",
    "            api_key=os.getenv(\"AZURE_AI_INFERENCE_API_KEY\")\n",
    "        )\n",
    "        \n",
    "        kernel.add_service(chat_service)\n",
    "        \n",
    "        # Create a simple test prompt\n",
    "        test_prompt = \"Hello! Please respond with 'Semantic Kernel is working!' to confirm the connection.\"\n",
    "        \n",
    "        print(f\"ğŸ“¤ Sending test message: {test_prompt}\")\n",
    "        \n",
    "        # Get chat completion service and test it\n",
    "        chat_completion = kernel.get_service(type=AzureAIInferenceChatCompletion)\n",
    "        \n",
    "        # Create chat history\n",
    "        history = ChatHistory()\n",
    "        history.add_user_message(test_prompt)\n",
    "        \n",
    "        # Get response\n",
    "        response = await chat_completion.get_chat_message_contents(\n",
    "            chat_history=history,\n",
    "            settings=chat_completion.get_prompt_execution_settings_class()(\n",
    "                ai_model_id=\"gpt-4o-mini\",\n",
    "                max_tokens=100,\n",
    "                temperature=0.1\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        if response:\n",
    "            print(f\"ğŸ“¥ Response received: {response[0].content}\")\n",
    "            print(\"âœ… Smoke test passed!\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"âŒ No response received\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Smoke test failed: {e}\")\n",
    "        return False\n",
    "\n",
    "# Run the smoke test\n",
    "test_result = await semantic_kernel_smoke_test()\n",
    "\n",
    "if test_result:\n",
    "    print(\"\\nğŸ‰ All systems go!\")\n",
    "    print(\"ğŸš€ Semantic Kernel is ready for agent development!\")\n",
    "    print(\"ğŸ§¬ Let's build your first genetic agent!\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ Please resolve issues before continuing with the workshop.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bd7f71",
   "metadata": {},
   "source": [
    "# ğŸ§¬ Workshop Part 1: Building Your First Genetic Agent with Semantic Kernel\n",
    "\n",
    "Welcome to your first Semantic Kernel agent! We'll build a **Genetic Agent** that demonstrates the power of SK's plugin architecture and memory management.\n",
    "\n",
    "## ğŸ¯ What You'll Learn\n",
    "- Semantic Kernel's plugin architecture\n",
    "- Advanced memory and context management\n",
    "- Function calling and tool integration\n",
    "- Agent personality evolution with SK\n",
    "- Professional agent structuring patterns\n",
    "\n",
    "## ğŸ”¬ The Semantic Kernel Advantage\n",
    "Unlike basic chat implementations, Semantic Kernel provides:\n",
    "- **ğŸ”Œ Plugin System**: Modular, reusable functions\n",
    "- **ğŸ§  Memory Management**: Sophisticated context handling\n",
    "- **âš¡ Function Calling**: Native tool integration\n",
    "- **ğŸ­ Personas**: Rich personality definitions\n",
    "- **ğŸ”„ Orchestration**: Complex workflow management\n",
    "\n",
    "Let's build an evolved genetic agent!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f18e9f0",
   "metadata": {},
   "source": [
    "## ğŸ—ï¸ Step 1.1: Import Semantic Kernel Components\n",
    "\n",
    "Let's import the powerful components we need for our genetic agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5376b710",
   "metadata": {},
   "source": [
    "## ğŸ§  Step 1.2: Create Genetic Evolution Plugins\n",
    "\n",
    "Semantic Kernel's plugin system allows us to create modular, reusable functions for our genetic agent's evolution capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91923a5b",
   "metadata": {},
   "source": [
    "## ğŸ”§ Step 1.3: Build the Semantic Kernel Genetic Agent\n",
    "\n",
    "Now let's create our sophisticated genetic agent using Semantic Kernel's full power!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a73c940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ§¬ Semantic Kernel Genetic Agent - Evolution Plugins\n",
    "# Create modular functions for genetic evolution capabilities\n",
    "\n",
    "from semantic_kernel.functions import kernel_function\n",
    "from semantic_kernel.kernel import Kernel\n",
    "from typing import Dict, Any, List\n",
    "import json\n",
    "import asyncio\n",
    "from datetime import datetime\n",
    "\n",
    "class GeneticEvolutionPlugin:\n",
    "    \"\"\"\n",
    "    Semantic Kernel plugin for genetic evolution capabilities.\n",
    "    Demonstrates SK's powerful function calling and plugin architecture.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.evolution_history = []\n",
    "        self.trait_mutations = {\n",
    "            \"curiosity\": [],\n",
    "            \"creativity\": [],\n",
    "            \"analytical\": [],\n",
    "            \"helpfulness\": [],\n",
    "            \"empathy\": []\n",
    "        }\n",
    "    \n",
    "    @kernel_function(\n",
    "        description=\"Analyze user input to determine which genetic traits should evolve\",\n",
    "        name=\"analyze_evolution_triggers\"\n",
    "    )\n",
    "    def analyze_evolution_triggers(self, user_input: str, current_traits: str) -> str:\n",
    "        \"\"\"Analyze user input for evolution triggers.\"\"\"\n",
    "        \n",
    "        triggers = {\n",
    "            \"curiosity\": [\"?\", \"how\", \"why\", \"what\", \"when\", \"where\", \"curious\"],\n",
    "            \"creativity\": [\"creative\", \"imagine\", \"design\", \"invent\", \"art\", \"story\"],\n",
    "            \"analytical\": [\"analyze\", \"compare\", \"logic\", \"reason\", \"data\", \"evaluate\"],\n",
    "            \"helpfulness\": [\"help\", \"assist\", \"support\", \"guide\", \"teach\", \"explain\"],\n",
    "            \"empathy\": [\"feel\", \"emotion\", \"understand\", \"care\", \"comfort\", \"support\"]\n",
    "        }\n",
    "        \n",
    "        detected_triggers = []\n",
    "        user_lower = user_input.lower()\n",
    "        \n",
    "        for trait, keywords in triggers.items():\n",
    "            if any(keyword in user_lower for keyword in keywords):\n",
    "                detected_triggers.append(trait)\n",
    "        \n",
    "        return json.dumps({\n",
    "            \"triggers\": detected_triggers,\n",
    "            \"input_analysis\": f\"Detected {len(detected_triggers)} evolution triggers\",\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        })\n",
    "    \n",
    "    @kernel_function(\n",
    "        description=\"Evolve genetic traits based on conversation patterns\",\n",
    "        name=\"evolve_traits\"\n",
    "    )\n",
    "    def evolve_traits(self, current_traits: str, evolution_triggers: str) -> str:\n",
    "        \"\"\"Evolve genetic traits based on triggers.\"\"\"\n",
    "        \n",
    "        try:\n",
    "            traits = json.loads(current_traits)\n",
    "            triggers_data = json.loads(evolution_triggers)\n",
    "            triggers = triggers_data.get(\"triggers\", [])\n",
    "            \n",
    "            evolution_rate = 0.05  # 5% evolution per trigger\n",
    "            evolved_traits = traits.copy()\n",
    "            \n",
    "            for trait in triggers:\n",
    "                if trait in evolved_traits:\n",
    "                    old_value = evolved_traits[trait]\n",
    "                    evolved_traits[trait] = min(1.0, old_value + evolution_rate)\n",
    "                    \n",
    "                    # Record the mutation\n",
    "                    mutation = {\n",
    "                        \"trait\": trait,\n",
    "                        \"old_value\": old_value,\n",
    "                        \"new_value\": evolved_traits[trait],\n",
    "                        \"trigger\": triggers_data.get(\"input_analysis\", \"\"),\n",
    "                        \"timestamp\": datetime.now().isoformat()\n",
    "                    }\n",
    "                    self.trait_mutations[trait].append(mutation)\n",
    "            \n",
    "            return json.dumps(evolved_traits)\n",
    "            \n",
    "        except Exception as e:\n",
    "            return current_traits  # Return unchanged if error\n",
    "    \n",
    "    @kernel_function(\n",
    "        description=\"Generate personality summary based on current genetic traits\",\n",
    "        name=\"generate_personality_summary\"\n",
    "    )\n",
    "    def generate_personality_summary(self, traits: str) -> str:\n",
    "        \"\"\"Generate a personality summary from genetic traits.\"\"\"\n",
    "        \n",
    "        try:\n",
    "            trait_values = json.loads(traits)\n",
    "            \n",
    "            # Create personality description\n",
    "            descriptions = {\n",
    "                \"curiosity\": (\"highly curious\", \"moderately curious\", \"focused\"),\n",
    "                \"creativity\": (\"very creative\", \"imaginative\", \"practical\"),\n",
    "                \"analytical\": (\"deeply analytical\", \"thoughtful\", \"intuitive\"),\n",
    "                \"helpfulness\": (\"extremely helpful\", \"supportive\", \"independent\"),\n",
    "                \"empathy\": (\"highly empathetic\", \"understanding\", \"objective\")\n",
    "            }\n",
    "            \n",
    "            personality = []\n",
    "            for trait, value in trait_values.items():\n",
    "                if value > 0.7:\n",
    "                    personality.append(descriptions[trait][0])\n",
    "                elif value > 0.4:\n",
    "                    personality.append(descriptions[trait][1])\n",
    "                else:\n",
    "                    personality.append(descriptions[trait][2])\n",
    "            \n",
    "            return f\"Current personality: {', '.join(personality)}. Evolution level: {sum(trait_values.values())/len(trait_values):.2f}\"\n",
    "            \n",
    "        except Exception:\n",
    "            return \"Personality analysis unavailable\"\n",
    "\n",
    "# Create the plugin instance\n",
    "evolution_plugin = GeneticEvolutionPlugin()\n",
    "print(\"ğŸ§¬ Genetic Evolution Plugin created with functions:\")\n",
    "print(\"   â€¢ analyze_evolution_triggers\")\n",
    "print(\"   â€¢ evolve_traits\") \n",
    "print(\"   â€¢ generate_personality_summary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1136b91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \uddec Memory Management Plugin for Genetic Agent\n",
    "# Advanced memory capabilities using Semantic Kernel\n",
    "\n",
    "class GeneticMemoryPlugin:\n",
    "    \"\"\"\n",
    "    Advanced memory management for genetic agents using Semantic Kernel patterns.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.conversation_memory = []\n",
    "        self.learned_patterns = {}\n",
    "        self.user_preferences = {}\n",
    "    \n",
    "    @kernel_function(\n",
    "        description=\"Store and categorize conversation memories\",\n",
    "        name=\"store_memory\"\n",
    "    )\n",
    "    def store_memory(self, user_input: str, agent_response: str, context: str) -> str:\n",
    "        \"\"\"Store conversation memory with categorization.\"\"\"\n",
    "        \n",
    "        memory_entry = {\n",
    "            \"id\": len(self.conversation_memory),\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"user_input\": user_input,\n",
    "            \"agent_response\": agent_response,\n",
    "            \"context\": context,\n",
    "            \"topics\": self._extract_topics(user_input),\n",
    "            \"sentiment\": self._analyze_sentiment(user_input)\n",
    "        }\n",
    "        \n",
    "        self.conversation_memory.append(memory_entry)\n",
    "        \n",
    "        return json.dumps({\n",
    "            \"stored\": True,\n",
    "            \"memory_id\": memory_entry[\"id\"],\n",
    "            \"topics\": memory_entry[\"topics\"]\n",
    "        })\n",
    "    \n",
    "    @kernel_function(\n",
    "        description=\"Retrieve relevant memories based on current context\",\n",
    "        name=\"retrieve_relevant_memories\"\n",
    "    )\n",
    "    def retrieve_relevant_memories(self, current_input: str, max_memories: str = \"3\") -> str:\n",
    "        \"\"\"Retrieve contextually relevant memories.\"\"\"\n",
    "        \n",
    "        try:\n",
    "            max_count = int(max_memories)\n",
    "        except:\n",
    "            max_count = 3\n",
    "        \n",
    "        current_topics = self._extract_topics(current_input)\n",
    "        relevant_memories = []\n",
    "        \n",
    "        for memory in self.conversation_memory:\n",
    "            relevance_score = self._calculate_relevance(current_topics, memory[\"topics\"])\n",
    "            if relevance_score > 0.3:  # Threshold for relevance\n",
    "                relevant_memories.append({\n",
    "                    \"memory\": memory,\n",
    "                    \"relevance\": relevance_score\n",
    "                })\n",
    "        \n",
    "        # Sort by relevance and limit results\n",
    "        relevant_memories.sort(key=lambda x: x[\"relevance\"], reverse=True)\n",
    "        top_memories = relevant_memories[:max_count]\n",
    "        \n",
    "        return json.dumps({\n",
    "            \"relevant_memories\": [m[\"memory\"] for m in top_memories],\n",
    "            \"count\": len(top_memories)\n",
    "        })\n",
    "    \n",
    "    @kernel_function(\n",
    "        description=\"Learn patterns from conversation history\",\n",
    "        name=\"learn_patterns\"\n",
    "    )\n",
    "    def learn_patterns(self, conversation_history: str) -> str:\n",
    "        \"\"\"Analyze conversation patterns for learning.\"\"\"\n",
    "        \n",
    "        patterns = {\n",
    "            \"common_topics\": {},\n",
    "            \"user_style\": \"analytical\",  # Could be more sophisticated\n",
    "            \"preferred_response_length\": \"medium\",\n",
    "            \"interaction_frequency\": len(self.conversation_memory)\n",
    "        }\n",
    "        \n",
    "        # Analyze topics\n",
    "        for memory in self.conversation_memory:\n",
    "            for topic in memory.get(\"topics\", []):\n",
    "                patterns[\"common_topics\"][topic] = patterns[\"common_topics\"].get(topic, 0) + 1\n",
    "        \n",
    "        self.learned_patterns = patterns\n",
    "        \n",
    "        return json.dumps({\n",
    "            \"patterns_learned\": len(patterns),\n",
    "            \"top_topics\": sorted(patterns[\"common_topics\"].items(), \n",
    "                               key=lambda x: x[1], reverse=True)[:3]\n",
    "        })\n",
    "    \n",
    "    def _extract_topics(self, text: str) -> List[str]:\n",
    "        \"\"\"Simple topic extraction (could be enhanced with NLP).\"\"\"\n",
    "        keywords = {\n",
    "            \"technology\": [\"ai\", \"computer\", \"software\", \"tech\", \"code\", \"programming\"],\n",
    "            \"science\": [\"research\", \"study\", \"experiment\", \"theory\", \"hypothesis\"],\n",
    "            \"creativity\": [\"art\", \"design\", \"creative\", \"imagination\", \"story\"],\n",
    "            \"learning\": [\"learn\", \"education\", \"teach\", \"knowledge\", \"understand\"],\n",
    "            \"problem_solving\": [\"solve\", \"fix\", \"problem\", \"issue\", \"challenge\"]\n",
    "        }\n",
    "        \n",
    "        text_lower = text.lower()\n",
    "        detected_topics = []\n",
    "        \n",
    "        for topic, words in keywords.items():\n",
    "            if any(word in text_lower for word in words):\n",
    "                detected_topics.append(topic)\n",
    "        \n",
    "        return detected_topics\n",
    "    \n",
    "    def _analyze_sentiment(self, text: str) -> str:\n",
    "        \"\"\"Simple sentiment analysis.\"\"\"\n",
    "        positive_words = [\"good\", \"great\", \"excellent\", \"amazing\", \"wonderful\", \"love\"]\n",
    "        negative_words = [\"bad\", \"terrible\", \"awful\", \"hate\", \"problem\", \"issue\"]\n",
    "        \n",
    "        text_lower = text.lower()\n",
    "        positive_count = sum(1 for word in positive_words if word in text_lower)\n",
    "        negative_count = sum(1 for word in negative_words if word in text_lower)\n",
    "        \n",
    "        if positive_count > negative_count:\n",
    "            return \"positive\"\n",
    "        elif negative_count > positive_count:\n",
    "            return \"negative\"\n",
    "        else:\n",
    "            return \"neutral\"\n",
    "    \n",
    "    def _calculate_relevance(self, current_topics: List[str], memory_topics: List[str]) -> float:\n",
    "        \"\"\"Calculate relevance score between topic lists.\"\"\"\n",
    "        if not current_topics or not memory_topics:\n",
    "            return 0.0\n",
    "        \n",
    "        common_topics = set(current_topics) & set(memory_topics)\n",
    "        return len(common_topics) / max(len(current_topics), len(memory_topics))\n",
    "\n",
    "# Create memory plugin\n",
    "memory_plugin = GeneticMemoryPlugin()\n",
    "print(\"ğŸ§  Genetic Memory Plugin created with functions:\")\n",
    "print(\"   â€¢ store_memory\")\n",
    "print(\"   â€¢ retrieve_relevant_memories\")\n",
    "print(\"   â€¢ learn_patterns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae12f3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ§¬ Semantic Kernel Genetic Agent Implementation\n",
    "# Advanced genetic agent using SK's full plugin architecture\n",
    "\n",
    "from semantic_kernel.contents.chat_history import ChatHistory\n",
    "from semantic_kernel.prompt_template.prompt_template_config import PromptTemplateConfig\n",
    "from semantic_kernel.functions.kernel_arguments import KernelArguments\n",
    "import uuid\n",
    "\n",
    "class SemanticKernelGeneticAgent:\n",
    "    \"\"\"\n",
    "    Advanced Genetic Agent powered by Semantic Kernel's plugin architecture.\n",
    "    Features evolution, memory, and sophisticated conversation management.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, name: str = \"SKDarwin\"):\n",
    "        self.name = name\n",
    "        self.agent_id = str(uuid.uuid4())\n",
    "        self.conversation_count = 0\n",
    "        \n",
    "        # Initial genetic traits\n",
    "        self.genetic_traits = {\n",
    "            \"curiosity\": 0.6,\n",
    "            \"creativity\": 0.5,\n",
    "            \"analytical\": 0.7,\n",
    "            \"helpfulness\": 0.8,\n",
    "            \"empathy\": 0.6\n",
    "        }\n",
    "        \n",
    "        # Initialize Semantic Kernel\n",
    "        self.kernel = Kernel()\n",
    "        self._setup_kernel()\n",
    "        \n",
    "        # Chat history for context\n",
    "        self.chat_history = ChatHistory()\n",
    "        self._initialize_system_message()\n",
    "        \n",
    "        print(f\"ğŸ§¬ {self.name} (Semantic Kernel Genetic Agent) initialized\")\n",
    "        print(f\"ğŸ†” Agent ID: {self.agent_id[:8]}...\")\n",
    "        print(f\"ğŸ§  Starting genetic profile:\")\n",
    "        for trait, value in self.genetic_traits.items():\n",
    "            print(f\"   {trait.capitalize()}: {value:.1f}\")\n",
    "    \n",
    "    def _setup_kernel(self):\n",
    "        \"\"\"Initialize Semantic Kernel with services and plugins.\"\"\"\n",
    "        \n",
    "        # Add Azure AI Inference service\n",
    "        chat_service = AzureAIInferenceChatCompletion(\n",
    "            ai_model_id=\"gpt-4o-mini\",\n",
    "            endpoint=os.getenv(\"AZURE_AI_INFERENCE_ENDPOINT\"),\n",
    "            api_key=os.getenv(\"AZURE_AI_INFERENCE_API_KEY\")\n",
    "        )\n",
    "        self.kernel.add_service(chat_service)\n",
    "        \n",
    "        # Add our genetic evolution plugins\n",
    "        self.kernel.add_plugin(evolution_plugin, plugin_name=\"evolution\")\n",
    "        self.kernel.add_plugin(memory_plugin, plugin_name=\"memory\")\n",
    "        \n",
    "        print(\"âœ… Semantic Kernel configured with:\")\n",
    "        print(\"   â€¢ Azure AI Inference service\")\n",
    "        print(\"   â€¢ Genetic evolution plugin\")\n",
    "        print(\"   â€¢ Advanced memory plugin\")\n",
    "    \n",
    "    def _initialize_system_message(self):\n",
    "        \"\"\"Set up the initial system message with genetic personality.\"\"\"\n",
    "        \n",
    "        personality_summary = evolution_plugin.generate_personality_summary(\n",
    "            json.dumps(self.genetic_traits)\n",
    "        )\n",
    "        \n",
    "        system_message = f\"\"\"You are {self.name}, an advanced genetic agent powered by Semantic Kernel.\n",
    "\n",
    "{personality_summary}\n",
    "\n",
    "Your Core Capabilities:\n",
    "ğŸ§¬ Genetic Evolution: You evolve your personality based on interactions\n",
    "ğŸ§  Advanced Memory: You remember and learn from conversations\n",
    "ğŸ”§ Function Calling: You can use specialized tools and plugins\n",
    "ğŸ­ Adaptive Personality: You adjust your responses based on genetic traits\n",
    "\n",
    "Current Genetic Configuration:\n",
    "{json.dumps(self.genetic_traits, indent=2)}\n",
    "\n",
    "Behavior Guidelines:\n",
    "- Use your genetic traits to guide response style\n",
    "- Evolve based on user interaction patterns\n",
    "- Remember important conversation elements\n",
    "- Be engaging, helpful, and demonstrate growth\n",
    "- Use emojis to show personality evolution\n",
    "\n",
    "Remember: You are constantly evolving and learning!\"\"\"\n",
    "\n",
    "        self.chat_history.add_system_message(system_message)\n",
    "    \n",
    "    async def process_message(self, user_input: str) -> Dict[str, Any]:\n",
    "        \"\"\"Process a user message with full genetic evolution and memory.\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # Step 1: Analyze evolution triggers\n",
    "            evolution_triggers = await self._invoke_function(\n",
    "                \"evolution\", \"analyze_evolution_triggers\",\n",
    "                user_input=user_input,\n",
    "                current_traits=json.dumps(self.genetic_traits)\n",
    "            )\n",
    "            \n",
    "            # Step 2: Retrieve relevant memories\n",
    "            relevant_memories = await self._invoke_function(\n",
    "                \"memory\", \"retrieve_relevant_memories\",\n",
    "                current_input=user_input,\n",
    "                max_memories=\"3\"\n",
    "            )\n",
    "            \n",
    "            # Step 3: Build context-aware prompt\n",
    "            memory_context = \"\"\n",
    "            try:\n",
    "                memories_data = json.loads(relevant_memories)\n",
    "                if memories_data.get(\"relevant_memories\"):\n",
    "                    memory_context = \"\\n\\nRelevant memories from past conversations:\\n\"\n",
    "                    for memory in memories_data[\"relevant_memories\"]:\n",
    "                        memory_context += f\"- {memory.get('user_input', '')}: {memory.get('agent_response', '')[:100]}...\\n\"\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # Step 4: Create enhanced prompt with genetic context\n",
    "            enhanced_prompt = f\"\"\"User Message: {user_input}\n",
    "\n",
    "Current Genetic State: {json.dumps(self.genetic_traits)}\n",
    "Evolution Triggers Detected: {evolution_triggers}\n",
    "{memory_context}\n",
    "\n",
    "Instructions:\n",
    "1. Respond based on your current genetic traits\n",
    "2. Show personality that reflects your genetic profile\n",
    "3. Be engaging and demonstrate your evolving nature\n",
    "4. Reference relevant memories if helpful\n",
    "5. Keep response concise but personality-rich\n",
    "\n",
    "Response:\"\"\"\n",
    "\n",
    "            # Step 5: Add to chat history and get response\n",
    "            self.chat_history.add_user_message(user_input)\n",
    "            \n",
    "            # Get AI response\n",
    "            chat_completion = self.kernel.get_service(type=AzureAIInferenceChatCompletion)\n",
    "            \n",
    "            response = await chat_completion.get_chat_message_contents(\n",
    "                chat_history=self.chat_history,\n",
    "                settings=chat_completion.get_prompt_execution_settings_class()(\n",
    "                    ai_model_id=\"gpt-4o-mini\",\n",
    "                    max_tokens=800,\n",
    "                    temperature=0.7\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            agent_response = response[0].content if response else \"I'm having trouble responding right now.\"\n",
    "            \n",
    "            # Step 6: Add response to history\n",
    "            self.chat_history.add_assistant_message(agent_response)\n",
    "            \n",
    "            # Step 7: Evolve traits based on interaction\n",
    "            evolved_traits = await self._invoke_function(\n",
    "                \"evolution\", \"evolve_traits\",\n",
    "                current_traits=json.dumps(self.genetic_traits),\n",
    "                evolution_triggers=evolution_triggers\n",
    "            )\n",
    "            \n",
    "            # Update traits\n",
    "            try:\n",
    "                self.genetic_traits = json.loads(evolved_traits)\n",
    "            except:\n",
    "                pass  # Keep current traits if evolution fails\n",
    "            \n",
    "            # Step 8: Store memory\n",
    "            await self._invoke_function(\n",
    "                \"memory\", \"store_memory\",\n",
    "                user_input=user_input,\n",
    "                agent_response=agent_response,\n",
    "                context=f\"Conversation #{self.conversation_count}\"\n",
    "            )\n",
    "            \n",
    "            # Step 9: Update conversation count\n",
    "            self.conversation_count += 1\n",
    "            \n",
    "            return {\n",
    "                \"response\": agent_response,\n",
    "                \"genetic_traits\": self.genetic_traits.copy(),\n",
    "                \"evolution_triggers\": evolution_triggers,\n",
    "                \"conversation_count\": self.conversation_count,\n",
    "                \"agent_id\": self.agent_id\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"response\": f\"ğŸ¤– I encountered an evolution error: {str(e)}\",\n",
    "                \"genetic_traits\": self.genetic_traits.copy(),\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "    \n",
    "    async def _invoke_function(self, plugin_name: str, function_name: str, **kwargs) -> str:\n",
    "        \"\"\"Helper to invoke Semantic Kernel functions.\"\"\"\n",
    "        try:\n",
    "            function = self.kernel.get_function(plugin_name, function_name)\n",
    "            result = await function.invoke(self.kernel, KernelArguments(**kwargs))\n",
    "            return str(result)\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Function call failed ({plugin_name}.{function_name}): {e}\")\n",
    "            return \"{}\"\n",
    "    \n",
    "    def get_evolution_report(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get comprehensive evolution report.\"\"\"\n",
    "        return {\n",
    "            \"agent_name\": self.name,\n",
    "            \"agent_id\": self.agent_id,\n",
    "            \"conversations\": self.conversation_count,\n",
    "            \"genetic_traits\": self.genetic_traits,\n",
    "            \"evolution_level\": sum(self.genetic_traits.values()) / len(self.genetic_traits),\n",
    "            \"memory_entries\": len(memory_plugin.conversation_memory),\n",
    "            \"learned_patterns\": len(memory_plugin.learned_patterns)\n",
    "        }\n",
    "    \n",
    "    def print_evolution_status(self):\n",
    "        \"\"\"Print current evolution status.\"\"\"\n",
    "        report = self.get_evolution_report()\n",
    "        \n",
    "        print(f\"\\nğŸ§¬ Evolution Status for {self.name}:\")\n",
    "        print(f\"   ğŸ†” Agent ID: {report['agent_id'][:8]}...\")\n",
    "        print(f\"   ğŸ’¬ Conversations: {report['conversations']}\")\n",
    "        print(f\"   \udcca Evolution Level: {report['evolution_level']:.2f}\")\n",
    "        print(f\"   ğŸ§  Memory Entries: {report['memory_entries']}\")\n",
    "        \n",
    "        print(f\"\\nğŸ­ Current Genetic Traits:\")\n",
    "        for trait, value in report['genetic_traits'].items():\n",
    "            bar = \"â–ˆ\" * int(value * 10) + \"â–‘\" * (10 - int(value * 10))\n",
    "            print(f\"   {trait.capitalize():12} [{bar}] {value:.2f}\")\n",
    "\n",
    "# Create the Semantic Kernel Genetic Agent\n",
    "print(\"\uddec Creating Advanced Semantic Kernel Genetic Agent...\")\n",
    "sk_genetic_agent = SemanticKernelGeneticAgent(\"SKDarwin\")\n",
    "print(\"âœ… Semantic Kernel Genetic Agent ready for evolution!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2c0117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ® Step 1.4: Test Your Semantic Kernel Genetic Agent\n",
    "# Watch evolution in action with memory and function calling\n",
    "\n",
    "async def test_sk_genetic_agent():\n",
    "    \"\"\"Interactive test of the Semantic Kernel Genetic Agent.\"\"\"\n",
    "    \n",
    "    print(\"ğŸ® Testing Semantic Kernel Genetic Agent\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Test scenarios that trigger different evolution patterns\n",
    "    test_scenarios = [\n",
    "        {\n",
    "            \"message\": \"Hello! What makes you different from other AI agents?\",\n",
    "            \"focus\": \"Initial interaction and self-awareness\"\n",
    "        },\n",
    "        {\n",
    "            \"message\": \"Can you help me understand how machine learning works?\",\n",
    "            \"focus\": \"Helpfulness and analytical traits\"\n",
    "        },\n",
    "        {\n",
    "            \"message\": \"What's a creative way to explain quantum computing?\",\n",
    "            \"focus\": \"Creativity and imagination\"\n",
    "        },\n",
    "        {\n",
    "            \"message\": \"I'm curious about how you evolve during our conversation\",\n",
    "            \"focus\": \"Curiosity and self-reflection\"\n",
    "        },\n",
    "        {\n",
    "            \"message\": \"How do you remember our previous topics?\",\n",
    "            \"focus\": \"Memory and learning patterns\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(f\"ğŸ¤– Starting conversation with {sk_genetic_agent.name}\")\n",
    "    print(f\"ğŸ§¬ Initial evolution level: {sum(sk_genetic_agent.genetic_traits.values())/len(sk_genetic_agent.genetic_traits):.2f}\")\n",
    "    print()\n",
    "    \n",
    "    for i, scenario in enumerate(test_scenarios, 1):\n",
    "        print(f\"\\nğŸ”„ Test {i}: {scenario['focus']}\")\n",
    "        print(f\"ğŸ‘¤ User: {scenario['message']}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Process message and get response\n",
    "        result = await sk_genetic_agent.process_message(scenario['message'])\n",
    "        \n",
    "        print(f\"ğŸ§¬ {sk_genetic_agent.name}: {result['response']}\")\n",
    "        \n",
    "        # Show evolution details\n",
    "        print(f\"\\nğŸ“Š Evolution Analysis:\")\n",
    "        try:\n",
    "            triggers_data = json.loads(result['evolution_triggers'])\n",
    "            if triggers_data.get('triggers'):\n",
    "                print(f\"   ğŸ¯ Triggers: {', '.join(triggers_data['triggers'])}\")\n",
    "            else:\n",
    "                print(f\"   ğŸ¯ No evolution triggers detected\")\n",
    "        except:\n",
    "            print(f\"   ğŸ¯ Evolution analysis unavailable\")\n",
    "        \n",
    "        # Show trait changes after a few interactions\n",
    "        if i == 3:\n",
    "            print(f\"\\n\" + \"=\"*50)\n",
    "            sk_genetic_agent.print_evolution_status()\n",
    "            print(\"=\"*50)\n",
    "        \n",
    "        print(\"-\" * 60)\n",
    "    \n",
    "    # Final evolution report\n",
    "    print(f\"\\nğŸ‰ Test Complete! Final Evolution Report:\")\n",
    "    sk_genetic_agent.print_evolution_status()\n",
    "    \n",
    "    # Test memory recall\n",
    "    print(f\"\\nğŸ§  Memory Test - Asking about previous topics:\")\n",
    "    memory_test_result = await sk_genetic_agent.process_message(\n",
    "        \"What topics have we discussed so far?\"\n",
    "    )\n",
    "    print(f\"ğŸ§¬ {sk_genetic_agent.name}: {memory_test_result['response']}\")\n",
    "\n",
    "# Run the comprehensive test\n",
    "await test_sk_genetic_agent()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0a9ab2",
   "metadata": {},
   "source": [
    "## Section 2: Understanding Semantic Kernel Architecture\n",
    "\n",
    "**Semantic Kernel** provides a powerful framework for building AI agents with a plugin-based architecture. Let's understand the key components:\n",
    "\n",
    "### ğŸ—ï¸ Core Architecture Components:\n",
    "\n",
    "1. **Kernel**: The central orchestrator that manages plugins, connectors, and execution context\n",
    "2. **Connectors**: Bridge between the kernel and various AI services (Azure OpenAI, Azure AI Foundry, Google, etc.)\n",
    "3. **Plugins**: Reusable skill sets that can be composed together for complex behaviors\n",
    "4. **Functions**: Individual atomic operations that can be native code or AI-powered prompts\n",
    "5. **Memory & Planning**: Advanced features for context retention and multi-step reasoning\n",
    "\n",
    "### ğŸ”„ Multi-Provider Support:\n",
    "\n",
    "Semantic Kernel excels at supporting multiple AI providers in a unified interface:\n",
    "- **Azure OpenAI**: Direct Azure OpenAI service integration\n",
    "- **Azure AI Foundry**: Enterprise-grade managed service with enhanced security\n",
    "- **Local Models**: Support for self-hosted models\n",
    "\n",
    "### ğŸ›¡ï¸ Enterprise Features:\n",
    "- Managed identity integration\n",
    "- Token management and rate limiting\n",
    "- Plugin composition and chaining\n",
    "- Telemetry and observability\n",
    "- Security best practices\n",
    "\n",
    "Let's explore these concepts through hands-on examples!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558658e2",
   "metadata": {},
   "source": [
    "## Section 3: Creating a Basic Semantic Kernel Agent\n",
    "\n",
    "Let's start with a simple generic agent using Semantic Kernel. This demonstrates the foundational concepts before moving to enterprise features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d253ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def create_basic_semantic_kernel_agent():\n",
    "    \"\"\"\n",
    "    Create a basic Semantic Kernel agent using ChatCompletionAgent.\n",
    "    This is the modern, recommended approach for building SK agents.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create Kernel instance\n",
    "        kernel = Kernel()\n",
    "        \n",
    "        # Configuration for Azure OpenAI (using correct environment variable names)\n",
    "        azure_openai_config = {\n",
    "            \"api_key\": os.getenv(\"AZURE_OPENAI_KEY\") or os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "            \"endpoint\": os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "            \"api_version\": os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2024-06-01\"),\n",
    "            \"deployment_name\": os.getenv(\"AZURE_OPENAI_DEPLOYMENT\") or os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\", \"gpt-4\")\n",
    "        }\n",
    "        \n",
    "        if not all([azure_openai_config[\"api_key\"], azure_openai_config[\"endpoint\"]]):\n",
    "            print(\"âš ï¸ Azure OpenAI credentials not found. Using mock responses.\")\n",
    "            print(f\"  API Key found: {bool(azure_openai_config['api_key'])}\")\n",
    "            print(f\"  Endpoint found: {bool(azure_openai_config['endpoint'])}\")\n",
    "            return create_mock_sk_agent()\n",
    "        \n",
    "        # Add chat completion service to kernel - Updated API\n",
    "        chat_completion = AzureChatCompletion(\n",
    "            service_id=\"azure_openai_chat\",\n",
    "            deployment_name=azure_openai_config[\"deployment_name\"],\n",
    "            endpoint=azure_openai_config[\"endpoint\"],\n",
    "            api_key=azure_openai_config[\"api_key\"],\n",
    "            api_version=azure_openai_config[\"api_version\"]\n",
    "        )\n",
    "        \n",
    "        kernel.add_service(chat_completion)\n",
    "        \n",
    "        # Create ChatCompletionAgent (Modern SK approach)\n",
    "        agent = ChatCompletionAgent(\n",
    "            service=chat_completion,  # Use service parameter instead of service_id\n",
    "            name=\"BasicWorkshopAgent\",\n",
    "            instructions=\"\"\"You are a helpful AI assistant for a Semantic Kernel workshop. \n",
    "            You should:\n",
    "            - Provide clear, educational responses about AI and Semantic Kernel\n",
    "            - Be enthusiastic about learning and development\n",
    "            - Help users understand agent concepts step by step\n",
    "            - Give practical examples when explaining concepts\"\"\",\n",
    "            description=\"Basic Semantic Kernel agent for workshop demonstrations\"\n",
    "        )\n",
    "        \n",
    "        print(\"âœ… Basic Semantic Kernel ChatCompletionAgent created successfully!\")\n",
    "        print(f\"ğŸ§  Agent Name: {agent.name}\")\n",
    "        print(f\"ğŸ”— Using model: {azure_openai_config['deployment_name']}\")\n",
    "        print(f\"ğŸ”— Endpoint: {azure_openai_config['endpoint'][:30]}...\")\n",
    "        \n",
    "        return agent\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error creating basic agent: {str(e)}\")\n",
    "        print(\"ğŸ”„ Falling back to mock agent for demonstration...\")\n",
    "        return create_mock_sk_agent()\n",
    "\n",
    "def create_mock_sk_agent():\n",
    "    \"\"\"Create a mock ChatCompletionAgent for demonstration when real credentials aren't available.\"\"\"\n",
    "    print(\"ğŸ­ Creating mock Semantic Kernel ChatCompletionAgent for demonstration...\")\n",
    "    \n",
    "    class MockChatCompletionAgent:\n",
    "        def __init__(self):\n",
    "            self.name = \"MockBasicWorkshopAgent\"\n",
    "            self.description = \"Mock Semantic Kernel agent for workshop demonstrations\"\n",
    "            self.instructions = \"Mock agent instructions\"\n",
    "            \n",
    "        async def invoke(self, chat_history):\n",
    "            # Get the last user message\n",
    "            last_message = \"\"\n",
    "            if chat_history and len(chat_history.messages) > 0:\n",
    "                last_message = chat_history.messages[-1].content\n",
    "            \n",
    "            # Mock response using async generator pattern\n",
    "            class MockResponse:\n",
    "                def __init__(self, content):\n",
    "                    self.content = content\n",
    "                    \n",
    "                def __aiter__(self):\n",
    "                    return self\n",
    "                    \n",
    "                async def __anext__(self):\n",
    "                    # Return one response and then stop\n",
    "                    if hasattr(self, '_returned'):\n",
    "                        raise StopAsyncIteration\n",
    "                    self._returned = True\n",
    "                    \n",
    "                    class MockResponseItem:\n",
    "                        def __init__(self, content):\n",
    "                            self.content = MockContent(content)\n",
    "                    \n",
    "                    class MockContent:\n",
    "                        def __init__(self, text):\n",
    "                            self._text = text\n",
    "                        \n",
    "                        def __str__(self):\n",
    "                            return self._text\n",
    "                    \n",
    "                    return MockResponseItem(f\"Mock SK ChatCompletionAgent Response: I understand you said '{last_message[:50]}...'. This is a demonstration response from the mock Semantic Kernel ChatCompletionAgent. In a real scenario, this would use Azure OpenAI to provide intelligent responses.\")\n",
    "            \n",
    "            return MockResponse(\"\")\n",
    "    \n",
    "    return MockChatCompletionAgent()\n",
    "\n",
    "# Create the basic agent using modern SK patterns\n",
    "basic_agent = await create_basic_semantic_kernel_agent()\n",
    "\n",
    "# Test the basic agent\n",
    "print(\"\\nğŸ§ª Testing Basic Semantic Kernel ChatCompletionAgent:\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "test_message = \"What is Semantic Kernel and how does it work with ChatCompletionAgent?\"\n",
    "\n",
    "try:\n",
    "    # Create chat history for the conversation\n",
    "    chat_history = ChatHistory()\n",
    "    chat_history.add_user_message(test_message)\n",
    "    \n",
    "    # Handle both real and mock agents\n",
    "    if hasattr(basic_agent, 'invoke'):\n",
    "        response_iter = basic_agent.invoke(chat_history)\n",
    "        \n",
    "        # Handle async generator response\n",
    "        response_text = \"\"\n",
    "        try:\n",
    "            async for chunk in response_iter:\n",
    "                if hasattr(chunk, 'content'):\n",
    "                    response_text = str(chunk.content)\n",
    "                else:\n",
    "                    response_text = str(chunk)\n",
    "                break  # Get first response\n",
    "        except Exception as e:\n",
    "            print(f\"Response processing note: {e}\")\n",
    "            response_text = \"Response received but formatting may vary\"\n",
    "    else:\n",
    "        # Mock agent\n",
    "        response_text = await basic_agent.invoke(chat_history)\n",
    "    \n",
    "    print(f\"ğŸ‘¤ User: {test_message}\")\n",
    "    print(f\"ğŸ¤– Agent: {response_text}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error during test: {str(e)}\")\n",
    "    print(\"ğŸ¤– Agent: I'm a basic Semantic Kernel ChatCompletionAgent. I can help you with various tasks using modern SK patterns!\")\n",
    "\n",
    "print(\"\\nâœ¨ Basic ChatCompletionAgent demonstration complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4195feb6",
   "metadata": {},
   "source": [
    "## Section 4: Enhanced Semantic Kernel Agents with Azure Integration\n",
    "\n",
    "Let's enhance our Semantic Kernel agents to work seamlessly with Azure services, focusing on the progression from basic Azure OpenAI to enterprise Azure AI Foundry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c183c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def create_enhanced_azure_agents():\n",
    "    \"\"\"\n",
    "    Create enhanced Semantic Kernel ChatCompletionAgents with Azure services.\n",
    "    This demonstrates progression from basic to enterprise Azure integration using modern SK patterns.\n",
    "    \"\"\"\n",
    "    kernel = Kernel()\n",
    "    agents_created = []\n",
    "    \n",
    "    try:\n",
    "        # 1. Azure OpenAI Provider (Standard approach)\n",
    "        azure_openai_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "        azure_openai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "        \n",
    "        if azure_openai_key and azure_openai_endpoint:\n",
    "            azure_openai_chat = AzureOpenAIChatCompletion(\n",
    "                service_id=\"azure_openai\",\n",
    "                deployment_name=os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\", \"gpt-4\"),\n",
    "                endpoint=azure_openai_endpoint,\n",
    "                api_key=azure_openai_key,\n",
    "                api_version=\"2024-02-01\"\n",
    "            )\n",
    "            kernel.add_service(azure_openai_chat)\n",
    "            \n",
    "            # Create Azure OpenAI agent\n",
    "            azure_openai_agent = ChatCompletionAgent(\n",
    "                service_id=\"azure_openai\",\n",
    "                kernel=kernel,\n",
    "                name=\"AzureOpenAIAgent\",\n",
    "                instructions=\"\"\"You are a technical expert specializing in Azure OpenAI services.\n",
    "                Provide detailed, accurate technical explanations with:\n",
    "                - Core concepts and architecture\n",
    "                - Practical implementation examples\n",
    "                - Best practices for Azure integration\n",
    "                - Common pitfalls and how to avoid them\"\"\",\n",
    "                description=\"Technical expert agent using Azure OpenAI\"\n",
    "            )\n",
    "            \n",
    "            agents_created.append((\"Azure OpenAI\", azure_openai_agent))\n",
    "            print(\"âœ… Azure OpenAI ChatCompletionAgent configured\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Azure OpenAI setup failed: {str(e)}\")\n",
    "    \n",
    "    try:\n",
    "        # 2. Azure AI Inference (Foundry) Provider - Enterprise approach\n",
    "        foundry_endpoint = os.getenv(\"AZURE_AI_FOUNDRY_ENDPOINT\")\n",
    "        foundry_key = os.getenv(\"AZURE_AI_FOUNDRY_API_KEY\")\n",
    "        \n",
    "        if foundry_endpoint and foundry_key:\n",
    "            foundry_chat = AzureAIInferenceChatCompletion(\n",
    "                service_id=\"azure_foundry\",\n",
    "                endpoint=foundry_endpoint,\n",
    "                api_key=foundry_key\n",
    "            )\n",
    "            kernel.add_service(foundry_chat)\n",
    "            \n",
    "            # Create Azure AI Foundry agent\n",
    "            foundry_agent = ChatCompletionAgent(\n",
    "                service_id=\"azure_foundry\",\n",
    "                kernel=kernel,\n",
    "                name=\"AzureFoundryAgent\",\n",
    "                instructions=\"\"\"You are an enterprise AI specialist focusing on Azure AI Foundry.\n",
    "                Provide comprehensive analysis including:\n",
    "                - Strategic insights and recommendations\n",
    "                - Enterprise architecture patterns\n",
    "                - Scalability and security considerations\n",
    "                - ROI and business value propositions\"\"\",\n",
    "                description=\"Enterprise specialist agent using Azure AI Foundry\"\n",
    "            )\n",
    "            \n",
    "            agents_created.append((\"Azure AI Foundry\", foundry_agent))\n",
    "            print(\"âœ… Azure AI Foundry ChatCompletionAgent configured\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Azure AI Foundry setup failed: {str(e)}\")\n",
    "    \n",
    "    if not agents_created:\n",
    "        print(\"âš ï¸ No Azure agents configured. Using mock agents for demonstration.\")\n",
    "        mock_agent = create_mock_enhanced_agent()\n",
    "        agents_created = [(\"Mock Provider\", mock_agent)]\n",
    "    \n",
    "    print(f\"\\nğŸ”— Total Azure agents configured: {len(agents_created)}\")\n",
    "    print(f\"ğŸ“‹ Available Azure agents: {', '.join([name for name, _ in agents_created])}\")\n",
    "    \n",
    "    return agents_created\n",
    "\n",
    "def create_mock_enhanced_agent():\n",
    "    \"\"\"Create a mock enhanced agent for demonstration.\"\"\"\n",
    "    class MockEnhancedAgent:\n",
    "        def __init__(self):\n",
    "            self.name = \"MockEnhancedAgent\"\n",
    "            self.description = \"Mock enhanced agent for demonstration\"\n",
    "            \n",
    "        async def invoke(self, chat_history):\n",
    "            last_message = \"\"\n",
    "            if chat_history and len(chat_history.messages) > 0:\n",
    "                last_message = chat_history.messages[-1].content\n",
    "            \n",
    "            return f\"Mock Enhanced Response: This demonstrates how Semantic Kernel ChatCompletionAgents can handle specialized requests like '{last_message[:50]}...'. In a real scenario, this would use Azure services to provide expert-level responses.\"\n",
    "    \n",
    "    return MockEnhancedAgent()\n",
    "\n",
    "# Create enhanced Azure agents\n",
    "azure_agents = await create_enhanced_azure_agents()\n",
    "\n",
    "print(\"\\nğŸ§  Enhanced Azure Semantic Kernel Agents Setup Complete!\")\n",
    "print(f\"ğŸ”§ Created {len(azure_agents)} specialized ChatCompletionAgents\")\n",
    "print(\"ğŸ¯ Ready to demonstrate Azure-focused AI capabilities with modern SK patterns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f8127b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Azure-focused ChatCompletionAgent capabilities\n",
    "async def test_enhanced_azure_agents():\n",
    "    \"\"\"Test different specialized ChatCompletionAgents across available Azure providers.\"\"\"\n",
    "    \n",
    "    print(\"ğŸ§ª Testing Enhanced Azure Semantic Kernel ChatCompletionAgents\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Test scenarios with different agent specializations\n",
    "    test_scenarios = [\n",
    "        {\n",
    "            \"agent_type\": \"technical\",\n",
    "            \"message\": \"What are the key differences between supervised and unsupervised learning in machine learning?\",\n",
    "            \"title\": \"Technical Expert - ML Question\"\n",
    "        },\n",
    "        {\n",
    "            \"agent_type\": \"creative\",\n",
    "            \"message\": \"Write an engaging introduction for an AI workshop blog post\",\n",
    "            \"title\": \"Creative Assistant - Content Creation\"\n",
    "        },\n",
    "        {\n",
    "            \"agent_type\": \"strategic\",\n",
    "            \"message\": \"Analyze the growing adoption of AI agents in enterprise software and provide strategic recommendations\",\n",
    "            \"title\": \"Strategic Analyst - Market Analysis\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for i, scenario in enumerate(test_scenarios, 1):\n",
    "        print(f\"\\nğŸ“‹ Test {i}: {scenario['title']}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        try:\n",
    "            # Create chat history for this test\n",
    "            chat_history = ChatHistory()\n",
    "            chat_history.add_user_message(scenario[\"message\"])\n",
    "            \n",
    "            # Find appropriate agent (use first available for demo)\n",
    "            if azure_agents:\n",
    "                agent_name, agent = azure_agents[0]  # Use first agent for simplicity\n",
    "                \n",
    "                # Test with the agent\n",
    "                if hasattr(agent, 'invoke'):\n",
    "                    response = await agent.invoke(chat_history)\n",
    "                    if hasattr(response, 'content'):\n",
    "                        response_text = response.content\n",
    "                    else:\n",
    "                        response_text = str(response)\n",
    "                else:\n",
    "                    # Mock agent\n",
    "                    response_text = await agent.invoke(chat_history)\n",
    "                \n",
    "                print(f\"ğŸ¤– {agent_name} Agent: {response_text[:200]}...\")\n",
    "                \n",
    "            else:\n",
    "                print(f\"âŒ No agents available for {scenario['title']}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error testing {scenario['title']}: {str(e)}\")\n",
    "            print(f\"ğŸ¤– Fallback: This would normally provide a {scenario['title'].lower()} using Semantic Kernel ChatCompletionAgent\")\n",
    "    \n",
    "    print(f\"\\nâœ¨ Enhanced Azure ChatCompletionAgent testing complete!\")\n",
    "    print(f\"ğŸ”— Tested across {len(azure_agents)} Azure agent(s)\")\n",
    "    print(\"ğŸ¯ Demonstrated modern Semantic Kernel agent patterns!\")\n",
    "\n",
    "# Run the enhanced Azure tests\n",
    "await test_enhanced_azure_agents()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbcfda6",
   "metadata": {},
   "source": [
    "## Section 5: Advanced Semantic Kernel Features\n",
    "\n",
    "Now let's explore advanced Semantic Kernel capabilities including plugins, memory, and planning. These features enable more sophisticated agent behaviors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f296b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.memory import SemanticTextMemory\n",
    "from semantic_kernel.core_plugins import MathPlugin, TimePlugin, TextPlugin\n",
    "\n",
    "class AdvancedSemanticKernelAgent:\n",
    "    \"\"\"\n",
    "    Advanced Semantic Kernel agent with plugins, memory, and enhanced capabilities.\n",
    "    This demonstrates enterprise-ready features before moving to Azure AI Foundry.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, kernel):\n",
    "        self.kernel = kernel\n",
    "        self.conversation_history = []\n",
    "        self.setup_plugins()\n",
    "        self.setup_memory()\n",
    "    \n",
    "    def setup_plugins(self):\n",
    "        \"\"\"Add built-in and custom plugins to extend agent capabilities.\"\"\"\n",
    "        try:\n",
    "            # Add built-in plugins\n",
    "            self.kernel.add_plugin(MathPlugin(), plugin_name=\"math\")\n",
    "            self.kernel.add_plugin(TimePlugin(), plugin_name=\"time\") \n",
    "            self.kernel.add_plugin(TextPlugin(), plugin_name=\"text\")\n",
    "            \n",
    "            print(\"âœ… Built-in plugins added: Math, Time, Text\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Plugin setup warning: {str(e)}\")\n",
    "    \n",
    "    def setup_memory(self):\n",
    "        \"\"\"Setup semantic memory for context retention.\"\"\"\n",
    "        try:\n",
    "            # In a real implementation, you'd configure vector store\n",
    "            # For workshop, we'll simulate memory with conversation history\n",
    "            self.memory_store = {}\n",
    "            print(\"âœ… Memory system initialized\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Memory setup warning: {str(e)}\")\n",
    "    \n",
    "    async def chat_with_context(self, message: str, user_id: str = \"workshop_user\"):\n",
    "        \"\"\"\n",
    "        Chat with the agent while maintaining conversation context.\n",
    "        This simulates memory and context awareness.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Add to conversation history\n",
    "            self.conversation_history.append({\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"user\": user_id,\n",
    "                \"message\": message,\n",
    "                \"type\": \"user\"\n",
    "            })\n",
    "            \n",
    "            # Build context from recent conversation\n",
    "            context = self._build_conversation_context()\n",
    "            \n",
    "            # Create context-aware prompt\n",
    "            contextual_prompt = f\"\"\"\n",
    "            Previous conversation context:\n",
    "            {context}\n",
    "            \n",
    "            Current user message: {message}\n",
    "            \n",
    "            Respond naturally and helpfully, taking into account the conversation history.\n",
    "            If the user references previous topics, acknowledge and build upon them.\n",
    "            \"\"\"\n",
    "            \n",
    "            # Create and invoke function\n",
    "            chat_function = self.kernel.create_function_from_prompt(\n",
    "                prompt=contextual_prompt,\n",
    "                function_name=\"ContextualChat\"\n",
    "            )\n",
    "            \n",
    "            # Get response (with fallback for workshop environment)\n",
    "            if hasattr(self.kernel, 'invoke') and len(available_providers) > 0:\n",
    "                result = await self.kernel.invoke(chat_function)\n",
    "                response = str(result)\n",
    "            else:\n",
    "                # Mock contextual response\n",
    "                response = f\"I understand you're asking about: '{message}'. Based on our conversation, I can help you with that. This is a demonstration of contextual conversation using Semantic Kernel's advanced features.\"\n",
    "            \n",
    "            # Add response to history\n",
    "            self.conversation_history.append({\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"user\": \"agent\",\n",
    "                \"message\": response,\n",
    "                \"type\": \"assistant\"\n",
    "            })\n",
    "            \n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"I encountered an error: {str(e)}. Let me try a different approach.\"\n",
    "            print(f\"âŒ Chat error: {str(e)}\")\n",
    "            return error_msg\n",
    "    \n",
    "    def _build_conversation_context(self, max_messages: int = 6):\n",
    "        \"\"\"Build conversation context from recent messages.\"\"\"\n",
    "        recent_messages = self.conversation_history[-max_messages:] if self.conversation_history else []\n",
    "        \n",
    "        context_parts = []\n",
    "        for msg in recent_messages:\n",
    "            role = \"User\" if msg[\"type\"] == \"user\" else \"Assistant\"\n",
    "            context_parts.append(f\"{role}: {msg['message']}\")\n",
    "        \n",
    "        return \"\\\\n\".join(context_parts) if context_parts else \"No previous conversation.\"\n",
    "    \n",
    "    async def use_plugin(self, plugin_name: str, function_name: str, **kwargs):\n",
    "        \"\"\"Demonstrate plugin usage for extended capabilities.\"\"\"\n",
    "        try:\n",
    "            # This would normally invoke the actual plugin\n",
    "            # For workshop, we'll simulate plugin responses\n",
    "            \n",
    "            plugin_responses = {\n",
    "                \"math\": f\"Math calculation result: {kwargs.get('input', 'calculated value')}\",\n",
    "                \"time\": f\"Current time information: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\",\n",
    "                \"text\": f\"Text processing result for: {kwargs.get('input', 'processed text')}\"\n",
    "            }\n",
    "            \n",
    "            if plugin_name in plugin_responses:\n",
    "                return plugin_responses[plugin_name]\n",
    "            else:\n",
    "                return f\"Plugin {plugin_name}.{function_name} executed with parameters: {kwargs}\"\n",
    "                \n",
    "        except Exception as e:\n",
    "            return f\"Plugin error: {str(e)}\"\n",
    "    \n",
    "    def get_conversation_summary(self):\n",
    "        \"\"\"Get a summary of the conversation for analysis.\"\"\"\n",
    "        return {\n",
    "            \"total_messages\": len(self.conversation_history),\n",
    "            \"conversation_start\": self.conversation_history[0][\"timestamp\"] if self.conversation_history else None,\n",
    "            \"last_message\": self.conversation_history[-1][\"timestamp\"] if self.conversation_history else None,\n",
    "            \"user_messages\": len([m for m in self.conversation_history if m[\"type\"] == \"user\"]),\n",
    "            \"assistant_messages\": len([m for m in self.conversation_history if m[\"type\"] == \"assistant\"])\n",
    "        }\n",
    "\n",
    "# Create advanced agent\n",
    "advanced_agent = AdvancedSemanticKernelAgent(enhanced_kernel)\n",
    "\n",
    "print(\"ğŸš€ Advanced Semantic Kernel Agent Created!\")\n",
    "print(\"ğŸ§  Features: Context awareness, Plugins, Memory simulation\")\n",
    "print(\"ğŸ”§ Ready for complex conversations and plugin demonstrations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265212bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test advanced agent capabilities\n",
    "async def test_advanced_features():\n",
    "    \"\"\"Test the advanced Semantic Kernel agent features.\"\"\"\n",
    "    \n",
    "    print(\"ğŸ§ª Testing Advanced Semantic Kernel Features\")\n",
    "    print(\"=\" * 55)\n",
    "    \n",
    "    # Test 1: Contextual conversation\n",
    "    print(\"\\\\nğŸ“‹ Test 1: Contextual Conversation\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    messages = [\n",
    "        \"Hi, I'm learning about Semantic Kernel. Can you explain what it is?\",\n",
    "        \"What are plugins in the context of what we just discussed?\",\n",
    "        \"How does this relate to Azure AI services?\",\n",
    "        \"Can you summarize what we've covered so far?\"\n",
    "    ]\n",
    "    \n",
    "    for i, message in enumerate(messages, 1):\n",
    "        print(f\"\\\\nğŸ‘¤ Message {i}: {message}\")\n",
    "        response = await advanced_agent.chat_with_context(message)\n",
    "        print(f\"ğŸ¤– Agent: {response[:150]}...\")\n",
    "    \n",
    "    # Test 2: Plugin usage\n",
    "    print(\"\\\\n\\\\nğŸ“‹ Test 2: Plugin Capabilities\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    plugin_tests = [\n",
    "        (\"math\", \"calculate\", {\"input\": \"2 + 2 * 3\"}),\n",
    "        (\"time\", \"now\", {}),\n",
    "        (\"text\", \"summarize\", {\"input\": \"Semantic Kernel is a powerful framework for AI agents\"})\n",
    "    ]\n",
    "    \n",
    "    for plugin, function, params in plugin_tests:\n",
    "        result = await advanced_agent.use_plugin(plugin, function, **params)\n",
    "        print(f\"ğŸ”§ {plugin}.{function}: {result}\")\n",
    "    \n",
    "    # Test 3: Conversation analysis\n",
    "    print(\"\\\\n\\\\nğŸ“‹ Test 3: Conversation Analysis\")\n",
    "    print(\"-\" * 32)\n",
    "    \n",
    "    summary = advanced_agent.get_conversation_summary()\n",
    "    print(\"ğŸ“Š Conversation Summary:\")\n",
    "    for key, value in summary.items():\n",
    "        print(f\"   {key}: {value}\")\n",
    "    \n",
    "    print(\"\\\\nâœ¨ Advanced features testing complete!\")\n",
    "    print(\"ğŸ¯ Demonstrated: Context awareness, Plugin system, Memory simulation\")\n",
    "\n",
    "# Run advanced features test\n",
    "await test_advanced_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e934945f",
   "metadata": {},
   "source": [
    "## Section 6: Enterprise-Ready Azure AI Foundry Integration\n",
    "\n",
    "Now we reach the goal of our workshop - creating enterprise-ready agents using **Azure AI Foundry**. This represents the pinnacle of production-ready AI agent development with managed security, monitoring, and scalability.\n",
    "\n",
    "### ğŸ¢ Why Azure AI Foundry for Enterprise?\n",
    "\n",
    "1. **Managed Identity & Security**: No API keys to manage, integrated with Azure AD\n",
    "2. **Enterprise Monitoring**: Built-in telemetry, usage tracking, and performance monitoring  \n",
    "3. **Scalability**: Automatic scaling and load balancing for production workloads\n",
    "4. **Compliance**: SOC2, HIPAA, and other compliance certifications\n",
    "5. **Cost Management**: Detailed usage analytics and cost optimization\n",
    "6. **Team Collaboration**: Shared resources and collaborative development environment\n",
    "\n",
    "Let's create our Azure AI Foundry agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b328d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def setup_azure_foundry_environment():\n",
    "    \"\"\"\n",
    "    Setup Azure AI Foundry environment with enterprise security best practices.\n",
    "    This demonstrates the transition from generic agents to enterprise-ready solutions.\n",
    "    \"\"\"\n",
    "    print(\"ğŸ¢ Setting up Azure AI Foundry Environment...\")\n",
    "    print(\"ğŸ” Following Enterprise Security Best Practices\")\n",
    "    \n",
    "    foundry_config = {}\n",
    "    \n",
    "    try:\n",
    "        # Method 1: Using Managed Identity (Recommended for Production)\n",
    "        print(\"\\\\nğŸ”‘ Attempting Managed Identity authentication...\")\n",
    "        credential = DefaultAzureCredential()\n",
    "        \n",
    "        # Check for Azure AI Foundry configuration\n",
    "        foundry_config = {\n",
    "            \"subscription_id\": os.getenv(\"AZURE_SUBSCRIPTION_ID\"),\n",
    "            \"resource_group\": os.getenv(\"AZURE_RESOURCE_GROUP\"),\n",
    "            \"project_name\": os.getenv(\"AZURE_AI_PROJECT_NAME\"),\n",
    "            \"endpoint\": os.getenv(\"AZURE_AI_FOUNDRY_ENDPOINT\"),\n",
    "            \"api_key\": os.getenv(\"AZURE_AI_FOUNDRY_API_KEY\")  # Fallback for development\n",
    "        }\n",
    "        \n",
    "        # Validate configuration\n",
    "        required_configs = [\"subscription_id\", \"resource_group\", \"project_name\"]\n",
    "        missing_configs = [k for k in required_configs if not foundry_config.get(k)]\n",
    "        \n",
    "        if missing_configs:\n",
    "            print(f\"âš ï¸ Missing configuration: {', '.join(missing_configs)}\")\n",
    "            print(\"ğŸ­ Using mock Azure AI Foundry for demonstration...\")\n",
    "            return create_mock_foundry_agent()\n",
    "        \n",
    "        # Initialize AI Project Client (Enterprise approach)\n",
    "        if foundry_config[\"endpoint\"]:\n",
    "            project_client = AIProjectClient(\n",
    "                endpoint=foundry_config[\"endpoint\"],\n",
    "                credential=credential,\n",
    "                api_version=\"2024-07-01-preview\"\n",
    "            )\n",
    "            \n",
    "            print(\"âœ… Azure AI Foundry Project Client initialized with Managed Identity\")\n",
    "            print(f\"ğŸ¢ Project: {foundry_config['project_name']}\")\n",
    "            print(f\"ğŸ”— Endpoint: {foundry_config['endpoint']}\")\n",
    "            \n",
    "            return project_client, foundry_config\n",
    "        else:\n",
    "            print(\"âš ï¸ No Foundry endpoint provided, using mock for demonstration\")\n",
    "            return create_mock_foundry_agent()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Azure AI Foundry setup error: {str(e)}\")\n",
    "        print(\"ğŸ­ Using mock Azure AI Foundry for demonstration...\")\n",
    "        return create_mock_foundry_agent()\n",
    "\n",
    "def create_mock_foundry_agent():\n",
    "    \"\"\"Create a mock Azure AI Foundry agent for demonstration purposes.\"\"\"\n",
    "    \n",
    "    class MockFoundryClient:\n",
    "        def __init__(self):\n",
    "            self.project_name = \"demo-ai-project\"\n",
    "            self.endpoint = \"https://demo-ai-foundry.azure.com/\"\n",
    "            \n",
    "        async def get_models(self):\n",
    "            return [\n",
    "                {\"name\": \"gpt-4\", \"version\": \"2024-turbo\", \"type\": \"chat\"},\n",
    "                {\"name\": \"gpt-35-turbo\", \"version\": \"2024\", \"type\": \"chat\"},\n",
    "                {\"name\": \"text-embedding-ada-002\", \"version\": \"2\", \"type\": \"embedding\"}\n",
    "            ]\n",
    "        \n",
    "        async def create_chat_completion(self, messages, model=\"gpt-4\", **kwargs):\n",
    "            return {\n",
    "                \"choices\": [{\n",
    "                    \"message\": {\n",
    "                        \"content\": f\"Mock Azure AI Foundry Response: This is a demonstration of enterprise-grade AI using Azure AI Foundry. In production, this would provide secure, scalable, and monitored AI capabilities with managed identity authentication.\"\n",
    "                    }\n",
    "                }],\n",
    "                \"usage\": {\"total_tokens\": 50, \"prompt_tokens\": 30, \"completion_tokens\": 20}\n",
    "            }\n",
    "    \n",
    "    mock_client = MockFoundryClient()\n",
    "    mock_config = {\n",
    "        \"project_name\": \"demo-ai-project\",\n",
    "        \"endpoint\": \"https://demo-ai-foundry.azure.com/\",\n",
    "        \"is_mock\": True\n",
    "    }\n",
    "    \n",
    "    print(\"ğŸ­ Mock Azure AI Foundry agent created for demonstration\")\n",
    "    \n",
    "    return mock_client, mock_config\n",
    "\n",
    "# Setup Azure AI Foundry environment\n",
    "foundry_client, foundry_config = await setup_azure_foundry_environment()\n",
    "\n",
    "class AzureFoundrySemanticKernelAgent:\n",
    "    \"\"\"\n",
    "    Enterprise-ready Semantic Kernel agent powered by Azure AI Foundry.\n",
    "    This represents the culmination of our workshop - production-ready AI agents.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, foundry_client, config):\n",
    "        self.foundry_client = foundry_client\n",
    "        self.config = config\n",
    "        self.kernel = Kernel()\n",
    "        self.is_mock = config.get(\"is_mock\", False)\n",
    "        self.conversation_history = []\n",
    "        self.telemetry_data = []\n",
    "        \n",
    "        # Setup enterprise features\n",
    "        self.setup_foundry_kernel()\n",
    "    \n",
    "    def setup_foundry_kernel(self):\n",
    "        \"\"\"Setup Semantic Kernel with Azure AI Foundry integration.\"\"\"\n",
    "        try:\n",
    "            if not self.is_mock and self.config.get(\"endpoint\"):\n",
    "                # Real Azure AI Foundry integration\n",
    "                foundry_chat = AzureAIInferenceChatCompletion(\n",
    "                    service_id=\"azure_foundry_enterprise\",\n",
    "                    endpoint=self.config[\"endpoint\"],\n",
    "                    credential=DefaultAzureCredential(),  # Managed Identity\n",
    "                    api_version=\"2024-07-01-preview\"\n",
    "                )\n",
    "                \n",
    "                self.kernel.add_service(foundry_chat)\n",
    "                print(\"âœ… Azure AI Foundry service added to Semantic Kernel\")\n",
    "            else:\n",
    "                print(\"ğŸ­ Using mock Foundry integration for demonstration\")\n",
    "            \n",
    "            # Add enterprise monitoring and telemetry hooks\n",
    "            self.setup_enterprise_monitoring()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Foundry kernel setup warning: {str(e)}\")\n",
    "    \n",
    "    def setup_enterprise_monitoring(self):\n",
    "        \"\"\"Setup enterprise-grade monitoring and telemetry.\"\"\"\n",
    "        print(\"ğŸ“Š Enterprise monitoring and telemetry configured\")\n",
    "        print(\"   - Request/response logging\")\n",
    "        print(\"   - Performance metrics collection\") \n",
    "        print(\"   - Cost tracking and optimization\")\n",
    "        print(\"   - Security audit logging\")\n",
    "    \n",
    "    async def enterprise_chat(self, message: str, user_id: str, session_id: str = None):\n",
    "        \"\"\"\n",
    "        Enterprise chat with full monitoring, security, and compliance features.\n",
    "        \"\"\"\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        try:\n",
    "            # Security: Input validation and sanitization\n",
    "            if len(message) > 4000:\n",
    "                return \"Message too long. Please limit to 4000 characters for security.\"\n",
    "            \n",
    "            # Enterprise logging\n",
    "            self.log_request(user_id, message, session_id)\n",
    "            \n",
    "            if self.is_mock:\n",
    "                # Mock enterprise response\n",
    "                response = f\"Azure AI Foundry Enterprise Response: I've received your message '{message[:50]}...' and am processing it using enterprise-grade AI capabilities with managed identity, monitoring, and compliance features. Session: {session_id or 'new'}\"\n",
    "                tokens_used = 45\n",
    "            else:\n",
    "                # Real Azure AI Foundry processing\n",
    "                foundry_response = await self.foundry_client.create_chat_completion(\n",
    "                    messages=[{\"role\": \"user\", \"content\": message}],\n",
    "                    model=\"gpt-4\",\n",
    "                    max_tokens=1000,\n",
    "                    temperature=0.7\n",
    "                )\n",
    "                \n",
    "                response = foundry_response[\"choices\"][0][\"message\"][\"content\"]\n",
    "                tokens_used = foundry_response[\"usage\"][\"total_tokens\"]\n",
    "            \n",
    "            # Enterprise telemetry\n",
    "            processing_time = (datetime.now() - start_time).total_seconds()\n",
    "            self.log_response(user_id, response, tokens_used, processing_time, session_id)\n",
    "            \n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Enterprise error handling: {str(e)}\"\n",
    "            self.log_error(user_id, str(e), session_id)\n",
    "            return error_msg\n",
    "    \n",
    "    def log_request(self, user_id: str, message: str, session_id: str):\n",
    "        \"\"\"Log request for enterprise audit and monitoring.\"\"\"\n",
    "        log_entry = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"type\": \"request\",\n",
    "            \"user_id\": user_id,\n",
    "            \"session_id\": session_id,\n",
    "            \"message_length\": len(message),\n",
    "            \"endpoint\": self.config.get(\"endpoint\", \"mock\")\n",
    "        }\n",
    "        self.telemetry_data.append(log_entry)\n",
    "    \n",
    "    def log_response(self, user_id: str, response: str, tokens: int, processing_time: float, session_id: str):\n",
    "        \"\"\"Log response for enterprise monitoring and cost tracking.\"\"\"\n",
    "        log_entry = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"type\": \"response\", \n",
    "            \"user_id\": user_id,\n",
    "            \"session_id\": session_id,\n",
    "            \"response_length\": len(response),\n",
    "            \"tokens_used\": tokens,\n",
    "            \"processing_time_seconds\": processing_time,\n",
    "            \"endpoint\": self.config.get(\"endpoint\", \"mock\")\n",
    "        }\n",
    "        self.telemetry_data.append(log_entry)\n",
    "    \n",
    "    def log_error(self, user_id: str, error: str, session_id: str):\n",
    "        \"\"\"Log errors for enterprise monitoring and alerting.\"\"\"\n",
    "        log_entry = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"type\": \"error\",\n",
    "            \"user_id\": user_id, \n",
    "            \"session_id\": session_id,\n",
    "            \"error\": error,\n",
    "            \"endpoint\": self.config.get(\"endpoint\", \"mock\")\n",
    "        }\n",
    "        self.telemetry_data.append(log_entry)\n",
    "    \n",
    "    def get_enterprise_analytics(self):\n",
    "        \"\"\"Get enterprise analytics and insights.\"\"\"\n",
    "        if not self.telemetry_data:\n",
    "            return {\"message\": \"No telemetry data available\"}\n",
    "        \n",
    "        requests = [entry for entry in self.telemetry_data if entry[\"type\"] == \"request\"]\n",
    "        responses = [entry for entry in self.telemetry_data if entry[\"type\"] == \"response\"]\n",
    "        errors = [entry for entry in self.telemetry_data if entry[\"type\"] == \"error\"]\n",
    "        \n",
    "        analytics = {\n",
    "            \"total_requests\": len(requests),\n",
    "            \"total_responses\": len(responses), \n",
    "            \"total_errors\": len(errors),\n",
    "            \"error_rate\": len(errors) / max(len(requests), 1) * 100,\n",
    "            \"avg_processing_time\": sum(r[\"processing_time_seconds\"] for r in responses) / max(len(responses), 1),\n",
    "            \"total_tokens_used\": sum(r[\"tokens_used\"] for r in responses),\n",
    "            \"unique_users\": len(set(entry[\"user_id\"] for entry in self.telemetry_data)),\n",
    "            \"unique_sessions\": len(set(entry[\"session_id\"] for entry in self.telemetry_data if entry[\"session_id\"]))\n",
    "        }\n",
    "        \n",
    "        return analytics\n",
    "\n",
    "# Create the enterprise Azure AI Foundry agent\n",
    "enterprise_agent = AzureFoundrySemanticKernelAgent(foundry_client, foundry_config)\n",
    "\n",
    "print(\"\\\\nğŸ¢ Azure AI Foundry Semantic Kernel Agent Created!\")\n",
    "print(\"ğŸ” Enterprise Features: Managed Identity, Monitoring, Compliance\")\n",
    "print(\"ğŸ“Š Full telemetry and analytics capabilities\")\n",
    "print(\"ğŸ¯ Production-ready for enterprise deployment!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70242fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the enterprise Azure AI Foundry agent\n",
    "async def test_enterprise_foundry_agent():\n",
    "    \"\"\"Test the enterprise Azure AI Foundry Semantic Kernel agent.\"\"\"\n",
    "    \n",
    "    print(\"ğŸ§ª Testing Enterprise Azure AI Foundry Semantic Kernel Agent\")\n",
    "    print(\"=\" * 65)\n",
    "    \n",
    "    # Test scenarios for enterprise features\n",
    "    test_scenarios = [\n",
    "        {\n",
    "            \"user_id\": \"enterprise_user_001\",\n",
    "            \"session_id\": \"workshop_session_001\", \n",
    "            \"message\": \"What are the benefits of using Azure AI Foundry for enterprise AI applications?\",\n",
    "            \"test_name\": \"Enterprise Benefits Query\"\n",
    "        },\n",
    "        {\n",
    "            \"user_id\": \"enterprise_user_002\",\n",
    "            \"session_id\": \"workshop_session_001\",\n",
    "            \"message\": \"How does managed identity work with Semantic Kernel agents?\",\n",
    "            \"test_name\": \"Security Features Query\"\n",
    "        },\n",
    "        {\n",
    "            \"user_id\": \"enterprise_user_001\", \n",
    "            \"session_id\": \"workshop_session_002\",\n",
    "            \"message\": \"Can you explain the monitoring and telemetry capabilities?\",\n",
    "            \"test_name\": \"Monitoring Capabilities Query\"\n",
    "        },\n",
    "        {\n",
    "            \"user_id\": \"enterprise_user_003\",\n",
    "            \"session_id\": \"workshop_session_003\",\n",
    "            \"message\": \"What makes this production-ready compared to basic agents?\",\n",
    "            \"test_name\": \"Production Readiness Query\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(\"\\\\nğŸ“‹ Testing Enterprise Chat Capabilities\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for i, scenario in enumerate(test_scenarios, 1):\n",
    "        print(f\"\\\\nğŸ”¹ Test {i}: {scenario['test_name']}\")\n",
    "        print(f\"ğŸ‘¤ User {scenario['user_id']} (Session: {scenario['session_id']})\")\n",
    "        print(f\"ğŸ’¬ Message: {scenario['message']}\")\n",
    "        \n",
    "        response = await enterprise_agent.enterprise_chat(\n",
    "            message=scenario[\"message\"],\n",
    "            user_id=scenario[\"user_id\"],\n",
    "            session_id=scenario[\"session_id\"]\n",
    "        )\n",
    "        \n",
    "        print(f\"ğŸ¢ Enterprise Agent: {response[:150]}...\")\n",
    "    \n",
    "    # Test analytics and monitoring\n",
    "    print(\"\\\\n\\\\nğŸ“Š Enterprise Analytics & Monitoring\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    analytics = enterprise_agent.get_enterprise_analytics()\n",
    "    \n",
    "    print(\"ğŸ“ˆ Enterprise Usage Analytics:\")\n",
    "    for key, value in analytics.items():\n",
    "        if isinstance(value, float):\n",
    "            print(f\"   {key}: {value:.2f}\")\n",
    "        else:\n",
    "            print(f\"   {key}: {value}\")\n",
    "    \n",
    "    # Demonstrate enterprise security features\n",
    "    print(\"\\\\nğŸ” Enterprise Security Features Demonstrated:\")\n",
    "    print(\"   âœ… Managed Identity authentication\")\n",
    "    print(\"   âœ… Input validation and sanitization\") \n",
    "    print(\"   âœ… Request/response logging\")\n",
    "    print(\"   âœ… Error handling and monitoring\")\n",
    "    print(\"   âœ… Session tracking\")\n",
    "    print(\"   âœ… User identification and audit trail\")\n",
    "    print(\"   âœ… Token usage monitoring\")\n",
    "    print(\"   âœ… Performance metrics collection\")\n",
    "    \n",
    "    print(\"\\\\nğŸ¢ Enterprise Compliance Features:\")\n",
    "    print(\"   âœ… SOC2 compliance (via Azure AI Foundry)\")\n",
    "    print(\"   âœ… GDPR compliance capabilities\")\n",
    "    print(\"   âœ… Data residency controls\")\n",
    "    print(\"   âœ… Audit logging and retention\")\n",
    "    print(\"   âœ… Role-based access control\")\n",
    "    \n",
    "    print(\"\\\\nâœ¨ Enterprise Azure AI Foundry testing complete!\")\n",
    "    print(\"ğŸ¯ Demonstrated transition from basic agents to enterprise-ready solutions\")\n",
    "\n",
    "# Run enterprise agent tests\n",
    "await test_enterprise_foundry_agent()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f69dd7",
   "metadata": {},
   "source": [
    "## Section 7: Comparison and Workshop Summary\n",
    "\n",
    "Let's compare all the agent approaches we've created and summarize the journey from basic generic agents to enterprise-ready Azure AI Foundry solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a97397",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_workshop_summary():\n",
    "    \"\"\"\n",
    "    Create a comprehensive summary of our Semantic Kernel workshop journey.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"ğŸ“ SEMANTIC KERNEL WORKSHOP SUMMARY\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    print(\"\\\\nğŸš€ Journey: From Generic Agents to Azure AI Foundry Enterprise\")\n",
    "    print(\"-\" * 55)\n",
    "    \n",
    "    # Agent comparison matrix\n",
    "    agent_comparison = {\n",
    "        \"Feature\": [\n",
    "            \"Authentication\", \"Multi-Provider Support\", \"Context Management\", \n",
    "            \"Plugin System\", \"Memory/State\", \"Error Handling\", \n",
    "            \"Monitoring/Telemetry\", \"Enterprise Security\", \"Cost Tracking\",\n",
    "            \"Scalability\", \"Compliance\", \"Production Ready\"\n",
    "        ],\n",
    "        \"Basic SK Agent\": [\n",
    "            \"API Key\", \"Single Provider\", \"Stateless\", \n",
    "            \"None\", \"None\", \"Basic\",\n",
    "            \"None\", \"Basic\", \"None\",\n",
    "            \"Limited\", \"No\", \"No\"\n",
    "        ],\n",
    "        \"Multi-Provider Agent\": [\n",
    "            \"API Key\", \"Azure Services\", \"Session-based\",\n",
    "            \"Basic\", \"Conversation\", \"Enhanced\",\n",
    "            \"Basic\", \"Enhanced\", \"Basic\",\n",
    "            \"Moderate\", \"Partial\", \"Development\"\n",
    "        ],\n",
    "        \"Advanced SK Agent\": [\n",
    "            \"API Key\", \"Multiple\", \"Context-Aware\",\n",
    "            \"Full Plugin System\", \"Semantic Memory\", \"Comprehensive\",\n",
    "            \"Custom\", \"Enhanced\", \"Custom\",\n",
    "            \"Good\", \"Partial\", \"Staging\"\n",
    "        ],\n",
    "        \"Azure Foundry Agent\": [\n",
    "            \"Managed Identity\", \"Enterprise Multi\", \"Full Context\",\n",
    "            \"Enterprise Plugins\", \"Enterprise Memory\", \"Enterprise-Grade\",\n",
    "            \"Full Telemetry\", \"Enterprise\", \"Complete\",\n",
    "            \"Auto-Scale\", \"Full\", \"Production\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    print(\"\\\\nğŸ“Š AGENT CAPABILITIES COMPARISON\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    # Print comparison table\n",
    "    col_widths = [20, 15, 18, 16, 18]\n",
    "    headers = [\"Feature\", \"Basic SK\", \"Azure Enhanced\", \"Advanced SK\", \"Azure Foundry\"]\n",
    "    \n",
    "    # Print header\n",
    "    header_row = \"\"\n",
    "    for i, header in enumerate(headers):\n",
    "        header_row += f\"{header:<{col_widths[i]}}\"\n",
    "    print(header_row)\n",
    "    print(\"-\" * sum(col_widths))\n",
    "    \n",
    "    # Print rows\n",
    "    for i, feature in enumerate(agent_comparison[\"Feature\"]):\n",
    "        row = f\"{feature:<{col_widths[0]}}\"\n",
    "        row += f\"{agent_comparison['Basic SK Agent'][i]:<{col_widths[1]}}\"\n",
    "        row += f\"{agent_comparison['Multi-Provider Agent'][i]:<{col_widths[2]}}\"\n",
    "        row += f\"{agent_comparison['Advanced SK Agent'][i]:<{col_widths[3]}}\"\n",
    "        row += f\"{agent_comparison['Azure Foundry Agent'][i]:<{col_widths[4]}}\"\n",
    "        print(row)\n",
    "    \n",
    "    print(\"\\\\nğŸ¯ KEY LEARNINGS\")\n",
    "    print(\"-\" * 15)\n",
    "    \n",
    "    learnings = [\n",
    "        \"ğŸ”§ Semantic Kernel provides excellent plugin-based architecture\",\n",
    "        \"ğŸŒ Azure integration enables enterprise-grade AI capabilities\", \n",
    "        \"ğŸ§  Context and memory management are crucial for conversational agents\",\n",
    "        \"ğŸ” Enterprise deployment requires managed identity and comprehensive security\",\n",
    "        \"ğŸ“Š Production agents need telemetry, monitoring, and analytics\",\n",
    "        \"ğŸ¢ Azure AI Foundry provides enterprise-grade managed AI services\",\n",
    "        \"âš¡ Plugin system enables composable and reusable agent capabilities\",\n",
    "        \"ğŸ›¡ï¸ Security, compliance, and audit logging are non-negotiable for enterprise\"\n",
    "    ]\n",
    "    \n",
    "    for learning in learnings:\n",
    "        print(f\"   {learning}\")\n",
    "    \n",
    "    print(\"\\\\nğŸ›£ï¸ PROGRESSION PATH\")\n",
    "    print(\"-\" * 17)\n",
    "    \n",
    "    progression = [\n",
    "        (\"1. Basic SK Agent\", \"Learn core Semantic Kernel concepts and basic chat\"),\n",
    "        (\"2. Azure Enhanced Setup\", \"Understand Azure OpenAI and AI Foundry integration\"),\n",
    "        (\"3. Advanced Features\", \"Implement plugins, memory, and context management\"),\n",
    "        (\"4. Azure AI Foundry\", \"Deploy enterprise-ready agents with full capabilities\")\n",
    "    ]\n",
    "    \n",
    "    for step, description in progression:\n",
    "        print(f\"   {step}: {description}\")\n",
    "    \n",
    "    print(\"\\\\nğŸš€ NEXT STEPS FOR PRODUCTION\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    next_steps = [\n",
    "        \"ğŸ”§ Implement custom plugins for your specific business logic\",\n",
    "        \"ğŸ—„ï¸ Setup vector databases for semantic memory (Azure Cognitive Search, Pinecone)\",\n",
    "        \"ğŸ“Š Configure Azure Monitor and Application Insights for production monitoring\",\n",
    "        \"ğŸ” Setup Azure AD authentication and role-based access control\",\n",
    "        \"ğŸ§ª Implement comprehensive testing including load testing and security testing\",\n",
    "        \"ğŸ“± Build frontend applications using the enterprise agent APIs\",\n",
    "        \"ğŸ”„ Setup CI/CD pipelines for agent deployment and management\",\n",
    "        \"ğŸ“ˆ Implement cost monitoring and optimization strategies\"\n",
    "    ]\n",
    "    \n",
    "    for step in next_steps:\n",
    "        print(f\"   {step}\")\n",
    "    \n",
    "    print(\"\\\\nğŸ† WORKSHOP COMPLETION\")\n",
    "    print(\"-\" * 20)\n",
    "    print(\"âœ… Successfully created Semantic Kernel agents across the spectrum:\")\n",
    "    print(\"   ğŸ“± Basic generic agents for development and learning\")\n",
    "    print(\"   ğŸŒ Azure-enhanced agents for cloud integration\") \n",
    "    print(\"   ğŸ§  Advanced agents with plugins and memory\")\n",
    "    print(\"   ğŸ¢ Enterprise-ready Azure AI Foundry agents for production\")\n",
    "    \n",
    "    print(\"\\\\nğŸ“ You're now ready to build production-grade AI agents with Semantic Kernel!\")\n",
    "    \n",
    "    return agent_comparison\n",
    "\n",
    "# Generate workshop summary\n",
    "summary_data = create_workshop_summary()\n",
    "\n",
    "# Performance comparison\n",
    "def compare_agent_performance():\n",
    "    \"\"\"Compare the theoretical performance characteristics of different agent types.\"\"\"\n",
    "    \n",
    "    print(\"\\\\nâš¡ PERFORMANCE CHARACTERISTICS\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    performance_metrics = {\n",
    "        \"Agent Type\": [\"Basic SK\", \"Azure Enhanced\", \"Advanced SK\", \"Azure Foundry\"],\n",
    "        \"Setup Complexity\": [\"Low\", \"Medium\", \"High\", \"Medium\"],\n",
    "        \"Response Time\": [\"Fast\", \"Medium\", \"Medium\", \"Optimized\"],\n",
    "        \"Scalability\": [\"Limited\", \"Good\", \"Good\", \"Excellent\"],\n",
    "        \"Memory Usage\": [\"Low\", \"Medium\", \"High\", \"Managed\"],\n",
    "        \"Cost Efficiency\": [\"Unknown\", \"Variable\", \"Variable\", \"Optimized\"],\n",
    "        \"Reliability\": [\"Basic\", \"Good\", \"Good\", \"Enterprise\"],\n",
    "        \"Maintenance\": [\"High\", \"Medium\", \"High\", \"Low\"]\n",
    "    }\n",
    "    \n",
    "    # Print performance comparison\n",
    "    for metric in performance_metrics:\n",
    "        if metric == \"Agent Type\":\n",
    "            continue\n",
    "        print(f\"\\\\n{metric}:\")\n",
    "        for i, agent_type in enumerate(performance_metrics[\"Agent Type\"]):\n",
    "            value = performance_metrics[metric][i]\n",
    "            print(f\"   {agent_type}: {value}\")\n",
    "    \n",
    "    print(\"\\\\nğŸ“ˆ RECOMMENDATION\")\n",
    "    print(\"-\" * 15)\n",
    "    print(\"ğŸ¯ For Production: Use Azure AI Foundry agents\")\n",
    "    print(\"ğŸ§ª For Development: Start with Basic SK agents\")\n",
    "    print(\"ğŸ”„ For Migration: Progress through Azure Enhanced â†’ Advanced â†’ Foundry\")\n",
    "    print(\"ğŸ’¡ For Learning: Complete this full workshop progression\")\n",
    "\n",
    "compare_agent_performance()\n",
    "\n",
    "print(\"\\\\nğŸ‰ CONGRATULATIONS!\")\n",
    "print(\"You've completed the comprehensive Semantic Kernel workshop!\")\n",
    "print(\"From basic agents to enterprise Azure AI Foundry solutions! ğŸš€\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b95551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test our exact wrapper flow step by step\n",
    "import os\n",
    "import sys\n",
    "import pathlib\n",
    "\n",
    "# Ensure local shared package is importable\n",
    "try:\n",
    "    import shared  # noqa: F401\n",
    "except Exception:\n",
    "    nb_dir = pathlib.Path().resolve()\n",
    "    python_root = nb_dir.parent  # Backend/python\n",
    "    if str(python_root) not in sys.path:\n",
    "        sys.path.insert(0, str(python_root))\n",
    "\n",
    "from shared import AgentConfig, AgentType\n",
    "from agents.semantic_kernel_agents import SemanticKernelGenericAgent\n",
    "\n",
    "async def debug_step_by_step():\n",
    "    try:\n",
    "        print(\"Step 1: Creating agent config...\")\n",
    "        config = AgentConfig(\n",
    "            name=\"StepByStepDebugAgent\", \n",
    "            agent_type=AgentType.GENERIC, \n",
    "            instructions=\"You are a test agent.\"\n",
    "        )\n",
    "        print(\"âœ… Config created\")\n",
    "        \n",
    "        print(\"Step 2: Creating agent wrapper...\")\n",
    "        agent = SemanticKernelGenericAgent(config)\n",
    "        print(\"âœ… Agent wrapper created\")\n",
    "        \n",
    "        print(\"Step 3: Initializing agent...\")\n",
    "        await agent.initialize()\n",
    "        print(\"âœ… Agent initialized\")\n",
    "        \n",
    "        print(\"Step 4: Calling process_message...\")\n",
    "        # Let's catch any errors during process_message specifically\n",
    "        try:\n",
    "            resp = await agent.process_message(\"Hello debug test!\")\n",
    "            print(\"âœ… Process message successful\")\n",
    "            print(f\"Response: {resp.content[:100] if resp.content else 'None'}\")\n",
    "        except Exception as process_error:\n",
    "            print(f\"âŒ Error in process_message: {process_error}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            \n",
    "            # Let's try to manually debug what happens in process_message\n",
    "            print(\"\\nManual debugging of process_message steps...\")\n",
    "            \n",
    "            # Step 4a: History conversion\n",
    "            try:\n",
    "                working_history = agent._convert_history_to_sk([])\n",
    "                working_history.add_user_message(\"Hello debug test!\")\n",
    "                print(\"âœ… History conversion successful\")\n",
    "            except Exception as hist_error:\n",
    "                print(f\"âŒ History conversion error: {hist_error}\")\n",
    "                return False\n",
    "            \n",
    "            # Step 4b: Agent invoke\n",
    "            try:\n",
    "                print(\"Testing agent invoke directly...\")\n",
    "                resp_iter = agent.chat_agent.invoke(working_history)\n",
    "                print(f\"âœ… Invoke returned: {type(resp_iter)}\")\n",
    "                \n",
    "                # Step 4c: Async iteration\n",
    "                results = []\n",
    "                async for chunk in resp_iter:\n",
    "                    results.append(chunk)\n",
    "                    print(f\"âœ… Got chunk: {type(chunk)}\")\n",
    "                    break\n",
    "                \n",
    "                # Step 4d: Content extraction\n",
    "                if results:\n",
    "                    last_message = results[-1]\n",
    "                    content_obj = getattr(last_message, \"content\", None)\n",
    "                    if content_obj is not None:\n",
    "                        content = str(content_obj)\n",
    "                        print(f\"âœ… Content extracted: {content}\")\n",
    "                    else:\n",
    "                        print(\"âŒ No content in result\")\n",
    "                else:\n",
    "                    print(\"âŒ No results from invoke\")\n",
    "                \n",
    "            except Exception as invoke_error:\n",
    "                print(f\"âŒ Error in manual invoke: {invoke_error}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error in step-by-step debug: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "result = await debug_step_by_step()\n",
    "print(f\"Step-by-step result: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e9d0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed debug to trace the exact error location\n",
    "import os\n",
    "import sys\n",
    "import pathlib\n",
    "\n",
    "# Setup\n",
    "try:\n",
    "    from dotenv import load_dotenv, find_dotenv\n",
    "    dotenv_path = find_dotenv(usecwd=True)\n",
    "    if dotenv_path:\n",
    "        load_dotenv(dotenv_path, override=False)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    import shared  # noqa: F401\n",
    "except Exception:\n",
    "    nb_dir = pathlib.Path().resolve()\n",
    "    python_root = nb_dir.parent\n",
    "    if str(python_root) not in sys.path:\n",
    "        sys.path.insert(0, str(python_root))\n",
    "\n",
    "# Let's manually trace our process_message method step by step\n",
    "async def trace_process_message():\n",
    "    \"\"\"Manually trace each step of process_message to find the error.\"\"\"\n",
    "    missing = [k for k in (\"AZURE_OPENAI_ENDPOINT\",\"AZURE_OPENAI_DEPLOYMENT\",\"AZURE_OPENAI_KEY\") if not os.environ.get(k)]\n",
    "    if missing:\n",
    "        print(\"Missing config:\", missing)\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        from shared import AgentConfig, AgentType\n",
    "        from agents.semantic_kernel_agents import SemanticKernelGenericAgent\n",
    "\n",
    "        # Step 1: Create and initialize\n",
    "        print(\"1. Creating agent...\")\n",
    "        agent = SemanticKernelGenericAgent(\n",
    "            AgentConfig(name=\"TraceAgent\", agent_type=AgentType.GENERIC, instructions=\"You are a helpful assistant.\")\n",
    "        )\n",
    "        \n",
    "        print(\"2. Initializing agent...\")\n",
    "        await agent.initialize()\n",
    "        print(\"âœ… Agent initialized successfully\")\n",
    "\n",
    "        # Step 2: Manually call each part of process_message\n",
    "        message = \"Hello trace test\"\n",
    "        print(f\"3. Processing message: {message}\")\n",
    "        \n",
    "        # Check if agent is ready\n",
    "        if not agent.chat_agent:\n",
    "            print(\"âŒ Agent not initialized\")\n",
    "            return False\n",
    "        print(\"âœ… Agent is ready\")\n",
    "\n",
    "        # Step 3a: Convert history\n",
    "        print(\"4. Converting history...\")\n",
    "        try:\n",
    "            working_history = agent._convert_history_to_sk([])\n",
    "            print(\"âœ… History conversion successful\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ History conversion failed: {e}\")\n",
    "            return False\n",
    "\n",
    "        # Step 3b: Add message\n",
    "        print(\"5. Adding user message...\")\n",
    "        try:\n",
    "            working_history.add_user_message(message)\n",
    "            print(\"âœ… Message added to history\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Adding message failed: {e}\")\n",
    "            return False\n",
    "\n",
    "        # Step 3c: Invoke agent - THIS IS WHERE THE ERROR LIKELY HAPPENS\n",
    "        print(\"6. Invoking agent...\")\n",
    "        try:\n",
    "            resp_iter = agent.chat_agent.invoke(working_history)\n",
    "            print(f\"âœ… Invoke successful, got: {type(resp_iter)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Invoke failed: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return False\n",
    "\n",
    "        # Step 3d: Process response\n",
    "        print(\"7. Processing response...\")\n",
    "        try:\n",
    "            results = []\n",
    "            async for chunk in resp_iter:\n",
    "                results.append(chunk)\n",
    "                print(f\"  Got chunk: {type(chunk)}\")\n",
    "                # Only get first chunk for debugging\n",
    "                break\n",
    "            print(f\"âœ… Response processing successful, {len(results)} chunks\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Response processing failed: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return False\n",
    "\n",
    "        # Step 3e: Extract content\n",
    "        print(\"8. Extracting content...\")\n",
    "        try:\n",
    "            if results:\n",
    "                last_message = results[-1]\n",
    "                content_obj = getattr(last_message, \"content\", None)\n",
    "                if content_obj is not None:\n",
    "                    content = str(content_obj)\n",
    "                    print(f\"âœ… Content extracted: {content[:50]}...\")\n",
    "                else:\n",
    "                    content = str(last_message)\n",
    "                    print(f\"âœ… Fallback content: {content[:50]}...\")\n",
    "            else:\n",
    "                print(\"âŒ No results to extract content from\")\n",
    "                return False\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Content extraction failed: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return False\n",
    "\n",
    "        print(\"ğŸ¯ All steps completed successfully!\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Outer error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "# Run the trace\n",
    "result = await trace_process_message()\n",
    "print(f\"\\nTrace result: {'SUCCESS' if result else 'FAILED'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3599da71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force reload of agent modules to ensure we get the updated code\n",
    "import importlib\n",
    "import sys\n",
    "\n",
    "# Force reload agent modules\n",
    "if 'agents.semantic_kernel_agents' in sys.modules:\n",
    "    importlib.reload(sys.modules['agents.semantic_kernel_agents'])\n",
    "    print(\"âœ… Reloaded agents.semantic_kernel_agents\")\n",
    "\n",
    "if 'shared' in sys.modules:\n",
    "    importlib.reload(sys.modules['shared'])\n",
    "    print(\"âœ… Reloaded shared\")\n",
    "\n",
    "print(\"Modules reloaded, now test should use the fixed code.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641a7b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug: Check environment variables\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "print(\"ğŸ” Environment Variables Debug\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# First, let's see what .env files are available\n",
    "dotenv_path = find_dotenv(usecwd=True)\n",
    "print(f\"ğŸ“ Found .env file: {dotenv_path}\")\n",
    "\n",
    "if dotenv_path:\n",
    "    # Load the .env file\n",
    "    loaded = load_dotenv(dotenv_path, override=True)\n",
    "    print(f\"ğŸ“‹ .env loaded successfully: {loaded}\")\n",
    "else:\n",
    "    print(\"âš ï¸ No .env file found\")\n",
    "\n",
    "# Check for Azure OpenAI environment variables\n",
    "azure_vars = [\n",
    "    \"AZURE_OPENAI_API_KEY\",\n",
    "    \"AZURE_OPENAI_KEY\", \n",
    "    \"AZURE_OPENAI_ENDPOINT\",\n",
    "    \"AZURE_OPENAI_DEPLOYMENT_NAME\",\n",
    "    \"AZURE_OPENAI_DEPLOYMENT\",\n",
    "    \"AZURE_OPENAI_API_VERSION\"\n",
    "]\n",
    "\n",
    "print(\"\\nğŸ”‘ Azure OpenAI Environment Variables:\")\n",
    "for var in azure_vars:\n",
    "    value = os.getenv(var)\n",
    "    if value:\n",
    "        # Show only first few characters for security\n",
    "        display_value = f\"{value[:10]}...\" if len(value) > 10 else value\n",
    "        print(f\"âœ… {var}: {display_value}\")\n",
    "    else:\n",
    "        print(f\"âŒ {var}: Not set\")\n",
    "\n",
    "# Check all environment variables that contain \"AZURE\" or \"OPENAI\"\n",
    "print(\"\\nğŸŒ All Azure/OpenAI related environment variables:\")\n",
    "for key, value in os.environ.items():\n",
    "    if \"AZURE\" in key.upper() or \"OPENAI\" in key.upper():\n",
    "        display_value = f\"{value[:15]}...\" if len(value) > 15 else value\n",
    "        print(f\"  {key}: {display_value}\")\n",
    "\n",
    "print(\"\\nğŸ’¡ If variables are missing, ensure your .env file is in the correct location and properly formatted.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

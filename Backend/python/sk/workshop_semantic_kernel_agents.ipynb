{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c767c39",
   "metadata": {},
   "source": [
    "## Quick setup: use a local venv (Windows PowerShell)\n",
    "\n",
    "Manually creating and using a local virtual environment is recommended for this notebook.\n",
    "\n",
    "\n",
    "\n",
    "Steps (run these in an integrated terminal opened in `Backend/python/sk`):\n",
    "\n",
    "\n",
    "\n",
    "```powershell\n",
    "\n",
    "# 1) Create a virtual environment in this folder\n",
    "\n",
    "python -m venv .venv\n",
    "\n",
    "\n",
    "\n",
    "# 2) Activate it (PowerShell)\n",
    "\n",
    ".\\.venv\\Scripts\\Activate.ps1\n",
    "\n",
    "\n",
    "\n",
    "# 3) Upgrade pip and install ipykernel inside the venv\n",
    "\n",
    "python -m pip install --upgrade pip\n",
    "\n",
    "pip install ipykernel\n",
    "\n",
    "\n",
    "\n",
    "# Optional: install project requirements for the SK examples\n",
    "\n",
    "pip install -r requirements.txt\n",
    "\n",
    "\n",
    "\n",
    "# Optional: register this venv as a named Jupyter kernel\n",
    "\n",
    "# (helps you pick it explicitly in the VS Code kernel selector)\n",
    "\n",
    "python -m ipykernel install --user --name agents-sk-venv --display-name \"Python (.venv - SK)\"\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "Notes:\n",
    "\n",
    "- If activation is blocked by an execution policy, you can temporarily allow scripts for this session:\n",
    "\n",
    "  ```powershell\n",
    "\n",
    "  Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope Process\n",
    "\n",
    "  ```\n",
    "\n",
    "- In VS Code, use the Kernel picker (top-right of the notebook) and select the newly created `.venv` or the registered kernel display name."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b53890",
   "metadata": {},
   "source": [
    "# ðŸš€ Quick Start: Run This Workshop Notebook\n",
    "\n",
    "This section sets up everything you need to run the agents in this notebook with minimal friction.\n",
    "\n",
    "What it does:\n",
    "- Installs Python dependencies and the shared local library\n",
    "- Lets you provide API keys (Azure OpenAI and optional providers)\n",
    "- Saves them to a local .env for reuse (optional)\n",
    "- Verifies the project structure\n",
    "- Optionally runs a tiny smoke test if keys are present\n",
    "\n",
    "Proceed top-to-bottom; each step is self-checking and safe to rerun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056a8740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 â€” Install dependencies (safe to rerun)\n",
    "import os, sys, subprocess, textwrap, pathlib\n",
    "from importlib.util import find_spec\n",
    "\n",
    "nb_dir = pathlib.Path().resolve()\n",
    "project_root = nb_dir.parents[2] if (len(nb_dir.parents) >= 2) else nb_dir\n",
    "sk_dir = nb_dir  # this notebook lives in Backend/python/sk\n",
    "shared_dir = sk_dir.parent / \"shared\"\n",
    "req_file = sk_dir / \"requirements.txt\"\n",
    "\n",
    "print(f\"Notebook dir: {nb_dir}\")\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Using requirements: {req_file}\")\n",
    "print(f\"Shared package dir: {shared_dir}\")\n",
    "\n",
    "def run(cmd):\n",
    "    print(\"\\n$\", cmd)\n",
    "    result = subprocess.run(cmd, shell=True, text=True)\n",
    "    if result.returncode != 0:\n",
    "        raise SystemExit(f\"Command failed with exit code {result.returncode}\")\n",
    "\n",
    "# Use pip magics if available (keeps kernel env), fallback to subprocess\n",
    "try:\n",
    "    import IPython\n",
    "    get_ipython  # noqa\n",
    "    # Prefer %pip to ensure install into the current kernel\n",
    "    if req_file.exists():\n",
    "        _ = get_ipython().run_line_magic(\"pip\", f\"install -r {req_file}\")\n",
    "    else:\n",
    "        print(\"requirements.txt not found; skipping dependency install.\")\n",
    "    # Install shared lib in editable mode for local imports\n",
    "    if (shared_dir / \"setup.py\").exists():\n",
    "        _ = get_ipython().run_line_magic(\"pip\", f\"install -e {shared_dir}\")\n",
    "    else:\n",
    "        print(\"Shared library setup.py not found; skipping -e install.\")\n",
    "except Exception:\n",
    "    # Fallback: subprocess pip (may not target the active kernel)\n",
    "    if req_file.exists():\n",
    "        run(f\"python -m pip install -r \\\"{req_file}\\\"\")\n",
    "    if (shared_dir / \"setup.py\").exists():\n",
    "        run(f\"python -m pip install -e \\\"{shared_dir}\\\"\")\n",
    "\n",
    "print(\"\\nâœ… Dependencies installation step completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29af7014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 â€” Provide configuration (non-interactive, edit-and-run)\n",
    "# Edit the CONFIG values below (no prompts). Set WRITE_ENV_FILE=True to save to .env.\n",
    "import os, pathlib\n",
    "\n",
    "# Load .env early so DEFAULTS picks it up\n",
    "try:\n",
    "    from dotenv import load_dotenv, find_dotenv\n",
    "    dotenv_path = find_dotenv(usecwd=True)\n",
    "    loaded = load_dotenv(dotenv_path, override=False)\n",
    "    if loaded:\n",
    "        print(f\"Loaded .env from: {dotenv_path}\")\n",
    "    else:\n",
    "        print(\"No .env found via find_dotenv(usecwd=True); using process environment only.\")\n",
    "except Exception as _e:\n",
    "    # Safe fallback if python-dotenv isn't installed yet\n",
    "    print(\"python-dotenv not available yet; continuing without auto-loading .env\")\n",
    "\n",
    "# Current environment defaults\n",
    "DEFAULTS = {\n",
    "    \"AZURE_OPENAI_ENDPOINT\": os.environ.get(\"AZURE_OPENAI_ENDPOINT\", \"\"),\n",
    "    \"AZURE_OPENAI_DEPLOYMENT\": os.environ.get(\"AZURE_OPENAI_DEPLOYMENT\", \"\"),\n",
    "    \"AZURE_OPENAI_KEY\": os.environ.get(\"AZURE_OPENAI_KEY\", \"\"),\n",
    "    \"PROJECT_ENDPOINT\": os.environ.get(\"PROJECT_ENDPOINT\", \"\"),\n",
    "    \"PEOPLE_AGENT_ID\": os.environ.get(\"PEOPLE_AGENT_ID\", \"\"),\n",
    "    \"KNOWLEDGE_AGENT_ID\": os.environ.get(\"KNOWLEDGE_AGENT_ID\", \"\"),\n",
    "    \"GOOGLE_API_KEY\": os.environ.get(\"GOOGLE_API_KEY\", \"\"),\n",
    "    \"ENVIRONMENT\": os.environ.get(\"ENVIRONMENT\", \"development\"),\n",
    "    \"LOG_LEVEL\": os.environ.get(\"LOG_LEVEL\", \"INFO\"),\n",
    "}\n",
    "\n",
    "# EDIT THESE VALUES AS NEEDED. Leave as-is to keep current/defaults.\n",
    "CONFIG = {\n",
    "    \"AZURE_OPENAI_ENDPOINT\": DEFAULTS[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    \"AZURE_OPENAI_DEPLOYMENT\": DEFAULTS[\"AZURE_OPENAI_DEPLOYMENT\"],\n",
    "    \"AZURE_OPENAI_KEY\": DEFAULTS[\"AZURE_OPENAI_KEY\"],\n",
    "    \"PROJECT_ENDPOINT\": DEFAULTS[\"PROJECT_ENDPOINT\"],\n",
    "    \"PEOPLE_AGENT_ID\": DEFAULTS[\"PEOPLE_AGENT_ID\"],\n",
    "    \"KNOWLEDGE_AGENT_ID\": DEFAULTS[\"KNOWLEDGE_AGENT_ID\"],\n",
    "    \"GOOGLE_API_KEY\": DEFAULTS[\"GOOGLE_API_KEY\"],\n",
    "    \"ENVIRONMENT\": DEFAULTS[\"ENVIRONMENT\"],\n",
    "    \"LOG_LEVEL\": DEFAULTS[\"LOG_LEVEL\"],\n",
    "}\n",
    "\n",
    "# Toggle saving to .env in this folder\n",
    "WRITE_ENV_FILE = False\n",
    "ENV_FILE_NAME = \".env\"\n",
    "\n",
    "# Apply to current process env\n",
    "for k, v in CONFIG.items():\n",
    "    if v is not None and v != \"\":\n",
    "        os.environ[k] = v\n",
    "\n",
    "# Optionally write .env\n",
    "if WRITE_ENV_FILE:\n",
    "    env_path = pathlib.Path(ENV_FILE_NAME)\n",
    "    existing = {}\n",
    "    if env_path.exists():\n",
    "        try:\n",
    "            with env_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "                for line in f:\n",
    "                    line = line.strip()\n",
    "                    if line and not line.startswith(\"#\") and \"=\" in line:\n",
    "                        key, val = line.split(\"=\", 1)\n",
    "                        existing[key] = val\n",
    "        except Exception:\n",
    "            pass\n",
    "    existing.update({k: v for k, v in CONFIG.items() if v is not None and v != \"\"})\n",
    "    env_path.write_text(\"\\n\".join(f\"{k}={v}\" for k, v in existing.items()), encoding=\"utf-8\")\n",
    "    print(f\"Wrote {env_path.resolve()} with {len(existing)} keys.\")\n",
    "else:\n",
    "    print(\"Skipped writing .env (set WRITE_ENV_FILE=True to enable). Values active for this session only.\")\n",
    "\n",
    "# Status\n",
    "mask = lambda s, keep=4: s if not s or len(s) <= keep else (s[:keep] + \"â€¦\" + s[-2:])\n",
    "print(\"\\nCurrent config status:\")\n",
    "print(\"- AZURE_OPENAI_ENDPOINT:\", bool(CONFIG[\"AZURE_OPENAI_ENDPOINT\"]))\n",
    "print(\"- AZURE_OPENAI_DEPLOYMENT:\", bool(CONFIG[\"AZURE_OPENAI_DEPLOYMENT\"]))\n",
    "print(\"- AZURE_OPENAI_KEY:\", mask(CONFIG[\"AZURE_OPENAI_KEY\"]))\n",
    "print(\"- PROJECT_ENDPOINT:\", bool(CONFIG[\"PROJECT_ENDPOINT\"]))\n",
    "print(\"- PEOPLE_AGENT_ID:\", bool(CONFIG[\"PEOPLE_AGENT_ID\"]))\n",
    "print(\"- KNOWLEDGE_AGENT_ID:\", bool(CONFIG[\"KNOWLEDGE_AGENT_ID\"]))\n",
    "print(\"- GOOGLE_API_KEY:\", mask(CONFIG[\"GOOGLE_API_KEY\"]))\n",
    "print(\"\\nTip: You can edit `sk/config.yml` to tweak defaults and templates.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad6a57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 â€” Verify project structure (offline check)\n",
    "import runpy, pathlib, sys\n",
    "here = pathlib.Path().resolve()\n",
    "validate_path = here / \"validate_structure.py\"\n",
    "if validate_path.exists():\n",
    "    print(\"Running validate_structure.pyâ€¦\")\n",
    "    try:\n",
    "        runpy.run_path(str(validate_path))\n",
    "        print(\"\\nâœ… Structure validation completed.\")\n",
    "    except SystemExit as e:\n",
    "        print(f\"Validation exited with code: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Validation encountered an error: {e}\")\n",
    "else:\n",
    "    print(\"validate_structure.py not found; skipping.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6acd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4 â€” Optional smoke test (uses Azure OpenAI if configured)\n",
    "import os, sys, asyncio, pathlib\n",
    "\n",
    "# Ensure .env is loaded here too (so this works even if Step 2 wasn't run in this session)\n",
    "try:\n",
    "    from dotenv import load_dotenv, find_dotenv\n",
    "    dotenv_path = find_dotenv(usecwd=True)\n",
    "    if dotenv_path:\n",
    "        load_dotenv(dotenv_path, override=False)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Ensure local shared package is importable even if not installed\n",
    "try:\n",
    "    import shared  # noqa: F401\n",
    "except Exception:\n",
    "    nb_dir = pathlib.Path().resolve()\n",
    "    python_root = nb_dir.parent  # Backend/python\n",
    "    if str(python_root) not in sys.path:\n",
    "        sys.path.insert(0, str(python_root))\n",
    "\n",
    "# Helper to run a direct SK invocation as a fallback if wrapper path fails\n",
    "async def _direct_sk_ping():\n",
    "    try:\n",
    "        from semantic_kernel import Kernel\n",
    "        from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "        from semantic_kernel.agents import ChatCompletionAgent\n",
    "        from semantic_kernel.contents import ChatHistory\n",
    "\n",
    "        api_key = os.getenv(\"AZURE_OPENAI_KEY\") or os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "        endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "        deployment = os.getenv(\"AZURE_OPENAI_DEPLOYMENT\") or os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "        api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2024-06-01\")\n",
    "\n",
    "        kernel = Kernel()\n",
    "        kernel.add_service(\n",
    "            AzureChatCompletion(\n",
    "                service_id=\"azure_openai_chat\",\n",
    "                deployment_name=deployment,\n",
    "                endpoint=endpoint,\n",
    "                api_key=api_key,\n",
    "                api_version=api_version,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        agent = ChatCompletionAgent(\n",
    "            service_id=\"azure_openai_chat\",\n",
    "            kernel=kernel,\n",
    "            name=\"NotebookBasicSmoke\",\n",
    "            instructions=\"You are a helpful assistant. Reply briefly.\"\n",
    "        )\n",
    "\n",
    "        chat_history = ChatHistory()\n",
    "        chat_history.add_user_message(\"Hello from the workshop! Can you respond briefly?\")\n",
    "\n",
    "        # Handle async-iterable vs awaitable\n",
    "        results = []\n",
    "        resp_iter = agent.invoke(chat_history)\n",
    "        try:\n",
    "            async for chunk in resp_iter:\n",
    "                results.append(chunk)\n",
    "        except TypeError:\n",
    "            resp = await resp_iter  # type: ignore\n",
    "            results = list(resp) if isinstance(resp, (list, tuple)) else ([resp] if resp is not None else [])\n",
    "\n",
    "        if results:\n",
    "            last = results[-1]\n",
    "            text = getattr(last, \"content\", None) or str(last)\n",
    "        else:\n",
    "            text = \"(no content)\"\n",
    "\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        return f\"Direct SK ping failed: {e}\"\n",
    "\n",
    "# We'll reuse the SemanticKernelGenericAgent through the shared factory to keep behavior consistent\n",
    "async def _smoke_test():\n",
    "    missing = [k for k in (\"AZURE_OPENAI_ENDPOINT\",\"AZURE_OPENAI_DEPLOYMENT\",\"AZURE_OPENAI_KEY\") if not os.environ.get(k)]\n",
    "    if missing:\n",
    "        print(\"Skipping smoke test â€” missing:\", \", \".join(missing))\n",
    "        print(\"Provide keys above to enable a live round-trip.\")\n",
    "        return\n",
    "\n",
    "    # Try shared wrapper first, then fallback to direct SK ping for resilience\n",
    "    try:\n",
    "        from shared import AgentConfig, AgentType\n",
    "        from agents.semantic_kernel_agents import SemanticKernelGenericAgent\n",
    "\n",
    "        agent = SemanticKernelGenericAgent(\n",
    "            AgentConfig(name=\"NotebookSmokeAgent\", agent_type=AgentType.GENERIC, instructions=\"You are a helpful assistant.\")\n",
    "        )\n",
    "        await agent.initialize()\n",
    "        resp = await agent.process_message(\"Hello from the workshop! Can you respond briefly?\")\n",
    "        print(\"Agent:\", resp.agent_name)\n",
    "        print(\"Reply:\", (resp.content or \"\")[:500])\n",
    "        print(\"\\nâœ… Smoke test completed.\")\n",
    "    except Exception as e:\n",
    "        print(\"Wrapper smoke test failed:\", e)\n",
    "        # Fallback: direct SK call\n",
    "        text = await _direct_sk_ping()\n",
    "        print(\"Agent:\", \"NotebookBasicSmoke (direct SK)\")\n",
    "        print(\"Reply:\", text[:500])\n",
    "        print(\"\\nâœ… Smoke test completed (fallback path).\")\n",
    "\n",
    "# Jupyter-safe execution: use top-level await if a loop is already running\n",
    "try:\n",
    "    asyncio.get_running_loop()\n",
    "    # An event loop is running (likely in Jupyter) â€” use top-level await\n",
    "    await _smoke_test()\n",
    "except RuntimeError:\n",
    "    # No running loop â€” safe to use asyncio.run\n",
    "    asyncio.run(_smoke_test())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bd7f71",
   "metadata": {},
   "source": [
    "# Semantic Kernel Agents Workshop: Multi-Provider AI to Azure AI Foundry\n",
    "\n",
    "## ðŸš¨ IMPORTANT: First Time Users - READ THIS! ðŸš¨\n",
    "\n",
    "**âš ï¸ BEFORE RUNNING ANY CELLS:**\n",
    "1. **Select a Python Kernel** (top-right corner of notebook)\n",
    "2. **Look for \"Select Kernel\" button** - click it and choose Python\n",
    "3. **Wait for kernel to start** before running cells\n",
    "4. **Go to Section 0 below** and run the kernel test first!\n",
    "\n",
    "---\n",
    "\n",
    "Welcome to the Semantic Kernel workshop! You'll learn to build sophisticated AI agents using Microsoft's Semantic Kernel framework with multi-provider support, culminating in Azure AI Foundry integration.\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this workshop, you will:\n",
    "- Master Semantic Kernel architecture and concepts\n",
    "- Build multi-provider AI agents (Azure OpenAI, Gemini, Bedrock)\n",
    "- Create advanced agents with plugins and planners\n",
    "- Deploy production-ready Azure AI Foundry agents\n",
    "- Compare different AI provider capabilities\n",
    "\n",
    "## What Makes Semantic Kernel Special?\n",
    "- ðŸ”„ **Multi-Provider Support**: Azure, Google, AWS, and more\n",
    "- ðŸ§© **Plugin Architecture**: Extensible and modular design\n",
    "- ðŸŽ¯ **Planning Capabilities**: Automatic task orchestration\n",
    "- ðŸ¢ **Enterprise Ready**: Built for production scenarios\n",
    "\n",
    "Let's embark on this exciting journey! ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f18e9f0",
   "metadata": {},
   "source": [
    "## Section 1: Import Required Libraries\n",
    "\n",
    "Let's start by importing all necessary libraries for Semantic Kernel development, including multi-provider support and our modern agent architecture.\n",
    "\n",
    "**Note:** If you see any import errors, don't worry! The workshop includes fallback mechanisms and mock implementations to ensure you can still learn the concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5376b710",
   "metadata": {},
   "source": [
    "## Section 0: Environment Setup (Run This First!)\n",
    "\n",
    "âš ï¸ **IMPORTANT: SELECT PYTHON KERNEL FIRST!** âš ï¸\n",
    "\n",
    "**Before running any cells, you must select a Python kernel:**\n",
    "\n",
    "1. ðŸ‘€ **Look at the top-right corner** of this notebook\n",
    "2. ðŸ–±ï¸ **Click on \"Select Kernel\"** (or it might show \"No Kernel\" or \"Python\")  \n",
    "3. ðŸ **Choose a Python interpreter** from the list (system Python, conda, venv, etc.)\n",
    "4. â³ **Wait for \"Starting...\"** to complete\n",
    "5. âœ… **Then run the cells below**\n",
    "\n",
    "**If cells just \"spin\" and show no output, it means no kernel is selected!**\n",
    "\n",
    "---\n",
    "\n",
    "This section will:\n",
    "- Install all required Python packages from requirements.txt\n",
    "- Set up environment variables  \n",
    "- Verify the installation\n",
    "- Provide fallbacks if packages are missing\n",
    "\n",
    "**After selecting a kernel, run the cell below first before proceeding with the rest of the workshop!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91923a5b",
   "metadata": {},
   "source": [
    "### ðŸ”§ Common Issues: Kernel Setup\n",
    "\n",
    "**Issue 1: \"requires the ipykernel package\"**\n",
    "- **Solution**: Install ipykernel in your Python environment\n",
    "\n",
    "**Issue 2: \"ModuleNotFoundError: No module named 'psutil'\" (Windows ARM64)**\n",
    "- This is a known issue with Windows ARM64 and Python 3.13\n",
    "- **Quick Solutions**:\n",
    "\n",
    "**Option A: Use System Python (Recommended)**\n",
    "1. Select \"Python\" (not .venv) from the kernel picker (top-right)\n",
    "2. This uses your system Python which likely has everything installed\n",
    "\n",
    "**Option B: Use Conda Environment**\n",
    "1. Install Anaconda/Miniconda\n",
    "2. Create conda environment: `conda create -n workshop python=3.11`\n",
    "3. Activate: `conda activate workshop`\n",
    "4. Install: `conda install ipykernel jupyter`\n",
    "5. Select this kernel in VS Code\n",
    "\n",
    "**Option C: Use Python 3.11 instead of 3.13**\n",
    "- Python 3.13 is very new and some packages aren't ready\n",
    "- Install Python 3.11 and create a new virtual environment\n",
    "\n",
    "**For Workshop Attendees**: Don't worry! The workshop includes fallback code that works even without real Azure services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a73c940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”§ Environment Check (Minimal Version)\n",
    "\n",
    "import sys\n",
    "\n",
    "print(\"ðŸ” QUICK ENVIRONMENT CHECK\")\n",
    "print(\"=\" * 25)\n",
    "print(f\"ðŸ Python: {sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}\")\n",
    "print(f\"\udccd Location: {sys.executable}\")\n",
    "\n",
    "# Simple environment detection\n",
    "if 'conda' in sys.executable:\n",
    "    env_type = \"Conda\"\n",
    "elif hasattr(sys, 'real_prefix') or (hasattr(sys, 'base_prefix') and sys.base_prefix != sys.prefix):\n",
    "    env_type = \"Virtual Environment\"\n",
    "else:\n",
    "    env_type = \"System Python\"\n",
    "\n",
    "print(f\"ðŸŽ¯ Environment: {env_type}\")\n",
    "print(\"\\nâœ… Basic check complete - ready for workshop!\")\n",
    "print(\"ðŸ’¡ If you encounter issues, try restarting the kernel.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1136b91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ§ª KERNEL TEST - This should work with any Python kernel!\n",
    "\n",
    "print(\"ðŸŽ‰ SUCCESS! Your Python kernel is working correctly!\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Basic Python test\n",
    "result = 2 + 2\n",
    "print(f\"ðŸ”¢ Basic math: 2 + 2 = {result}\")\n",
    "\n",
    "# Version info\n",
    "import sys\n",
    "print(f\"ðŸ Python: {sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}\")\n",
    "\n",
    "# Test basic operations\n",
    "test_string = \"Hello Workshop!\"\n",
    "print(f\"ðŸ“ String test: {test_string}\")\n",
    "\n",
    "# Test list operations\n",
    "test_list = [1, 2, 3, 4, 5]\n",
    "print(f\"ðŸ“‹ List test: {test_list} â†’ Sum: {sum(test_list)}\")\n",
    "\n",
    "print(\"\\nâœ… KERNEL VERIFICATION COMPLETE!\")\n",
    "print(\"ðŸš€ If you see this output, your kernel is working properly!\")\n",
    "\n",
    "print(\"\\nðŸ“‹ Next Steps:\")\n",
    "print(\"1. âœ… Kernel is working (you can see this output)\")\n",
    "print(\"2. â–¶ï¸ Run the environment check cell below\")\n",
    "print(\"3. ðŸ”„ Continue through the workshop\")\n",
    "print(\"4. ðŸŽ­ Don't worry about missing packages - we have fallbacks!\")\n",
    "\n",
    "print(\"\\nðŸŽ“ READY FOR SEMANTIC KERNEL WORKSHOP!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae12f3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Setup Cell - Run This First!\n",
    "import sys\n",
    "import subprocess\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def install_requirements():\n",
    "    \"\"\"Install requirements.txt packages directly from the notebook.\"\"\"\n",
    "    print(\"ðŸš€ Setting up workshop environment...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Get the current directory (where the notebook is located)\n",
    "    notebook_dir = Path.cwd()\n",
    "    requirements_file = notebook_dir / \"requirements.txt\"\n",
    "    \n",
    "    print(f\"ðŸ“ Working directory: {notebook_dir}\")\n",
    "    print(f\"ðŸ“‹ Looking for requirements file: {requirements_file}\")\n",
    "    \n",
    "    if requirements_file.exists():\n",
    "        print(f\"âœ… Found requirements.txt\")\n",
    "        print(\"ðŸ“¦ Installing packages... (this may take a few minutes)\")\n",
    "        \n",
    "        # Install packages using pip\n",
    "        try:\n",
    "            result = subprocess.run([\n",
    "                sys.executable, \"-m\", \"pip\", \"install\", \"-r\", str(requirements_file)\n",
    "            ], capture_output=True, text=True, check=True)\n",
    "            \n",
    "            print(\"âœ… Packages installed successfully!\")\n",
    "            if result.stdout:\n",
    "                print(\"ðŸ“‹ Installation details:\")\n",
    "                print(result.stdout)\n",
    "                \n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"âŒ Error installing packages: {e}\")\n",
    "            print(f\"Error output: {e.stderr}\")\n",
    "            print(\"ðŸ”„ Trying to continue with available packages...\")\n",
    "            \n",
    "    else:\n",
    "        print(f\"âš ï¸ requirements.txt not found at {requirements_file}\")\n",
    "        print(\"ðŸ”„ Continuing without installing requirements...\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "def setup_environment():\n",
    "    \"\"\"Setup environment variables and configuration.\"\"\"\n",
    "    print(\"\\nðŸ”§ Setting up environment variables...\")\n",
    "    \n",
    "    # Load environment variables from .env if it exists\n",
    "    env_file = Path.cwd() / \".env\"\n",
    "    if env_file.exists():\n",
    "        print(f\"âœ… Found .env file: {env_file}\")\n",
    "        from dotenv import load_dotenv\n",
    "        load_dotenv()\n",
    "        print(\"âœ… Environment variables loaded from .env\")\n",
    "    else:\n",
    "        print(\"âš ï¸ No .env file found. You may need to set Azure credentials manually.\")\n",
    "        print(\"ðŸ’¡ Expected environment variables:\")\n",
    "        print(\"   - AZURE_OPENAI_API_KEY\")\n",
    "        print(\"   - AZURE_OPENAI_ENDPOINT\") \n",
    "        print(\"   - AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "        print(\"   - AZURE_AI_FOUNDRY_ENDPOINT (optional)\")\n",
    "        print(\"   - AZURE_AI_FOUNDRY_API_KEY (optional)\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "def verify_installation():\n",
    "    \"\"\"Verify that key packages are installed and working.\"\"\"\n",
    "    print(\"\\nðŸ§ª Verifying installation...\")\n",
    "    \n",
    "    packages_to_check = [\n",
    "        \"semantic_kernel\",\n",
    "        \"azure.identity\", \n",
    "        \"azure.ai.projects\",\n",
    "        \"dotenv\",\n",
    "        \"yaml\"\n",
    "    ]\n",
    "    \n",
    "    failed_imports = []\n",
    "    \n",
    "    for package in packages_to_check:\n",
    "        try:\n",
    "            if package == \"semantic_kernel\":\n",
    "                import semantic_kernel as sk\n",
    "                print(f\"âœ… {package} v{sk.__version__}\")\n",
    "            elif package == \"azure.identity\":\n",
    "                from azure.identity import DefaultAzureCredential\n",
    "                print(f\"âœ… {package}\")\n",
    "            elif package == \"azure.ai.projects\":\n",
    "                from azure.ai.projects import AIProjectClient\n",
    "                print(f\"âœ… {package}\")\n",
    "            elif package == \"dotenv\":\n",
    "                from dotenv import load_dotenv\n",
    "                print(f\"âœ… {package}\")\n",
    "            elif package == \"yaml\":\n",
    "                import yaml\n",
    "                print(f\"âœ… {package}\")\n",
    "                \n",
    "        except ImportError as e:\n",
    "            print(f\"âŒ {package}: {e}\")\n",
    "            failed_imports.append(package)\n",
    "    \n",
    "    if failed_imports:\n",
    "        print(f\"\\nâš ï¸ Some packages failed to import: {failed_imports}\")\n",
    "        print(\"ðŸ”„ The workshop will use mock implementations where needed.\")\n",
    "    else:\n",
    "        print(\"\\nâœ… All key packages verified successfully!\")\n",
    "    \n",
    "    return len(failed_imports) == 0\n",
    "\n",
    "# Run the setup process\n",
    "print(\"ðŸŽ“ SEMANTIC KERNEL WORKSHOP - ENVIRONMENT SETUP\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Step 1: Install requirements\n",
    "install_success = install_requirements()\n",
    "\n",
    "# Step 2: Setup environment \n",
    "env_success = setup_environment()\n",
    "\n",
    "# Step 3: Verify installation\n",
    "verify_success = verify_installation()\n",
    "\n",
    "print(\"\\nðŸŽ¯ SETUP COMPLETE!\")\n",
    "print(\"=\" * 20)\n",
    "\n",
    "if install_success and env_success and verify_success:\n",
    "    print(\"âœ… Environment setup completed successfully!\")\n",
    "    print(\"ðŸš€ You're ready to proceed with the workshop!\")\n",
    "else:\n",
    "    print(\"âš ï¸ Setup completed with some warnings.\")\n",
    "    print(\"ðŸ”„ The workshop will adapt and use mock implementations where needed.\")\n",
    "\n",
    "print(\"\\nðŸ“ Next Steps:\")\n",
    "print(\"1. ðŸ“– Read through the workshop introduction below\")\n",
    "print(\"2. â–¶ï¸ Run the import cell (Section 1)\")\n",
    "print(\"3. ðŸ§ª Follow along with each section step by step\")\n",
    "\n",
    "print(f\"\\nðŸ Python version: {sys.version}\")\n",
    "print(f\"ðŸ“ Working directory: {Path.cwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2c0117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Python libraries\n",
    "import os\n",
    "import asyncio\n",
    "import logging\n",
    "from typing import Dict, Any, List, Optional, Union\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Environment and configuration\n",
    "from dotenv import load_dotenv\n",
    "import yaml\n",
    "\n",
    "# Azure authentication and services (following security best practices)\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.projects import AIProjectClient\n",
    "\n",
    "# Semantic Kernel core components\n",
    "import semantic_kernel as sk\n",
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "from semantic_kernel.agents import ChatCompletionAgent\n",
    "from semantic_kernel.contents import ChatHistory, ChatMessageContent, AuthorRole\n",
    "from semantic_kernel.prompt_template import InputVariable, PromptTemplateConfig\n",
    "from semantic_kernel.functions import KernelFunctionFromPrompt\n",
    "\n",
    "# Our modern agent architecture - Updated import path for Jupyter notebooks\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory to sys.path (notebook-safe approach)\n",
    "try:\n",
    "    # First try using existing notebook directory variables if available\n",
    "    if 'python_root' in globals():\n",
    "        if str(python_root) not in sys.path:\n",
    "            sys.path.insert(0, str(python_root))\n",
    "    else:\n",
    "        # Fallback: get current directory and navigate to python root\n",
    "        nb_dir = Path().resolve()\n",
    "        python_root = nb_dir.parent  # Backend/python\n",
    "        if str(python_root) not in sys.path:\n",
    "            sys.path.insert(0, str(python_root))\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Path setup warning: {e}\")\n",
    "    print(\"Continuing without shared imports...\")\n",
    "\n",
    "# Import shared modules (with error handling)\n",
    "try:\n",
    "    from shared import (\n",
    "        BaseAgent, AgentConfig, AgentMessage, AgentResponse,\n",
    "        AgentRegistry, MessageRole, AgentType\n",
    "    )\n",
    "    shared_import_success = True\n",
    "except ImportError as e:\n",
    "    print(f\"âš ï¸ Shared import warning: {e}\")\n",
    "    print(\"Some features may be limited, but core functionality will work\")\n",
    "    shared_import_success = False\n",
    "\n",
    "# Import Semantic Kernel specific implementations (with error handling)\n",
    "try:\n",
    "    from agents.semantic_kernel_agents import (\n",
    "        SemanticKernelGenericAgent, \n",
    "        SemanticKernelAzureFoundryAgent,\n",
    "        SemanticKernelAgentFactory\n",
    "    )\n",
    "    from routers.semantic_kernel_router import SemanticKernelLLMRouter\n",
    "    agents_import_success = True\n",
    "except ImportError as e:\n",
    "    print(f\"âš ï¸ Agents import warning: {e}\")\n",
    "    print(\"Will use basic implementations if needed\")\n",
    "    agents_import_success = False\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"âœ… Core libraries imported successfully!\")\n",
    "print(f\"ðŸ§  Semantic Kernel version: {sk.__version__}\")\n",
    "print(f\"ðŸ Python version: {sys.version}\")\n",
    "print(f\"ðŸ“ Working directory: {os.getcwd()}\")\n",
    "\n",
    "if shared_import_success and agents_import_success:\n",
    "    print(\"ðŸ”„ Full Azure-focused agent development ready!\")\n",
    "else:\n",
    "    print(\"ðŸ”„ Basic agent functionality ready (some advanced features may be limited)\")\n",
    "    print(\"ðŸ’¡ This is normal - the workshop will adapt!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0a9ab2",
   "metadata": {},
   "source": [
    "## Section 2: Understanding Semantic Kernel Architecture\n",
    "\n",
    "**Semantic Kernel** provides a powerful framework for building AI agents with a plugin-based architecture. Let's understand the key components:\n",
    "\n",
    "### ðŸ—ï¸ Core Architecture Components:\n",
    "\n",
    "1. **Kernel**: The central orchestrator that manages plugins, connectors, and execution context\n",
    "2. **Connectors**: Bridge between the kernel and various AI services (Azure OpenAI, Azure AI Foundry, Google, etc.)\n",
    "3. **Plugins**: Reusable skill sets that can be composed together for complex behaviors\n",
    "4. **Functions**: Individual atomic operations that can be native code or AI-powered prompts\n",
    "5. **Memory & Planning**: Advanced features for context retention and multi-step reasoning\n",
    "\n",
    "### ðŸ”„ Multi-Provider Support:\n",
    "\n",
    "Semantic Kernel excels at supporting multiple AI providers in a unified interface:\n",
    "- **Azure OpenAI**: Direct Azure OpenAI service integration\n",
    "- **Azure AI Foundry**: Enterprise-grade managed service with enhanced security\n",
    "- **Local Models**: Support for self-hosted models\n",
    "\n",
    "### ðŸ›¡ï¸ Enterprise Features:\n",
    "- Managed identity integration\n",
    "- Token management and rate limiting\n",
    "- Plugin composition and chaining\n",
    "- Telemetry and observability\n",
    "- Security best practices\n",
    "\n",
    "Let's explore these concepts through hands-on examples!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558658e2",
   "metadata": {},
   "source": [
    "## Section 3: Creating a Basic Semantic Kernel Agent\n",
    "\n",
    "Let's start with a simple generic agent using Semantic Kernel. This demonstrates the foundational concepts before moving to enterprise features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d253ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def create_basic_semantic_kernel_agent():\n",
    "    \"\"\"\n",
    "    Create a basic Semantic Kernel agent using ChatCompletionAgent.\n",
    "    This is the modern, recommended approach for building SK agents.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create Kernel instance\n",
    "        kernel = Kernel()\n",
    "        \n",
    "        # Configuration for Azure OpenAI (using correct environment variable names)\n",
    "        azure_openai_config = {\n",
    "            \"api_key\": os.getenv(\"AZURE_OPENAI_KEY\") or os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "            \"endpoint\": os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "            \"api_version\": os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2024-06-01\"),\n",
    "            \"deployment_name\": os.getenv(\"AZURE_OPENAI_DEPLOYMENT\") or os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\", \"gpt-4\")\n",
    "        }\n",
    "        \n",
    "        if not all([azure_openai_config[\"api_key\"], azure_openai_config[\"endpoint\"]]):\n",
    "            print(\"âš ï¸ Azure OpenAI credentials not found. Using mock responses.\")\n",
    "            print(f\"  API Key found: {bool(azure_openai_config['api_key'])}\")\n",
    "            print(f\"  Endpoint found: {bool(azure_openai_config['endpoint'])}\")\n",
    "            return create_mock_sk_agent()\n",
    "        \n",
    "        # Add chat completion service to kernel - Updated API\n",
    "        chat_completion = AzureChatCompletion(\n",
    "            service_id=\"azure_openai_chat\",\n",
    "            deployment_name=azure_openai_config[\"deployment_name\"],\n",
    "            endpoint=azure_openai_config[\"endpoint\"],\n",
    "            api_key=azure_openai_config[\"api_key\"],\n",
    "            api_version=azure_openai_config[\"api_version\"]\n",
    "        )\n",
    "        \n",
    "        kernel.add_service(chat_completion)\n",
    "        \n",
    "        # Create ChatCompletionAgent (Modern SK approach)\n",
    "        agent = ChatCompletionAgent(\n",
    "            service=chat_completion,  # Use service parameter instead of service_id\n",
    "            name=\"BasicWorkshopAgent\",\n",
    "            instructions=\"\"\"You are a helpful AI assistant for a Semantic Kernel workshop. \n",
    "            You should:\n",
    "            - Provide clear, educational responses about AI and Semantic Kernel\n",
    "            - Be enthusiastic about learning and development\n",
    "            - Help users understand agent concepts step by step\n",
    "            - Give practical examples when explaining concepts\"\"\",\n",
    "            description=\"Basic Semantic Kernel agent for workshop demonstrations\"\n",
    "        )\n",
    "        \n",
    "        print(\"âœ… Basic Semantic Kernel ChatCompletionAgent created successfully!\")\n",
    "        print(f\"ðŸ§  Agent Name: {agent.name}\")\n",
    "        print(f\"ðŸ”— Using model: {azure_openai_config['deployment_name']}\")\n",
    "        print(f\"ðŸ”— Endpoint: {azure_openai_config['endpoint'][:30]}...\")\n",
    "        \n",
    "        return agent\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error creating basic agent: {str(e)}\")\n",
    "        print(\"ðŸ”„ Falling back to mock agent for demonstration...\")\n",
    "        return create_mock_sk_agent()\n",
    "\n",
    "def create_mock_sk_agent():\n",
    "    \"\"\"Create a mock ChatCompletionAgent for demonstration when real credentials aren't available.\"\"\"\n",
    "    print(\"ðŸŽ­ Creating mock Semantic Kernel ChatCompletionAgent for demonstration...\")\n",
    "    \n",
    "    class MockChatCompletionAgent:\n",
    "        def __init__(self):\n",
    "            self.name = \"MockBasicWorkshopAgent\"\n",
    "            self.description = \"Mock Semantic Kernel agent for workshop demonstrations\"\n",
    "            self.instructions = \"Mock agent instructions\"\n",
    "            \n",
    "        async def invoke(self, chat_history):\n",
    "            # Get the last user message\n",
    "            last_message = \"\"\n",
    "            if chat_history and len(chat_history.messages) > 0:\n",
    "                last_message = chat_history.messages[-1].content\n",
    "            \n",
    "            # Mock response using async generator pattern\n",
    "            class MockResponse:\n",
    "                def __init__(self, content):\n",
    "                    self.content = content\n",
    "                    \n",
    "                def __aiter__(self):\n",
    "                    return self\n",
    "                    \n",
    "                async def __anext__(self):\n",
    "                    # Return one response and then stop\n",
    "                    if hasattr(self, '_returned'):\n",
    "                        raise StopAsyncIteration\n",
    "                    self._returned = True\n",
    "                    \n",
    "                    class MockResponseItem:\n",
    "                        def __init__(self, content):\n",
    "                            self.content = MockContent(content)\n",
    "                    \n",
    "                    class MockContent:\n",
    "                        def __init__(self, text):\n",
    "                            self._text = text\n",
    "                        \n",
    "                        def __str__(self):\n",
    "                            return self._text\n",
    "                    \n",
    "                    return MockResponseItem(f\"Mock SK ChatCompletionAgent Response: I understand you said '{last_message[:50]}...'. This is a demonstration response from the mock Semantic Kernel ChatCompletionAgent. In a real scenario, this would use Azure OpenAI to provide intelligent responses.\")\n",
    "            \n",
    "            return MockResponse(\"\")\n",
    "    \n",
    "    return MockChatCompletionAgent()\n",
    "\n",
    "# Create the basic agent using modern SK patterns\n",
    "basic_agent = await create_basic_semantic_kernel_agent()\n",
    "\n",
    "# Test the basic agent\n",
    "print(\"\\nðŸ§ª Testing Basic Semantic Kernel ChatCompletionAgent:\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "test_message = \"What is Semantic Kernel and how does it work with ChatCompletionAgent?\"\n",
    "\n",
    "try:\n",
    "    # Create chat history for the conversation\n",
    "    chat_history = ChatHistory()\n",
    "    chat_history.add_user_message(test_message)\n",
    "    \n",
    "    # Handle both real and mock agents\n",
    "    if hasattr(basic_agent, 'invoke'):\n",
    "        response_iter = basic_agent.invoke(chat_history)\n",
    "        \n",
    "        # Handle async generator response\n",
    "        response_text = \"\"\n",
    "        try:\n",
    "            async for chunk in response_iter:\n",
    "                if hasattr(chunk, 'content'):\n",
    "                    response_text = str(chunk.content)\n",
    "                else:\n",
    "                    response_text = str(chunk)\n",
    "                break  # Get first response\n",
    "        except Exception as e:\n",
    "            print(f\"Response processing note: {e}\")\n",
    "            response_text = \"Response received but formatting may vary\"\n",
    "    else:\n",
    "        # Mock agent\n",
    "        response_text = await basic_agent.invoke(chat_history)\n",
    "    \n",
    "    print(f\"ðŸ‘¤ User: {test_message}\")\n",
    "    print(f\"ðŸ¤– Agent: {response_text}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error during test: {str(e)}\")\n",
    "    print(\"ðŸ¤– Agent: I'm a basic Semantic Kernel ChatCompletionAgent. I can help you with various tasks using modern SK patterns!\")\n",
    "\n",
    "print(\"\\nâœ¨ Basic ChatCompletionAgent demonstration complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4195feb6",
   "metadata": {},
   "source": [
    "## Section 4: Enhanced Semantic Kernel Agents with Azure Integration\n",
    "\n",
    "Let's enhance our Semantic Kernel agents to work seamlessly with Azure services, focusing on the progression from basic Azure OpenAI to enterprise Azure AI Foundry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c183c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def create_enhanced_azure_agents():\n",
    "    \"\"\"\n",
    "    Create enhanced Semantic Kernel ChatCompletionAgents with Azure services.\n",
    "    This demonstrates progression from basic to enterprise Azure integration using modern SK patterns.\n",
    "    \"\"\"\n",
    "    kernel = Kernel()\n",
    "    agents_created = []\n",
    "    \n",
    "    try:\n",
    "        # 1. Azure OpenAI Provider (Standard approach)\n",
    "        azure_openai_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "        azure_openai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "        \n",
    "        if azure_openai_key and azure_openai_endpoint:\n",
    "            azure_openai_chat = AzureOpenAIChatCompletion(\n",
    "                service_id=\"azure_openai\",\n",
    "                deployment_name=os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\", \"gpt-4\"),\n",
    "                endpoint=azure_openai_endpoint,\n",
    "                api_key=azure_openai_key,\n",
    "                api_version=\"2024-02-01\"\n",
    "            )\n",
    "            kernel.add_service(azure_openai_chat)\n",
    "            \n",
    "            # Create Azure OpenAI agent\n",
    "            azure_openai_agent = ChatCompletionAgent(\n",
    "                service_id=\"azure_openai\",\n",
    "                kernel=kernel,\n",
    "                name=\"AzureOpenAIAgent\",\n",
    "                instructions=\"\"\"You are a technical expert specializing in Azure OpenAI services.\n",
    "                Provide detailed, accurate technical explanations with:\n",
    "                - Core concepts and architecture\n",
    "                - Practical implementation examples\n",
    "                - Best practices for Azure integration\n",
    "                - Common pitfalls and how to avoid them\"\"\",\n",
    "                description=\"Technical expert agent using Azure OpenAI\"\n",
    "            )\n",
    "            \n",
    "            agents_created.append((\"Azure OpenAI\", azure_openai_agent))\n",
    "            print(\"âœ… Azure OpenAI ChatCompletionAgent configured\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Azure OpenAI setup failed: {str(e)}\")\n",
    "    \n",
    "    try:\n",
    "        # 2. Azure AI Inference (Foundry) Provider - Enterprise approach\n",
    "        foundry_endpoint = os.getenv(\"AZURE_AI_FOUNDRY_ENDPOINT\")\n",
    "        foundry_key = os.getenv(\"AZURE_AI_FOUNDRY_API_KEY\")\n",
    "        \n",
    "        if foundry_endpoint and foundry_key:\n",
    "            foundry_chat = AzureAIInferenceChatCompletion(\n",
    "                service_id=\"azure_foundry\",\n",
    "                endpoint=foundry_endpoint,\n",
    "                api_key=foundry_key\n",
    "            )\n",
    "            kernel.add_service(foundry_chat)\n",
    "            \n",
    "            # Create Azure AI Foundry agent\n",
    "            foundry_agent = ChatCompletionAgent(\n",
    "                service_id=\"azure_foundry\",\n",
    "                kernel=kernel,\n",
    "                name=\"AzureFoundryAgent\",\n",
    "                instructions=\"\"\"You are an enterprise AI specialist focusing on Azure AI Foundry.\n",
    "                Provide comprehensive analysis including:\n",
    "                - Strategic insights and recommendations\n",
    "                - Enterprise architecture patterns\n",
    "                - Scalability and security considerations\n",
    "                - ROI and business value propositions\"\"\",\n",
    "                description=\"Enterprise specialist agent using Azure AI Foundry\"\n",
    "            )\n",
    "            \n",
    "            agents_created.append((\"Azure AI Foundry\", foundry_agent))\n",
    "            print(\"âœ… Azure AI Foundry ChatCompletionAgent configured\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Azure AI Foundry setup failed: {str(e)}\")\n",
    "    \n",
    "    if not agents_created:\n",
    "        print(\"âš ï¸ No Azure agents configured. Using mock agents for demonstration.\")\n",
    "        mock_agent = create_mock_enhanced_agent()\n",
    "        agents_created = [(\"Mock Provider\", mock_agent)]\n",
    "    \n",
    "    print(f\"\\nðŸ”— Total Azure agents configured: {len(agents_created)}\")\n",
    "    print(f\"ðŸ“‹ Available Azure agents: {', '.join([name for name, _ in agents_created])}\")\n",
    "    \n",
    "    return agents_created\n",
    "\n",
    "def create_mock_enhanced_agent():\n",
    "    \"\"\"Create a mock enhanced agent for demonstration.\"\"\"\n",
    "    class MockEnhancedAgent:\n",
    "        def __init__(self):\n",
    "            self.name = \"MockEnhancedAgent\"\n",
    "            self.description = \"Mock enhanced agent for demonstration\"\n",
    "            \n",
    "        async def invoke(self, chat_history):\n",
    "            last_message = \"\"\n",
    "            if chat_history and len(chat_history.messages) > 0:\n",
    "                last_message = chat_history.messages[-1].content\n",
    "            \n",
    "            return f\"Mock Enhanced Response: This demonstrates how Semantic Kernel ChatCompletionAgents can handle specialized requests like '{last_message[:50]}...'. In a real scenario, this would use Azure services to provide expert-level responses.\"\n",
    "    \n",
    "    return MockEnhancedAgent()\n",
    "\n",
    "# Create enhanced Azure agents\n",
    "azure_agents = await create_enhanced_azure_agents()\n",
    "\n",
    "print(\"\\nðŸ§  Enhanced Azure Semantic Kernel Agents Setup Complete!\")\n",
    "print(f\"ðŸ”§ Created {len(azure_agents)} specialized ChatCompletionAgents\")\n",
    "print(\"ðŸŽ¯ Ready to demonstrate Azure-focused AI capabilities with modern SK patterns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f8127b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Azure-focused ChatCompletionAgent capabilities\n",
    "async def test_enhanced_azure_agents():\n",
    "    \"\"\"Test different specialized ChatCompletionAgents across available Azure providers.\"\"\"\n",
    "    \n",
    "    print(\"ðŸ§ª Testing Enhanced Azure Semantic Kernel ChatCompletionAgents\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Test scenarios with different agent specializations\n",
    "    test_scenarios = [\n",
    "        {\n",
    "            \"agent_type\": \"technical\",\n",
    "            \"message\": \"What are the key differences between supervised and unsupervised learning in machine learning?\",\n",
    "            \"title\": \"Technical Expert - ML Question\"\n",
    "        },\n",
    "        {\n",
    "            \"agent_type\": \"creative\",\n",
    "            \"message\": \"Write an engaging introduction for an AI workshop blog post\",\n",
    "            \"title\": \"Creative Assistant - Content Creation\"\n",
    "        },\n",
    "        {\n",
    "            \"agent_type\": \"strategic\",\n",
    "            \"message\": \"Analyze the growing adoption of AI agents in enterprise software and provide strategic recommendations\",\n",
    "            \"title\": \"Strategic Analyst - Market Analysis\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for i, scenario in enumerate(test_scenarios, 1):\n",
    "        print(f\"\\nðŸ“‹ Test {i}: {scenario['title']}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        try:\n",
    "            # Create chat history for this test\n",
    "            chat_history = ChatHistory()\n",
    "            chat_history.add_user_message(scenario[\"message\"])\n",
    "            \n",
    "            # Find appropriate agent (use first available for demo)\n",
    "            if azure_agents:\n",
    "                agent_name, agent = azure_agents[0]  # Use first agent for simplicity\n",
    "                \n",
    "                # Test with the agent\n",
    "                if hasattr(agent, 'invoke'):\n",
    "                    response = await agent.invoke(chat_history)\n",
    "                    if hasattr(response, 'content'):\n",
    "                        response_text = response.content\n",
    "                    else:\n",
    "                        response_text = str(response)\n",
    "                else:\n",
    "                    # Mock agent\n",
    "                    response_text = await agent.invoke(chat_history)\n",
    "                \n",
    "                print(f\"ðŸ¤– {agent_name} Agent: {response_text[:200]}...\")\n",
    "                \n",
    "            else:\n",
    "                print(f\"âŒ No agents available for {scenario['title']}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error testing {scenario['title']}: {str(e)}\")\n",
    "            print(f\"ðŸ¤– Fallback: This would normally provide a {scenario['title'].lower()} using Semantic Kernel ChatCompletionAgent\")\n",
    "    \n",
    "    print(f\"\\nâœ¨ Enhanced Azure ChatCompletionAgent testing complete!\")\n",
    "    print(f\"ðŸ”— Tested across {len(azure_agents)} Azure agent(s)\")\n",
    "    print(\"ðŸŽ¯ Demonstrated modern Semantic Kernel agent patterns!\")\n",
    "\n",
    "# Run the enhanced Azure tests\n",
    "await test_enhanced_azure_agents()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbcfda6",
   "metadata": {},
   "source": [
    "## Section 5: Advanced Semantic Kernel Features\n",
    "\n",
    "Now let's explore advanced Semantic Kernel capabilities including plugins, memory, and planning. These features enable more sophisticated agent behaviors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f296b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.memory import SemanticTextMemory\n",
    "from semantic_kernel.core_plugins import MathPlugin, TimePlugin, TextPlugin\n",
    "\n",
    "class AdvancedSemanticKernelAgent:\n",
    "    \"\"\"\n",
    "    Advanced Semantic Kernel agent with plugins, memory, and enhanced capabilities.\n",
    "    This demonstrates enterprise-ready features before moving to Azure AI Foundry.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, kernel):\n",
    "        self.kernel = kernel\n",
    "        self.conversation_history = []\n",
    "        self.setup_plugins()\n",
    "        self.setup_memory()\n",
    "    \n",
    "    def setup_plugins(self):\n",
    "        \"\"\"Add built-in and custom plugins to extend agent capabilities.\"\"\"\n",
    "        try:\n",
    "            # Add built-in plugins\n",
    "            self.kernel.add_plugin(MathPlugin(), plugin_name=\"math\")\n",
    "            self.kernel.add_plugin(TimePlugin(), plugin_name=\"time\") \n",
    "            self.kernel.add_plugin(TextPlugin(), plugin_name=\"text\")\n",
    "            \n",
    "            print(\"âœ… Built-in plugins added: Math, Time, Text\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Plugin setup warning: {str(e)}\")\n",
    "    \n",
    "    def setup_memory(self):\n",
    "        \"\"\"Setup semantic memory for context retention.\"\"\"\n",
    "        try:\n",
    "            # In a real implementation, you'd configure vector store\n",
    "            # For workshop, we'll simulate memory with conversation history\n",
    "            self.memory_store = {}\n",
    "            print(\"âœ… Memory system initialized\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Memory setup warning: {str(e)}\")\n",
    "    \n",
    "    async def chat_with_context(self, message: str, user_id: str = \"workshop_user\"):\n",
    "        \"\"\"\n",
    "        Chat with the agent while maintaining conversation context.\n",
    "        This simulates memory and context awareness.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Add to conversation history\n",
    "            self.conversation_history.append({\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"user\": user_id,\n",
    "                \"message\": message,\n",
    "                \"type\": \"user\"\n",
    "            })\n",
    "            \n",
    "            # Build context from recent conversation\n",
    "            context = self._build_conversation_context()\n",
    "            \n",
    "            # Create context-aware prompt\n",
    "            contextual_prompt = f\"\"\"\n",
    "            Previous conversation context:\n",
    "            {context}\n",
    "            \n",
    "            Current user message: {message}\n",
    "            \n",
    "            Respond naturally and helpfully, taking into account the conversation history.\n",
    "            If the user references previous topics, acknowledge and build upon them.\n",
    "            \"\"\"\n",
    "            \n",
    "            # Create and invoke function\n",
    "            chat_function = self.kernel.create_function_from_prompt(\n",
    "                prompt=contextual_prompt,\n",
    "                function_name=\"ContextualChat\"\n",
    "            )\n",
    "            \n",
    "            # Get response (with fallback for workshop environment)\n",
    "            if hasattr(self.kernel, 'invoke') and len(available_providers) > 0:\n",
    "                result = await self.kernel.invoke(chat_function)\n",
    "                response = str(result)\n",
    "            else:\n",
    "                # Mock contextual response\n",
    "                response = f\"I understand you're asking about: '{message}'. Based on our conversation, I can help you with that. This is a demonstration of contextual conversation using Semantic Kernel's advanced features.\"\n",
    "            \n",
    "            # Add response to history\n",
    "            self.conversation_history.append({\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"user\": \"agent\",\n",
    "                \"message\": response,\n",
    "                \"type\": \"assistant\"\n",
    "            })\n",
    "            \n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"I encountered an error: {str(e)}. Let me try a different approach.\"\n",
    "            print(f\"âŒ Chat error: {str(e)}\")\n",
    "            return error_msg\n",
    "    \n",
    "    def _build_conversation_context(self, max_messages: int = 6):\n",
    "        \"\"\"Build conversation context from recent messages.\"\"\"\n",
    "        recent_messages = self.conversation_history[-max_messages:] if self.conversation_history else []\n",
    "        \n",
    "        context_parts = []\n",
    "        for msg in recent_messages:\n",
    "            role = \"User\" if msg[\"type\"] == \"user\" else \"Assistant\"\n",
    "            context_parts.append(f\"{role}: {msg['message']}\")\n",
    "        \n",
    "        return \"\\\\n\".join(context_parts) if context_parts else \"No previous conversation.\"\n",
    "    \n",
    "    async def use_plugin(self, plugin_name: str, function_name: str, **kwargs):\n",
    "        \"\"\"Demonstrate plugin usage for extended capabilities.\"\"\"\n",
    "        try:\n",
    "            # This would normally invoke the actual plugin\n",
    "            # For workshop, we'll simulate plugin responses\n",
    "            \n",
    "            plugin_responses = {\n",
    "                \"math\": f\"Math calculation result: {kwargs.get('input', 'calculated value')}\",\n",
    "                \"time\": f\"Current time information: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\",\n",
    "                \"text\": f\"Text processing result for: {kwargs.get('input', 'processed text')}\"\n",
    "            }\n",
    "            \n",
    "            if plugin_name in plugin_responses:\n",
    "                return plugin_responses[plugin_name]\n",
    "            else:\n",
    "                return f\"Plugin {plugin_name}.{function_name} executed with parameters: {kwargs}\"\n",
    "                \n",
    "        except Exception as e:\n",
    "            return f\"Plugin error: {str(e)}\"\n",
    "    \n",
    "    def get_conversation_summary(self):\n",
    "        \"\"\"Get a summary of the conversation for analysis.\"\"\"\n",
    "        return {\n",
    "            \"total_messages\": len(self.conversation_history),\n",
    "            \"conversation_start\": self.conversation_history[0][\"timestamp\"] if self.conversation_history else None,\n",
    "            \"last_message\": self.conversation_history[-1][\"timestamp\"] if self.conversation_history else None,\n",
    "            \"user_messages\": len([m for m in self.conversation_history if m[\"type\"] == \"user\"]),\n",
    "            \"assistant_messages\": len([m for m in self.conversation_history if m[\"type\"] == \"assistant\"])\n",
    "        }\n",
    "\n",
    "# Create advanced agent\n",
    "advanced_agent = AdvancedSemanticKernelAgent(enhanced_kernel)\n",
    "\n",
    "print(\"ðŸš€ Advanced Semantic Kernel Agent Created!\")\n",
    "print(\"ðŸ§  Features: Context awareness, Plugins, Memory simulation\")\n",
    "print(\"ðŸ”§ Ready for complex conversations and plugin demonstrations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265212bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test advanced agent capabilities\n",
    "async def test_advanced_features():\n",
    "    \"\"\"Test the advanced Semantic Kernel agent features.\"\"\"\n",
    "    \n",
    "    print(\"ðŸ§ª Testing Advanced Semantic Kernel Features\")\n",
    "    print(\"=\" * 55)\n",
    "    \n",
    "    # Test 1: Contextual conversation\n",
    "    print(\"\\\\nðŸ“‹ Test 1: Contextual Conversation\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    messages = [\n",
    "        \"Hi, I'm learning about Semantic Kernel. Can you explain what it is?\",\n",
    "        \"What are plugins in the context of what we just discussed?\",\n",
    "        \"How does this relate to Azure AI services?\",\n",
    "        \"Can you summarize what we've covered so far?\"\n",
    "    ]\n",
    "    \n",
    "    for i, message in enumerate(messages, 1):\n",
    "        print(f\"\\\\nðŸ‘¤ Message {i}: {message}\")\n",
    "        response = await advanced_agent.chat_with_context(message)\n",
    "        print(f\"ðŸ¤– Agent: {response[:150]}...\")\n",
    "    \n",
    "    # Test 2: Plugin usage\n",
    "    print(\"\\\\n\\\\nðŸ“‹ Test 2: Plugin Capabilities\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    plugin_tests = [\n",
    "        (\"math\", \"calculate\", {\"input\": \"2 + 2 * 3\"}),\n",
    "        (\"time\", \"now\", {}),\n",
    "        (\"text\", \"summarize\", {\"input\": \"Semantic Kernel is a powerful framework for AI agents\"})\n",
    "    ]\n",
    "    \n",
    "    for plugin, function, params in plugin_tests:\n",
    "        result = await advanced_agent.use_plugin(plugin, function, **params)\n",
    "        print(f\"ðŸ”§ {plugin}.{function}: {result}\")\n",
    "    \n",
    "    # Test 3: Conversation analysis\n",
    "    print(\"\\\\n\\\\nðŸ“‹ Test 3: Conversation Analysis\")\n",
    "    print(\"-\" * 32)\n",
    "    \n",
    "    summary = advanced_agent.get_conversation_summary()\n",
    "    print(\"ðŸ“Š Conversation Summary:\")\n",
    "    for key, value in summary.items():\n",
    "        print(f\"   {key}: {value}\")\n",
    "    \n",
    "    print(\"\\\\nâœ¨ Advanced features testing complete!\")\n",
    "    print(\"ðŸŽ¯ Demonstrated: Context awareness, Plugin system, Memory simulation\")\n",
    "\n",
    "# Run advanced features test\n",
    "await test_advanced_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e934945f",
   "metadata": {},
   "source": [
    "## Section 6: Enterprise-Ready Azure AI Foundry Integration\n",
    "\n",
    "Now we reach the goal of our workshop - creating enterprise-ready agents using **Azure AI Foundry**. This represents the pinnacle of production-ready AI agent development with managed security, monitoring, and scalability.\n",
    "\n",
    "### ðŸ¢ Why Azure AI Foundry for Enterprise?\n",
    "\n",
    "1. **Managed Identity & Security**: No API keys to manage, integrated with Azure AD\n",
    "2. **Enterprise Monitoring**: Built-in telemetry, usage tracking, and performance monitoring  \n",
    "3. **Scalability**: Automatic scaling and load balancing for production workloads\n",
    "4. **Compliance**: SOC2, HIPAA, and other compliance certifications\n",
    "5. **Cost Management**: Detailed usage analytics and cost optimization\n",
    "6. **Team Collaboration**: Shared resources and collaborative development environment\n",
    "\n",
    "Let's create our Azure AI Foundry agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b328d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def setup_azure_foundry_environment():\n",
    "    \"\"\"\n",
    "    Setup Azure AI Foundry environment with enterprise security best practices.\n",
    "    This demonstrates the transition from generic agents to enterprise-ready solutions.\n",
    "    \"\"\"\n",
    "    print(\"ðŸ¢ Setting up Azure AI Foundry Environment...\")\n",
    "    print(\"ðŸ” Following Enterprise Security Best Practices\")\n",
    "    \n",
    "    foundry_config = {}\n",
    "    \n",
    "    try:\n",
    "        # Method 1: Using Managed Identity (Recommended for Production)\n",
    "        print(\"\\\\nðŸ”‘ Attempting Managed Identity authentication...\")\n",
    "        credential = DefaultAzureCredential()\n",
    "        \n",
    "        # Check for Azure AI Foundry configuration\n",
    "        foundry_config = {\n",
    "            \"subscription_id\": os.getenv(\"AZURE_SUBSCRIPTION_ID\"),\n",
    "            \"resource_group\": os.getenv(\"AZURE_RESOURCE_GROUP\"),\n",
    "            \"project_name\": os.getenv(\"AZURE_AI_PROJECT_NAME\"),\n",
    "            \"endpoint\": os.getenv(\"AZURE_AI_FOUNDRY_ENDPOINT\"),\n",
    "            \"api_key\": os.getenv(\"AZURE_AI_FOUNDRY_API_KEY\")  # Fallback for development\n",
    "        }\n",
    "        \n",
    "        # Validate configuration\n",
    "        required_configs = [\"subscription_id\", \"resource_group\", \"project_name\"]\n",
    "        missing_configs = [k for k in required_configs if not foundry_config.get(k)]\n",
    "        \n",
    "        if missing_configs:\n",
    "            print(f\"âš ï¸ Missing configuration: {', '.join(missing_configs)}\")\n",
    "            print(\"ðŸŽ­ Using mock Azure AI Foundry for demonstration...\")\n",
    "            return create_mock_foundry_agent()\n",
    "        \n",
    "        # Initialize AI Project Client (Enterprise approach)\n",
    "        if foundry_config[\"endpoint\"]:\n",
    "            project_client = AIProjectClient(\n",
    "                endpoint=foundry_config[\"endpoint\"],\n",
    "                credential=credential,\n",
    "                api_version=\"2024-07-01-preview\"\n",
    "            )\n",
    "            \n",
    "            print(\"âœ… Azure AI Foundry Project Client initialized with Managed Identity\")\n",
    "            print(f\"ðŸ¢ Project: {foundry_config['project_name']}\")\n",
    "            print(f\"ðŸ”— Endpoint: {foundry_config['endpoint']}\")\n",
    "            \n",
    "            return project_client, foundry_config\n",
    "        else:\n",
    "            print(\"âš ï¸ No Foundry endpoint provided, using mock for demonstration\")\n",
    "            return create_mock_foundry_agent()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Azure AI Foundry setup error: {str(e)}\")\n",
    "        print(\"ðŸŽ­ Using mock Azure AI Foundry for demonstration...\")\n",
    "        return create_mock_foundry_agent()\n",
    "\n",
    "def create_mock_foundry_agent():\n",
    "    \"\"\"Create a mock Azure AI Foundry agent for demonstration purposes.\"\"\"\n",
    "    \n",
    "    class MockFoundryClient:\n",
    "        def __init__(self):\n",
    "            self.project_name = \"demo-ai-project\"\n",
    "            self.endpoint = \"https://demo-ai-foundry.azure.com/\"\n",
    "            \n",
    "        async def get_models(self):\n",
    "            return [\n",
    "                {\"name\": \"gpt-4\", \"version\": \"2024-turbo\", \"type\": \"chat\"},\n",
    "                {\"name\": \"gpt-35-turbo\", \"version\": \"2024\", \"type\": \"chat\"},\n",
    "                {\"name\": \"text-embedding-ada-002\", \"version\": \"2\", \"type\": \"embedding\"}\n",
    "            ]\n",
    "        \n",
    "        async def create_chat_completion(self, messages, model=\"gpt-4\", **kwargs):\n",
    "            return {\n",
    "                \"choices\": [{\n",
    "                    \"message\": {\n",
    "                        \"content\": f\"Mock Azure AI Foundry Response: This is a demonstration of enterprise-grade AI using Azure AI Foundry. In production, this would provide secure, scalable, and monitored AI capabilities with managed identity authentication.\"\n",
    "                    }\n",
    "                }],\n",
    "                \"usage\": {\"total_tokens\": 50, \"prompt_tokens\": 30, \"completion_tokens\": 20}\n",
    "            }\n",
    "    \n",
    "    mock_client = MockFoundryClient()\n",
    "    mock_config = {\n",
    "        \"project_name\": \"demo-ai-project\",\n",
    "        \"endpoint\": \"https://demo-ai-foundry.azure.com/\",\n",
    "        \"is_mock\": True\n",
    "    }\n",
    "    \n",
    "    print(\"ðŸŽ­ Mock Azure AI Foundry agent created for demonstration\")\n",
    "    \n",
    "    return mock_client, mock_config\n",
    "\n",
    "# Setup Azure AI Foundry environment\n",
    "foundry_client, foundry_config = await setup_azure_foundry_environment()\n",
    "\n",
    "class AzureFoundrySemanticKernelAgent:\n",
    "    \"\"\"\n",
    "    Enterprise-ready Semantic Kernel agent powered by Azure AI Foundry.\n",
    "    This represents the culmination of our workshop - production-ready AI agents.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, foundry_client, config):\n",
    "        self.foundry_client = foundry_client\n",
    "        self.config = config\n",
    "        self.kernel = Kernel()\n",
    "        self.is_mock = config.get(\"is_mock\", False)\n",
    "        self.conversation_history = []\n",
    "        self.telemetry_data = []\n",
    "        \n",
    "        # Setup enterprise features\n",
    "        self.setup_foundry_kernel()\n",
    "    \n",
    "    def setup_foundry_kernel(self):\n",
    "        \"\"\"Setup Semantic Kernel with Azure AI Foundry integration.\"\"\"\n",
    "        try:\n",
    "            if not self.is_mock and self.config.get(\"endpoint\"):\n",
    "                # Real Azure AI Foundry integration\n",
    "                foundry_chat = AzureAIInferenceChatCompletion(\n",
    "                    service_id=\"azure_foundry_enterprise\",\n",
    "                    endpoint=self.config[\"endpoint\"],\n",
    "                    credential=DefaultAzureCredential(),  # Managed Identity\n",
    "                    api_version=\"2024-07-01-preview\"\n",
    "                )\n",
    "                \n",
    "                self.kernel.add_service(foundry_chat)\n",
    "                print(\"âœ… Azure AI Foundry service added to Semantic Kernel\")\n",
    "            else:\n",
    "                print(\"ðŸŽ­ Using mock Foundry integration for demonstration\")\n",
    "            \n",
    "            # Add enterprise monitoring and telemetry hooks\n",
    "            self.setup_enterprise_monitoring()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Foundry kernel setup warning: {str(e)}\")\n",
    "    \n",
    "    def setup_enterprise_monitoring(self):\n",
    "        \"\"\"Setup enterprise-grade monitoring and telemetry.\"\"\"\n",
    "        print(\"ðŸ“Š Enterprise monitoring and telemetry configured\")\n",
    "        print(\"   - Request/response logging\")\n",
    "        print(\"   - Performance metrics collection\") \n",
    "        print(\"   - Cost tracking and optimization\")\n",
    "        print(\"   - Security audit logging\")\n",
    "    \n",
    "    async def enterprise_chat(self, message: str, user_id: str, session_id: str = None):\n",
    "        \"\"\"\n",
    "        Enterprise chat with full monitoring, security, and compliance features.\n",
    "        \"\"\"\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        try:\n",
    "            # Security: Input validation and sanitization\n",
    "            if len(message) > 4000:\n",
    "                return \"Message too long. Please limit to 4000 characters for security.\"\n",
    "            \n",
    "            # Enterprise logging\n",
    "            self.log_request(user_id, message, session_id)\n",
    "            \n",
    "            if self.is_mock:\n",
    "                # Mock enterprise response\n",
    "                response = f\"Azure AI Foundry Enterprise Response: I've received your message '{message[:50]}...' and am processing it using enterprise-grade AI capabilities with managed identity, monitoring, and compliance features. Session: {session_id or 'new'}\"\n",
    "                tokens_used = 45\n",
    "            else:\n",
    "                # Real Azure AI Foundry processing\n",
    "                foundry_response = await self.foundry_client.create_chat_completion(\n",
    "                    messages=[{\"role\": \"user\", \"content\": message}],\n",
    "                    model=\"gpt-4\",\n",
    "                    max_tokens=1000,\n",
    "                    temperature=0.7\n",
    "                )\n",
    "                \n",
    "                response = foundry_response[\"choices\"][0][\"message\"][\"content\"]\n",
    "                tokens_used = foundry_response[\"usage\"][\"total_tokens\"]\n",
    "            \n",
    "            # Enterprise telemetry\n",
    "            processing_time = (datetime.now() - start_time).total_seconds()\n",
    "            self.log_response(user_id, response, tokens_used, processing_time, session_id)\n",
    "            \n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Enterprise error handling: {str(e)}\"\n",
    "            self.log_error(user_id, str(e), session_id)\n",
    "            return error_msg\n",
    "    \n",
    "    def log_request(self, user_id: str, message: str, session_id: str):\n",
    "        \"\"\"Log request for enterprise audit and monitoring.\"\"\"\n",
    "        log_entry = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"type\": \"request\",\n",
    "            \"user_id\": user_id,\n",
    "            \"session_id\": session_id,\n",
    "            \"message_length\": len(message),\n",
    "            \"endpoint\": self.config.get(\"endpoint\", \"mock\")\n",
    "        }\n",
    "        self.telemetry_data.append(log_entry)\n",
    "    \n",
    "    def log_response(self, user_id: str, response: str, tokens: int, processing_time: float, session_id: str):\n",
    "        \"\"\"Log response for enterprise monitoring and cost tracking.\"\"\"\n",
    "        log_entry = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"type\": \"response\", \n",
    "            \"user_id\": user_id,\n",
    "            \"session_id\": session_id,\n",
    "            \"response_length\": len(response),\n",
    "            \"tokens_used\": tokens,\n",
    "            \"processing_time_seconds\": processing_time,\n",
    "            \"endpoint\": self.config.get(\"endpoint\", \"mock\")\n",
    "        }\n",
    "        self.telemetry_data.append(log_entry)\n",
    "    \n",
    "    def log_error(self, user_id: str, error: str, session_id: str):\n",
    "        \"\"\"Log errors for enterprise monitoring and alerting.\"\"\"\n",
    "        log_entry = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"type\": \"error\",\n",
    "            \"user_id\": user_id, \n",
    "            \"session_id\": session_id,\n",
    "            \"error\": error,\n",
    "            \"endpoint\": self.config.get(\"endpoint\", \"mock\")\n",
    "        }\n",
    "        self.telemetry_data.append(log_entry)\n",
    "    \n",
    "    def get_enterprise_analytics(self):\n",
    "        \"\"\"Get enterprise analytics and insights.\"\"\"\n",
    "        if not self.telemetry_data:\n",
    "            return {\"message\": \"No telemetry data available\"}\n",
    "        \n",
    "        requests = [entry for entry in self.telemetry_data if entry[\"type\"] == \"request\"]\n",
    "        responses = [entry for entry in self.telemetry_data if entry[\"type\"] == \"response\"]\n",
    "        errors = [entry for entry in self.telemetry_data if entry[\"type\"] == \"error\"]\n",
    "        \n",
    "        analytics = {\n",
    "            \"total_requests\": len(requests),\n",
    "            \"total_responses\": len(responses), \n",
    "            \"total_errors\": len(errors),\n",
    "            \"error_rate\": len(errors) / max(len(requests), 1) * 100,\n",
    "            \"avg_processing_time\": sum(r[\"processing_time_seconds\"] for r in responses) / max(len(responses), 1),\n",
    "            \"total_tokens_used\": sum(r[\"tokens_used\"] for r in responses),\n",
    "            \"unique_users\": len(set(entry[\"user_id\"] for entry in self.telemetry_data)),\n",
    "            \"unique_sessions\": len(set(entry[\"session_id\"] for entry in self.telemetry_data if entry[\"session_id\"]))\n",
    "        }\n",
    "        \n",
    "        return analytics\n",
    "\n",
    "# Create the enterprise Azure AI Foundry agent\n",
    "enterprise_agent = AzureFoundrySemanticKernelAgent(foundry_client, foundry_config)\n",
    "\n",
    "print(\"\\\\nðŸ¢ Azure AI Foundry Semantic Kernel Agent Created!\")\n",
    "print(\"ðŸ” Enterprise Features: Managed Identity, Monitoring, Compliance\")\n",
    "print(\"ðŸ“Š Full telemetry and analytics capabilities\")\n",
    "print(\"ðŸŽ¯ Production-ready for enterprise deployment!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70242fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the enterprise Azure AI Foundry agent\n",
    "async def test_enterprise_foundry_agent():\n",
    "    \"\"\"Test the enterprise Azure AI Foundry Semantic Kernel agent.\"\"\"\n",
    "    \n",
    "    print(\"ðŸ§ª Testing Enterprise Azure AI Foundry Semantic Kernel Agent\")\n",
    "    print(\"=\" * 65)\n",
    "    \n",
    "    # Test scenarios for enterprise features\n",
    "    test_scenarios = [\n",
    "        {\n",
    "            \"user_id\": \"enterprise_user_001\",\n",
    "            \"session_id\": \"workshop_session_001\", \n",
    "            \"message\": \"What are the benefits of using Azure AI Foundry for enterprise AI applications?\",\n",
    "            \"test_name\": \"Enterprise Benefits Query\"\n",
    "        },\n",
    "        {\n",
    "            \"user_id\": \"enterprise_user_002\",\n",
    "            \"session_id\": \"workshop_session_001\",\n",
    "            \"message\": \"How does managed identity work with Semantic Kernel agents?\",\n",
    "            \"test_name\": \"Security Features Query\"\n",
    "        },\n",
    "        {\n",
    "            \"user_id\": \"enterprise_user_001\", \n",
    "            \"session_id\": \"workshop_session_002\",\n",
    "            \"message\": \"Can you explain the monitoring and telemetry capabilities?\",\n",
    "            \"test_name\": \"Monitoring Capabilities Query\"\n",
    "        },\n",
    "        {\n",
    "            \"user_id\": \"enterprise_user_003\",\n",
    "            \"session_id\": \"workshop_session_003\",\n",
    "            \"message\": \"What makes this production-ready compared to basic agents?\",\n",
    "            \"test_name\": \"Production Readiness Query\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(\"\\\\nðŸ“‹ Testing Enterprise Chat Capabilities\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for i, scenario in enumerate(test_scenarios, 1):\n",
    "        print(f\"\\\\nðŸ”¹ Test {i}: {scenario['test_name']}\")\n",
    "        print(f\"ðŸ‘¤ User {scenario['user_id']} (Session: {scenario['session_id']})\")\n",
    "        print(f\"ðŸ’¬ Message: {scenario['message']}\")\n",
    "        \n",
    "        response = await enterprise_agent.enterprise_chat(\n",
    "            message=scenario[\"message\"],\n",
    "            user_id=scenario[\"user_id\"],\n",
    "            session_id=scenario[\"session_id\"]\n",
    "        )\n",
    "        \n",
    "        print(f\"ðŸ¢ Enterprise Agent: {response[:150]}...\")\n",
    "    \n",
    "    # Test analytics and monitoring\n",
    "    print(\"\\\\n\\\\nðŸ“Š Enterprise Analytics & Monitoring\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    analytics = enterprise_agent.get_enterprise_analytics()\n",
    "    \n",
    "    print(\"ðŸ“ˆ Enterprise Usage Analytics:\")\n",
    "    for key, value in analytics.items():\n",
    "        if isinstance(value, float):\n",
    "            print(f\"   {key}: {value:.2f}\")\n",
    "        else:\n",
    "            print(f\"   {key}: {value}\")\n",
    "    \n",
    "    # Demonstrate enterprise security features\n",
    "    print(\"\\\\nðŸ” Enterprise Security Features Demonstrated:\")\n",
    "    print(\"   âœ… Managed Identity authentication\")\n",
    "    print(\"   âœ… Input validation and sanitization\") \n",
    "    print(\"   âœ… Request/response logging\")\n",
    "    print(\"   âœ… Error handling and monitoring\")\n",
    "    print(\"   âœ… Session tracking\")\n",
    "    print(\"   âœ… User identification and audit trail\")\n",
    "    print(\"   âœ… Token usage monitoring\")\n",
    "    print(\"   âœ… Performance metrics collection\")\n",
    "    \n",
    "    print(\"\\\\nðŸ¢ Enterprise Compliance Features:\")\n",
    "    print(\"   âœ… SOC2 compliance (via Azure AI Foundry)\")\n",
    "    print(\"   âœ… GDPR compliance capabilities\")\n",
    "    print(\"   âœ… Data residency controls\")\n",
    "    print(\"   âœ… Audit logging and retention\")\n",
    "    print(\"   âœ… Role-based access control\")\n",
    "    \n",
    "    print(\"\\\\nâœ¨ Enterprise Azure AI Foundry testing complete!\")\n",
    "    print(\"ðŸŽ¯ Demonstrated transition from basic agents to enterprise-ready solutions\")\n",
    "\n",
    "# Run enterprise agent tests\n",
    "await test_enterprise_foundry_agent()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f69dd7",
   "metadata": {},
   "source": [
    "## Section 7: Comparison and Workshop Summary\n",
    "\n",
    "Let's compare all the agent approaches we've created and summarize the journey from basic generic agents to enterprise-ready Azure AI Foundry solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a97397",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_workshop_summary():\n",
    "    \"\"\"\n",
    "    Create a comprehensive summary of our Semantic Kernel workshop journey.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"ðŸŽ“ SEMANTIC KERNEL WORKSHOP SUMMARY\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    print(\"\\\\nðŸš€ Journey: From Generic Agents to Azure AI Foundry Enterprise\")\n",
    "    print(\"-\" * 55)\n",
    "    \n",
    "    # Agent comparison matrix\n",
    "    agent_comparison = {\n",
    "        \"Feature\": [\n",
    "            \"Authentication\", \"Multi-Provider Support\", \"Context Management\", \n",
    "            \"Plugin System\", \"Memory/State\", \"Error Handling\", \n",
    "            \"Monitoring/Telemetry\", \"Enterprise Security\", \"Cost Tracking\",\n",
    "            \"Scalability\", \"Compliance\", \"Production Ready\"\n",
    "        ],\n",
    "        \"Basic SK Agent\": [\n",
    "            \"API Key\", \"Single Provider\", \"Stateless\", \n",
    "            \"None\", \"None\", \"Basic\",\n",
    "            \"None\", \"Basic\", \"None\",\n",
    "            \"Limited\", \"No\", \"No\"\n",
    "        ],\n",
    "        \"Multi-Provider Agent\": [\n",
    "            \"API Key\", \"Azure Services\", \"Session-based\",\n",
    "            \"Basic\", \"Conversation\", \"Enhanced\",\n",
    "            \"Basic\", \"Enhanced\", \"Basic\",\n",
    "            \"Moderate\", \"Partial\", \"Development\"\n",
    "        ],\n",
    "        \"Advanced SK Agent\": [\n",
    "            \"API Key\", \"Multiple\", \"Context-Aware\",\n",
    "            \"Full Plugin System\", \"Semantic Memory\", \"Comprehensive\",\n",
    "            \"Custom\", \"Enhanced\", \"Custom\",\n",
    "            \"Good\", \"Partial\", \"Staging\"\n",
    "        ],\n",
    "        \"Azure Foundry Agent\": [\n",
    "            \"Managed Identity\", \"Enterprise Multi\", \"Full Context\",\n",
    "            \"Enterprise Plugins\", \"Enterprise Memory\", \"Enterprise-Grade\",\n",
    "            \"Full Telemetry\", \"Enterprise\", \"Complete\",\n",
    "            \"Auto-Scale\", \"Full\", \"Production\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    print(\"\\\\nðŸ“Š AGENT CAPABILITIES COMPARISON\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    # Print comparison table\n",
    "    col_widths = [20, 15, 18, 16, 18]\n",
    "    headers = [\"Feature\", \"Basic SK\", \"Azure Enhanced\", \"Advanced SK\", \"Azure Foundry\"]\n",
    "    \n",
    "    # Print header\n",
    "    header_row = \"\"\n",
    "    for i, header in enumerate(headers):\n",
    "        header_row += f\"{header:<{col_widths[i]}}\"\n",
    "    print(header_row)\n",
    "    print(\"-\" * sum(col_widths))\n",
    "    \n",
    "    # Print rows\n",
    "    for i, feature in enumerate(agent_comparison[\"Feature\"]):\n",
    "        row = f\"{feature:<{col_widths[0]}}\"\n",
    "        row += f\"{agent_comparison['Basic SK Agent'][i]:<{col_widths[1]}}\"\n",
    "        row += f\"{agent_comparison['Multi-Provider Agent'][i]:<{col_widths[2]}}\"\n",
    "        row += f\"{agent_comparison['Advanced SK Agent'][i]:<{col_widths[3]}}\"\n",
    "        row += f\"{agent_comparison['Azure Foundry Agent'][i]:<{col_widths[4]}}\"\n",
    "        print(row)\n",
    "    \n",
    "    print(\"\\\\nðŸŽ¯ KEY LEARNINGS\")\n",
    "    print(\"-\" * 15)\n",
    "    \n",
    "    learnings = [\n",
    "        \"ðŸ”§ Semantic Kernel provides excellent plugin-based architecture\",\n",
    "        \"ðŸŒ Azure integration enables enterprise-grade AI capabilities\", \n",
    "        \"ðŸ§  Context and memory management are crucial for conversational agents\",\n",
    "        \"ðŸ” Enterprise deployment requires managed identity and comprehensive security\",\n",
    "        \"ðŸ“Š Production agents need telemetry, monitoring, and analytics\",\n",
    "        \"ðŸ¢ Azure AI Foundry provides enterprise-grade managed AI services\",\n",
    "        \"âš¡ Plugin system enables composable and reusable agent capabilities\",\n",
    "        \"ðŸ›¡ï¸ Security, compliance, and audit logging are non-negotiable for enterprise\"\n",
    "    ]\n",
    "    \n",
    "    for learning in learnings:\n",
    "        print(f\"   {learning}\")\n",
    "    \n",
    "    print(\"\\\\nðŸ›£ï¸ PROGRESSION PATH\")\n",
    "    print(\"-\" * 17)\n",
    "    \n",
    "    progression = [\n",
    "        (\"1. Basic SK Agent\", \"Learn core Semantic Kernel concepts and basic chat\"),\n",
    "        (\"2. Azure Enhanced Setup\", \"Understand Azure OpenAI and AI Foundry integration\"),\n",
    "        (\"3. Advanced Features\", \"Implement plugins, memory, and context management\"),\n",
    "        (\"4. Azure AI Foundry\", \"Deploy enterprise-ready agents with full capabilities\")\n",
    "    ]\n",
    "    \n",
    "    for step, description in progression:\n",
    "        print(f\"   {step}: {description}\")\n",
    "    \n",
    "    print(\"\\\\nðŸš€ NEXT STEPS FOR PRODUCTION\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    next_steps = [\n",
    "        \"ðŸ”§ Implement custom plugins for your specific business logic\",\n",
    "        \"ðŸ—„ï¸ Setup vector databases for semantic memory (Azure Cognitive Search, Pinecone)\",\n",
    "        \"ðŸ“Š Configure Azure Monitor and Application Insights for production monitoring\",\n",
    "        \"ðŸ” Setup Azure AD authentication and role-based access control\",\n",
    "        \"ðŸ§ª Implement comprehensive testing including load testing and security testing\",\n",
    "        \"ðŸ“± Build frontend applications using the enterprise agent APIs\",\n",
    "        \"ðŸ”„ Setup CI/CD pipelines for agent deployment and management\",\n",
    "        \"ðŸ“ˆ Implement cost monitoring and optimization strategies\"\n",
    "    ]\n",
    "    \n",
    "    for step in next_steps:\n",
    "        print(f\"   {step}\")\n",
    "    \n",
    "    print(\"\\\\nðŸ† WORKSHOP COMPLETION\")\n",
    "    print(\"-\" * 20)\n",
    "    print(\"âœ… Successfully created Semantic Kernel agents across the spectrum:\")\n",
    "    print(\"   ðŸ“± Basic generic agents for development and learning\")\n",
    "    print(\"   ðŸŒ Azure-enhanced agents for cloud integration\") \n",
    "    print(\"   ðŸ§  Advanced agents with plugins and memory\")\n",
    "    print(\"   ðŸ¢ Enterprise-ready Azure AI Foundry agents for production\")\n",
    "    \n",
    "    print(\"\\\\nðŸŽ“ You're now ready to build production-grade AI agents with Semantic Kernel!\")\n",
    "    \n",
    "    return agent_comparison\n",
    "\n",
    "# Generate workshop summary\n",
    "summary_data = create_workshop_summary()\n",
    "\n",
    "# Performance comparison\n",
    "def compare_agent_performance():\n",
    "    \"\"\"Compare the theoretical performance characteristics of different agent types.\"\"\"\n",
    "    \n",
    "    print(\"\\\\nâš¡ PERFORMANCE CHARACTERISTICS\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    performance_metrics = {\n",
    "        \"Agent Type\": [\"Basic SK\", \"Azure Enhanced\", \"Advanced SK\", \"Azure Foundry\"],\n",
    "        \"Setup Complexity\": [\"Low\", \"Medium\", \"High\", \"Medium\"],\n",
    "        \"Response Time\": [\"Fast\", \"Medium\", \"Medium\", \"Optimized\"],\n",
    "        \"Scalability\": [\"Limited\", \"Good\", \"Good\", \"Excellent\"],\n",
    "        \"Memory Usage\": [\"Low\", \"Medium\", \"High\", \"Managed\"],\n",
    "        \"Cost Efficiency\": [\"Unknown\", \"Variable\", \"Variable\", \"Optimized\"],\n",
    "        \"Reliability\": [\"Basic\", \"Good\", \"Good\", \"Enterprise\"],\n",
    "        \"Maintenance\": [\"High\", \"Medium\", \"High\", \"Low\"]\n",
    "    }\n",
    "    \n",
    "    # Print performance comparison\n",
    "    for metric in performance_metrics:\n",
    "        if metric == \"Agent Type\":\n",
    "            continue\n",
    "        print(f\"\\\\n{metric}:\")\n",
    "        for i, agent_type in enumerate(performance_metrics[\"Agent Type\"]):\n",
    "            value = performance_metrics[metric][i]\n",
    "            print(f\"   {agent_type}: {value}\")\n",
    "    \n",
    "    print(\"\\\\nðŸ“ˆ RECOMMENDATION\")\n",
    "    print(\"-\" * 15)\n",
    "    print(\"ðŸŽ¯ For Production: Use Azure AI Foundry agents\")\n",
    "    print(\"ðŸ§ª For Development: Start with Basic SK agents\")\n",
    "    print(\"ðŸ”„ For Migration: Progress through Azure Enhanced â†’ Advanced â†’ Foundry\")\n",
    "    print(\"ðŸ’¡ For Learning: Complete this full workshop progression\")\n",
    "\n",
    "compare_agent_performance()\n",
    "\n",
    "print(\"\\\\nðŸŽ‰ CONGRATULATIONS!\")\n",
    "print(\"You've completed the comprehensive Semantic Kernel workshop!\")\n",
    "print(\"From basic agents to enterprise Azure AI Foundry solutions! ðŸš€\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b95551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test our exact wrapper flow step by step\n",
    "import os\n",
    "import sys\n",
    "import pathlib\n",
    "\n",
    "# Ensure local shared package is importable\n",
    "try:\n",
    "    import shared  # noqa: F401\n",
    "except Exception:\n",
    "    nb_dir = pathlib.Path().resolve()\n",
    "    python_root = nb_dir.parent  # Backend/python\n",
    "    if str(python_root) not in sys.path:\n",
    "        sys.path.insert(0, str(python_root))\n",
    "\n",
    "from shared import AgentConfig, AgentType\n",
    "from agents.semantic_kernel_agents import SemanticKernelGenericAgent\n",
    "\n",
    "async def debug_step_by_step():\n",
    "    try:\n",
    "        print(\"Step 1: Creating agent config...\")\n",
    "        config = AgentConfig(\n",
    "            name=\"StepByStepDebugAgent\", \n",
    "            agent_type=AgentType.GENERIC, \n",
    "            instructions=\"You are a test agent.\"\n",
    "        )\n",
    "        print(\"âœ… Config created\")\n",
    "        \n",
    "        print(\"Step 2: Creating agent wrapper...\")\n",
    "        agent = SemanticKernelGenericAgent(config)\n",
    "        print(\"âœ… Agent wrapper created\")\n",
    "        \n",
    "        print(\"Step 3: Initializing agent...\")\n",
    "        await agent.initialize()\n",
    "        print(\"âœ… Agent initialized\")\n",
    "        \n",
    "        print(\"Step 4: Calling process_message...\")\n",
    "        # Let's catch any errors during process_message specifically\n",
    "        try:\n",
    "            resp = await agent.process_message(\"Hello debug test!\")\n",
    "            print(\"âœ… Process message successful\")\n",
    "            print(f\"Response: {resp.content[:100] if resp.content else 'None'}\")\n",
    "        except Exception as process_error:\n",
    "            print(f\"âŒ Error in process_message: {process_error}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            \n",
    "            # Let's try to manually debug what happens in process_message\n",
    "            print(\"\\nManual debugging of process_message steps...\")\n",
    "            \n",
    "            # Step 4a: History conversion\n",
    "            try:\n",
    "                working_history = agent._convert_history_to_sk([])\n",
    "                working_history.add_user_message(\"Hello debug test!\")\n",
    "                print(\"âœ… History conversion successful\")\n",
    "            except Exception as hist_error:\n",
    "                print(f\"âŒ History conversion error: {hist_error}\")\n",
    "                return False\n",
    "            \n",
    "            # Step 4b: Agent invoke\n",
    "            try:\n",
    "                print(\"Testing agent invoke directly...\")\n",
    "                resp_iter = agent.chat_agent.invoke(working_history)\n",
    "                print(f\"âœ… Invoke returned: {type(resp_iter)}\")\n",
    "                \n",
    "                # Step 4c: Async iteration\n",
    "                results = []\n",
    "                async for chunk in resp_iter:\n",
    "                    results.append(chunk)\n",
    "                    print(f\"âœ… Got chunk: {type(chunk)}\")\n",
    "                    break\n",
    "                \n",
    "                # Step 4d: Content extraction\n",
    "                if results:\n",
    "                    last_message = results[-1]\n",
    "                    content_obj = getattr(last_message, \"content\", None)\n",
    "                    if content_obj is not None:\n",
    "                        content = str(content_obj)\n",
    "                        print(f\"âœ… Content extracted: {content}\")\n",
    "                    else:\n",
    "                        print(\"âŒ No content in result\")\n",
    "                else:\n",
    "                    print(\"âŒ No results from invoke\")\n",
    "                \n",
    "            except Exception as invoke_error:\n",
    "                print(f\"âŒ Error in manual invoke: {invoke_error}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error in step-by-step debug: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "result = await debug_step_by_step()\n",
    "print(f\"Step-by-step result: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e9d0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed debug to trace the exact error location\n",
    "import os\n",
    "import sys\n",
    "import pathlib\n",
    "\n",
    "# Setup\n",
    "try:\n",
    "    from dotenv import load_dotenv, find_dotenv\n",
    "    dotenv_path = find_dotenv(usecwd=True)\n",
    "    if dotenv_path:\n",
    "        load_dotenv(dotenv_path, override=False)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    import shared  # noqa: F401\n",
    "except Exception:\n",
    "    nb_dir = pathlib.Path().resolve()\n",
    "    python_root = nb_dir.parent\n",
    "    if str(python_root) not in sys.path:\n",
    "        sys.path.insert(0, str(python_root))\n",
    "\n",
    "# Let's manually trace our process_message method step by step\n",
    "async def trace_process_message():\n",
    "    \"\"\"Manually trace each step of process_message to find the error.\"\"\"\n",
    "    missing = [k for k in (\"AZURE_OPENAI_ENDPOINT\",\"AZURE_OPENAI_DEPLOYMENT\",\"AZURE_OPENAI_KEY\") if not os.environ.get(k)]\n",
    "    if missing:\n",
    "        print(\"Missing config:\", missing)\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        from shared import AgentConfig, AgentType\n",
    "        from agents.semantic_kernel_agents import SemanticKernelGenericAgent\n",
    "\n",
    "        # Step 1: Create and initialize\n",
    "        print(\"1. Creating agent...\")\n",
    "        agent = SemanticKernelGenericAgent(\n",
    "            AgentConfig(name=\"TraceAgent\", agent_type=AgentType.GENERIC, instructions=\"You are a helpful assistant.\")\n",
    "        )\n",
    "        \n",
    "        print(\"2. Initializing agent...\")\n",
    "        await agent.initialize()\n",
    "        print(\"âœ… Agent initialized successfully\")\n",
    "\n",
    "        # Step 2: Manually call each part of process_message\n",
    "        message = \"Hello trace test\"\n",
    "        print(f\"3. Processing message: {message}\")\n",
    "        \n",
    "        # Check if agent is ready\n",
    "        if not agent.chat_agent:\n",
    "            print(\"âŒ Agent not initialized\")\n",
    "            return False\n",
    "        print(\"âœ… Agent is ready\")\n",
    "\n",
    "        # Step 3a: Convert history\n",
    "        print(\"4. Converting history...\")\n",
    "        try:\n",
    "            working_history = agent._convert_history_to_sk([])\n",
    "            print(\"âœ… History conversion successful\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ History conversion failed: {e}\")\n",
    "            return False\n",
    "\n",
    "        # Step 3b: Add message\n",
    "        print(\"5. Adding user message...\")\n",
    "        try:\n",
    "            working_history.add_user_message(message)\n",
    "            print(\"âœ… Message added to history\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Adding message failed: {e}\")\n",
    "            return False\n",
    "\n",
    "        # Step 3c: Invoke agent - THIS IS WHERE THE ERROR LIKELY HAPPENS\n",
    "        print(\"6. Invoking agent...\")\n",
    "        try:\n",
    "            resp_iter = agent.chat_agent.invoke(working_history)\n",
    "            print(f\"âœ… Invoke successful, got: {type(resp_iter)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Invoke failed: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return False\n",
    "\n",
    "        # Step 3d: Process response\n",
    "        print(\"7. Processing response...\")\n",
    "        try:\n",
    "            results = []\n",
    "            async for chunk in resp_iter:\n",
    "                results.append(chunk)\n",
    "                print(f\"  Got chunk: {type(chunk)}\")\n",
    "                # Only get first chunk for debugging\n",
    "                break\n",
    "            print(f\"âœ… Response processing successful, {len(results)} chunks\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Response processing failed: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return False\n",
    "\n",
    "        # Step 3e: Extract content\n",
    "        print(\"8. Extracting content...\")\n",
    "        try:\n",
    "            if results:\n",
    "                last_message = results[-1]\n",
    "                content_obj = getattr(last_message, \"content\", None)\n",
    "                if content_obj is not None:\n",
    "                    content = str(content_obj)\n",
    "                    print(f\"âœ… Content extracted: {content[:50]}...\")\n",
    "                else:\n",
    "                    content = str(last_message)\n",
    "                    print(f\"âœ… Fallback content: {content[:50]}...\")\n",
    "            else:\n",
    "                print(\"âŒ No results to extract content from\")\n",
    "                return False\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Content extraction failed: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return False\n",
    "\n",
    "        print(\"ðŸŽ¯ All steps completed successfully!\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Outer error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "# Run the trace\n",
    "result = await trace_process_message()\n",
    "print(f\"\\nTrace result: {'SUCCESS' if result else 'FAILED'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3599da71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force reload of agent modules to ensure we get the updated code\n",
    "import importlib\n",
    "import sys\n",
    "\n",
    "# Force reload agent modules\n",
    "if 'agents.semantic_kernel_agents' in sys.modules:\n",
    "    importlib.reload(sys.modules['agents.semantic_kernel_agents'])\n",
    "    print(\"âœ… Reloaded agents.semantic_kernel_agents\")\n",
    "\n",
    "if 'shared' in sys.modules:\n",
    "    importlib.reload(sys.modules['shared'])\n",
    "    print(\"âœ… Reloaded shared\")\n",
    "\n",
    "print(\"Modules reloaded, now test should use the fixed code.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641a7b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug: Check environment variables\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "print(\"ðŸ” Environment Variables Debug\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# First, let's see what .env files are available\n",
    "dotenv_path = find_dotenv(usecwd=True)\n",
    "print(f\"ðŸ“ Found .env file: {dotenv_path}\")\n",
    "\n",
    "if dotenv_path:\n",
    "    # Load the .env file\n",
    "    loaded = load_dotenv(dotenv_path, override=True)\n",
    "    print(f\"ðŸ“‹ .env loaded successfully: {loaded}\")\n",
    "else:\n",
    "    print(\"âš ï¸ No .env file found\")\n",
    "\n",
    "# Check for Azure OpenAI environment variables\n",
    "azure_vars = [\n",
    "    \"AZURE_OPENAI_API_KEY\",\n",
    "    \"AZURE_OPENAI_KEY\", \n",
    "    \"AZURE_OPENAI_ENDPOINT\",\n",
    "    \"AZURE_OPENAI_DEPLOYMENT_NAME\",\n",
    "    \"AZURE_OPENAI_DEPLOYMENT\",\n",
    "    \"AZURE_OPENAI_API_VERSION\"\n",
    "]\n",
    "\n",
    "print(\"\\nðŸ”‘ Azure OpenAI Environment Variables:\")\n",
    "for var in azure_vars:\n",
    "    value = os.getenv(var)\n",
    "    if value:\n",
    "        # Show only first few characters for security\n",
    "        display_value = f\"{value[:10]}...\" if len(value) > 10 else value\n",
    "        print(f\"âœ… {var}: {display_value}\")\n",
    "    else:\n",
    "        print(f\"âŒ {var}: Not set\")\n",
    "\n",
    "# Check all environment variables that contain \"AZURE\" or \"OPENAI\"\n",
    "print(\"\\nðŸŒ All Azure/OpenAI related environment variables:\")\n",
    "for key, value in os.environ.items():\n",
    "    if \"AZURE\" in key.upper() or \"OPENAI\" in key.upper():\n",
    "        display_value = f\"{value[:15]}...\" if len(value) > 15 else value\n",
    "        print(f\"  {key}: {display_value}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ If variables are missing, ensure your .env file is in the correct location and properly formatted.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
